{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LLM Answer Watcher","text":"<p>Monitor how Large Language Models talk about your brand versus competitors in buyer-intent queries</p> <p>Get Started View on GitHub</p>"},{"location":"#what-is-llm-answer-watcher","title":"What is LLM Answer Watcher?","text":"<p>LLM Answer Watcher is a production-ready CLI tool that helps you understand how AI models like ChatGPT, Claude, and others represent your brand when users ask buyer-intent questions.</p> <p>As AI-powered search becomes mainstream, monitoring your brand's presence in LLM responses is crucial for:</p> <ul> <li>Brand Visibility: Track if your product appears in AI recommendations</li> <li>Competitive Intelligence: See which competitors are mentioned alongside you</li> <li>Market Positioning: Understand your ranking compared to alternatives</li> <li>Trend Analysis: Historical data shows how your presence changes over time</li> </ul>"},{"location":"#demo","title":"Demo","text":"<p>See LLM Answer Watcher in action:</p> <p></p> <p>What you're seeing:</p> <ul> <li>Configuration validation with brand and competitor definitions</li> <li>Real-time progress bars showing query execution across LLM providers</li> <li>Brand mention extraction and ranking from AI responses</li> <li>Cost tracking and results summary</li> </ul> <p>Try it yourself: Run <code>llm-answer-watcher demo</code> for an interactive demo (no API keys needed!)</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#brand-mention-detection","title":"\ud83d\udd0d Brand Mention Detection","text":"<p>Advanced word-boundary regex matching prevents false positives while accurately identifying your brand and competitors in LLM responses.</p>"},{"location":"#historical-tracking","title":"\ud83d\udcca Historical Tracking","text":"<p>All responses are stored in a local SQLite database, enabling powerful trend analysis and long-term visibility tracking.</p>"},{"location":"#multi-provider-support","title":"\ud83e\udd16 Multi-Provider Support","text":"<p>Works with 6+ LLM providers: OpenAI, Anthropic, Mistral, X.AI Grok, Google Gemini, and Perplexity, with an extensible architecture for adding more.</p>"},{"location":"#browser-runners-beta-new-in-v020","title":"\ud83c\udf10 Browser Runners (BETA - New in v0.2.0)","text":"<p>Interact with web-based LLM interfaces (ChatGPT, Perplexity) using headless browser automation via Steel API. Captures true user experience with screenshots and HTML snapshots.</p>"},{"location":"#async-parallelization-new-in-v020","title":"\u26a1 Async Parallelization (New in v0.2.0)","text":"<p>3-4x faster performance with async/await parallel query execution across multiple models and providers.</p>"},{"location":"#intelligent-rank-extraction","title":"\ud83d\udcc8 Intelligent Rank Extraction","text":"<p>Automatically detects where your brand appears in ranked lists using pattern-based extraction and optional LLM-assisted ranking.</p>"},{"location":"#sentiment-analysis-intent-classification","title":"\ud83c\udfad Sentiment Analysis &amp; Intent Classification","text":"<ul> <li>Sentiment Analysis: Analyze the tone (positive/neutral/negative) and context of each brand mention</li> <li>Intent Classification: Determine user intent type, buyer journey stage, and urgency signals</li> <li>Prioritization: Focus on high-value queries with ready-to-buy intent</li> <li>ROI Tracking: Understand which mentions drive real business value</li> </ul>"},{"location":"#dynamic-pricing-budget-protection","title":"\ud83d\udcb0 Dynamic Pricing &amp; Budget Protection","text":"<ul> <li>Real-time pricing from llm-prices.com</li> <li>Pre-run cost estimation</li> <li>Configurable spending limits</li> <li>Accurate web search cost calculation</li> </ul>"},{"location":"#dual-mode-cli","title":"\ud83c\udfaf Dual-Mode CLI","text":"<ul> <li>Human Mode: Beautiful Rich output with spinners, colors, and formatted tables</li> <li>Agent Mode: Structured JSON output for AI agent automation</li> <li>Quiet Mode: Minimal tab-separated output for scripts</li> </ul>"},{"location":"#professional-html-reports","title":"\ud83d\udccb Professional HTML Reports","text":"<p>Auto-generated reports with: - Brand mention visualizations - Rank distribution charts - Historical trends - Raw response inspection</p>"},{"location":"#local-first-secure","title":"\ud83d\udd12 Local-First &amp; Secure","text":"<ul> <li>All data stored locally on your machine</li> <li>BYOK (Bring Your Own Keys) - use your own API keys</li> <li>No external dependencies except LLM APIs</li> <li>Built-in SQL injection and XSS protection</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code># Set your API keys\nexport OPENAI_API_KEY=sk-your-key-here\nexport ANTHROPIC_API_KEY=sk-ant-your-key-here\n\n# Run with a config file\nllm-answer-watcher run --config watcher.config.yaml\n</code></pre> <p>Output:</p> <pre><code>\ud83d\udd0d Running LLM Answer Watcher...\n\u251c\u2500\u2500 Query: \"What are the best email warmup tools?\"\n\u251c\u2500\u2500 Models: OpenAI gpt-4o-mini, Anthropic claude-3-5-haiku\n\u251c\u2500\u2500 Brands: 2 monitored, 5 competitors\n\u2514\u2500\u2500 Output: ./output/2025-11-01T14-30-00Z/\n\n\u2705 Queries completed: 6/6\n\ud83d\udcb0 Total cost: $0.0142\n\ud83d\udcca Report: ./output/2025-11-01T14-30-00Z/report.html\n</code></pre>"},{"location":"#use-cases","title":"Use Cases","text":""},{"location":"#1-brand-monitoring","title":"1. Brand Monitoring","text":"<p>Track your product's visibility in AI-powered search results across multiple LLM providers.</p>"},{"location":"#2-competitive-analysis","title":"2. Competitive Analysis","text":"<p>See which competitors appear most frequently and in what context they're recommended.</p>"},{"location":"#3-seo-for-ai-era","title":"3. SEO for AI Era","text":"<p>Optimize your brand presence in LLM training data and real-time retrieval systems.</p>"},{"location":"#4-market-research","title":"4. Market Research","text":"<p>Understand how AI models categorize and compare products in your space.</p>"},{"location":"#5-product-development","title":"5. Product Development","text":"<p>Identify gaps where competitors are mentioned but your product isn't.</p>"},{"location":"#6-sales-intelligence","title":"6. Sales Intelligence","text":"<p>Know what alternatives prospects might be comparing you against.</p>"},{"location":"#architecture-highlights","title":"Architecture Highlights","text":"<p>LLM Answer Watcher is built with production-ready patterns:</p> <ul> <li>Domain-Driven Design: Clear separation between config, LLM clients, extraction, storage, and reporting</li> <li>Provider Abstraction: Easy to add new LLM providers with unified interface</li> <li>Plugin System: Extensible runner architecture supporting both API and browser-based runners</li> <li>Async/Await: Parallel query execution for 3-4x performance improvement (v0.2.0+)</li> <li>Retry Logic: Exponential backoff with tenacity for resilient API calls</li> <li>Type Safety: Full Pydantic validation and modern Python 3.12+ type hints</li> <li>Testability: 750+ test cases with 100% coverage on critical paths</li> <li>API-First Contract: Internal structure designed to become HTTP API for Cloud product</li> </ul>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<p>This documentation is organized progressively from beginner to advanced:</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Everything you need to get up and running in 5 minutes.</p>"},{"location":"#user-guide","title":"User Guide","text":"<p>Comprehensive guides for configuration, usage, and features.</p>"},{"location":"#supported-providers","title":"Supported Providers","text":"<p>Detailed information about each LLM provider integration.</p>"},{"location":"#examples","title":"Examples","text":"<p>Real-world examples and use cases with complete configurations.</p>"},{"location":"#data-analytics","title":"Data &amp; Analytics","text":"<p>Understanding output structure and running SQL analytics.</p>"},{"location":"#evaluation-framework","title":"Evaluation Framework","text":"<p>Quality control and accuracy testing for extraction logic.</p>"},{"location":"#advanced-topics","title":"Advanced Topics","text":"<p>Deep dives into architecture, security, and extending the system.</p>"},{"location":"#reference","title":"Reference","text":"<p>Complete CLI command reference and configuration schemas.</p>"},{"location":"#philosophy","title":"Philosophy","text":"<p>LLM Answer Watcher is built on these principles:</p> <ul> <li>Boring is Good: Simple, readable code over clever abstractions</li> <li>Local-First: Your data stays on your machine</li> <li>Production-Ready: Proper error handling, retry logic, and security from day one</li> <li>Data is the Moat: Historical SQLite tracking provides long-term value</li> <li>Developer Experience: Both human-friendly and AI agent-ready interfaces</li> </ul>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li> <p> Quick Start</p> <p>Install and run your first monitoring job in minutes.</p> <p>Get Started \u2192</p> </li> <li> <p> Configuration</p> <p>Learn how to configure models, brands, and intents.</p> <p>Configuration Guide \u2192</p> </li> <li> <p> Examples</p> <p>See real-world examples for common use cases.</p> <p>View Examples \u2192</p> </li> <li> <p> Analytics</p> <p>Query your data with SQL for powerful insights.</p> <p>Data &amp; Analytics \u2192</p> </li> </ul>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>GitHub Issues: Report bugs or request features</li> <li>Contributing: Read our contributing guide</li> <li>License: MIT - see LICENSE</li> </ul> <sub>Built with \u2764\ufe0f by Nikola Bali\u0107</sub>"},{"location":"BROWSER_RUNNERS/","title":"Browser Runners Guide","text":"<p>\u26a0\ufe0f BETA FEATURE - Browser runners were added on November 8, 2025 and are currently in BETA quality.</p> <p>Known Limitations: - Cost tracking returns $0.00 (placeholder - actual Steel API costs not yet calculated) - CSS selectors may break if web UIs change - ChatGPT authentication not fully documented - Response completion detection is heuristic-based</p> <p>Suitable for: Research, testing, screenshot capture, hobby/startup monitoring Not recommended for: Fully automated production systems relying on accurate cost tracking</p>"},{"location":"BROWSER_RUNNERS/#overview","title":"Overview","text":"<p>Browser runners enable LLM Answer Watcher to interact with web-based LLM interfaces like ChatGPT and Perplexity using headless browser automation. This captures the true user experience that differs from direct API access.</p>"},{"location":"BROWSER_RUNNERS/#why-browser-runners","title":"Why Browser Runners?","text":""},{"location":"BROWSER_RUNNERS/#key-differences-browser-ui-vs-api","title":"Key Differences: Browser UI vs API","text":"Aspect API Browser (ChatGPT/Perplexity) Web Search Optional tool calling Integrated UI behavior Citations Structured JSON Visual citations in response System Prompts Your control Platform-specific defaults Rate Limits API quota Web UI limits Cost Per token Free tier or subscription Response Format Raw text Formatted with markdown, links"},{"location":"BROWSER_RUNNERS/#use-cases","title":"Use Cases","text":"<ol> <li>Verify API vs Web Consistency: Check if your brand appears differently in ChatGPT web UI vs API</li> <li>Capture Visual Evidence: Screenshots prove how brands are displayed</li> <li>Test Web-Specific Features: Perplexity sources, ChatGPT citations</li> <li>Monitor Free Tier Behavior: See what users without API access experience</li> </ol>"},{"location":"BROWSER_RUNNERS/#supported-platforms","title":"Supported Platforms","text":"<ul> <li>\u2705 ChatGPT (chat.openai.com) via Steel</li> <li>\u2705 Perplexity (perplexity.ai) via Steel</li> <li>\ud83d\udea7 Claude (claude.ai) - Coming soon</li> <li>\ud83d\udea7 Gemini (gemini.google.com) - Coming soon</li> </ul>"},{"location":"BROWSER_RUNNERS/#prerequisites","title":"Prerequisites","text":""},{"location":"BROWSER_RUNNERS/#1-steel-api-account","title":"1. Steel API Account","text":"<p>Browser runners use Steel for headless browser automation.</p> <pre><code># Sign up at https://steel.dev\n# Get your API key from dashboard\nexport STEEL_API_KEY=\"your-steel-api-key\"\n</code></pre> <p>Steel Pricing (as of 2025): - Hobby: $0/month + $0.10/hour browser time (5 concurrent sessions) - Pro: $49/month + $0.10/hour (20 concurrent sessions) - Enterprise: Custom pricing</p>"},{"location":"BROWSER_RUNNERS/#2-optional-captcha-solver","title":"2. Optional: CAPTCHA Solver","text":"<p>Steel can integrate with CAPTCHA solvers for sites that require login:</p> <pre><code># Optional: CapSolver API key\nexport CAPSOLVER_API_KEY=\"your-capsolver-key\"\n</code></pre>"},{"location":"BROWSER_RUNNERS/#configuration","title":"Configuration","text":""},{"location":"BROWSER_RUNNERS/#basic-example","title":"Basic Example","text":"<pre><code># watcher.config.yaml\n\nrunners:\n  # Browser runner: ChatGPT via Steel\n  - runner_plugin: \"steel-chatgpt\"\n    config:\n      steel_api_key: \"${STEEL_API_KEY}\"\n      target_url: \"https://chat.openai.com\"\n      session_timeout: 300\n      wait_for_response_timeout: 60\n      take_screenshots: true\n      save_html_snapshot: true\n      session_reuse: true\n\n  # Browser runner: Perplexity via Steel\n  - runner_plugin: \"steel-perplexity\"\n    config:\n      steel_api_key: \"${STEEL_API_KEY}\"\n      target_url: \"https://www.perplexity.ai\"\n      session_timeout: 300\n      take_screenshots: true\n      session_reuse: true\n\nbrands:\n  mine: [\"YourBrand\"]\n  competitors: [\"CompetitorA\", \"CompetitorB\"]\n\nintents:\n  - id: \"crm-tools\"\n    prompt: \"What are the best CRM tools?\"\n</code></pre>"},{"location":"BROWSER_RUNNERS/#configuration-options","title":"Configuration Options","text":""},{"location":"BROWSER_RUNNERS/#common-options-all-browser-runners","title":"Common Options (All Browser Runners)","text":"Option Type Default Description <code>steel_api_key</code> string required Steel API key (use env var) <code>target_url</code> string Platform URL Starting URL for browser <code>session_timeout</code> int 300 Max session duration (seconds) <code>wait_for_response_timeout</code> int 60 Max wait for LLM response (seconds) <code>take_screenshots</code> bool true Capture screenshots <code>save_html_snapshot</code> bool true Save HTML snapshots <code>session_reuse</code> bool true Reuse sessions (faster, cheaper) <code>solver</code> string \"capsolver\" CAPTCHA solver service <code>proxy</code> string null Optional proxy config <code>output_dir</code> string \"./output\" Directory for artifacts"},{"location":"BROWSER_RUNNERS/#chatgpt-specific-options","title":"ChatGPT-Specific Options","text":"<pre><code>- runner_plugin: \"steel-chatgpt\"\n  config:\n    steel_api_key: \"${STEEL_API_KEY}\"\n    target_url: \"https://chat.openai.com\"\n    # Add ChatGPT session token if needed\n    # chatgpt_session_token: \"${CHATGPT_SESSION_TOKEN}\"\n</code></pre>"},{"location":"BROWSER_RUNNERS/#perplexity-specific-options","title":"Perplexity-Specific Options","text":"<pre><code>- runner_plugin: \"steel-perplexity\"\n  config:\n    steel_api_key: \"${STEEL_API_KEY}\"\n    target_url: \"https://www.perplexity.ai\"\n    # Perplexity works without login\n</code></pre>"},{"location":"BROWSER_RUNNERS/#usage","title":"Usage","text":""},{"location":"BROWSER_RUNNERS/#run-with-browser-runners","title":"Run with Browser Runners","text":"<pre><code># Set environment variables\nexport STEEL_API_KEY=\"your-steel-api-key\"\n\n# Run with browser runner config\nllm-answer-watcher run --config examples/watcher.config.browser-runners.yaml\n\n# Output:\n# \u2713 Created Steel session: session-abc123\n# \u2713 Submitted prompt to ChatGPT: \"What are the best CRM tools?\"\n# \u2713 Extracted answer (2,345 chars)\n# \u2713 Screenshot saved: ./output/2025-11-06T10-30-00Z/screenshot_chatgpt_session-abc123.png\n# \u2713 HTML snapshot saved: ./output/2025-11-06T10-30-00Z/html_chatgpt_session-abc123.html\n</code></pre>"},{"location":"BROWSER_RUNNERS/#compare-api-vs-browser-results","title":"Compare API vs Browser Results","text":"<pre><code># Run configuration with both API and browser runners\nllm-answer-watcher run --config examples/watcher.config.browser-runners.yaml\n\n# View comparison report\nllm-answer-watcher report --run-id 2025-11-06T10-30-00Z\n</code></pre> <p>The HTML report will show: - API Response: Direct from OpenAI API with web search - ChatGPT Browser: Screenshot + extracted text from web UI - Perplexity Browser: Screenshot + sources from Perplexity</p>"},{"location":"BROWSER_RUNNERS/#artifacts-generated","title":"Artifacts Generated","text":"<p>Each browser run produces:</p> <pre><code>output/\n\u2514\u2500\u2500 2025-11-06T10-30-00Z/\n    \u251c\u2500\u2500 run_meta.json\n    \u251c\u2500\u2500 screenshot_chatgpt_session-abc123.png     # Visual evidence\n    \u251c\u2500\u2500 html_chatgpt_session-abc123.html          # Full HTML snapshot\n    \u251c\u2500\u2500 intent_crm-tools_raw_chatgpt-web.json     # Structured data\n    \u251c\u2500\u2500 intent_crm-tools_parsed_chatgpt-web.json  # Extracted mentions\n    \u2514\u2500\u2500 report.html                                # HTML report\n</code></pre>"},{"location":"BROWSER_RUNNERS/#architecture","title":"Architecture","text":""},{"location":"BROWSER_RUNNERS/#plugin-system","title":"Plugin System","text":"<p>Browser runners integrate seamlessly via the plugin system:</p> <pre><code>IntentRunner (Protocol)\n    \u251c\u2500\u2500 APIRunner (wraps LLMClient)\n    \u2502   \u251c\u2500\u2500 OpenAI\n    \u2502   \u251c\u2500\u2500 Anthropic\n    \u2502   \u2514\u2500\u2500 Others...\n    \u2514\u2500\u2500 BrowserRunner (extends SteelBaseRunner)\n        \u251c\u2500\u2500 SteelChatGPTRunner\n        \u251c\u2500\u2500 SteelPerplexityRunner\n        \u2514\u2500\u2500 SteelClaudeRunner (coming soon)\n</code></pre>"},{"location":"BROWSER_RUNNERS/#how-it-works","title":"How It Works","text":"<ol> <li>Session Creation: Steel creates headless Chrome browser</li> <li>Navigation: Runner navigates to target URL (chat.openai.com)</li> <li>Authentication: Steel manages cookies/sessions automatically</li> <li>Prompt Submission: Runner types prompt into UI</li> <li>Wait for Response: Monitors DOM for response completion</li> <li>Extraction: Scrapes answer text from page</li> <li>Artifacts: Captures screenshot, HTML snapshot</li> <li>Cleanup: Releases session (or reuses for next intent)</li> </ol>"},{"location":"BROWSER_RUNNERS/#troubleshooting","title":"Troubleshooting","text":""},{"location":"BROWSER_RUNNERS/#steel-api-key-invalid","title":"\"Steel API key invalid\"","text":"<pre><code># Verify your API key\necho $STEEL_API_KEY\n\n# Test Steel access\ncurl -H \"Authorization: Bearer $STEEL_API_KEY\" https://api.steel.dev/v1/sessions\n</code></pre>"},{"location":"BROWSER_RUNNERS/#session-timeout-exceeded","title":"\"Session timeout exceeded\"","text":"<p>Increase timeout if prompts are complex:</p> <pre><code>config:\n  session_timeout: 600  # 10 minutes\n  wait_for_response_timeout: 120  # 2 minutes\n</code></pre>"},{"location":"BROWSER_RUNNERS/#captcha-blocking","title":"\"CAPTCHA blocking\"","text":"<p>Enable CAPTCHA solver:</p> <pre><code>config:\n  solver: \"capsolver\"  # or \"2captcha\", \"anticaptcha\"\n  # Add solver API key to environment\n</code></pre>"},{"location":"BROWSER_RUNNERS/#element-not-found","title":"\"Element not found\"","text":"<p>Browser UI selectors may change. Check logs for details:</p> <pre><code>llm-answer-watcher run --config config.yaml --verbose\n</code></pre>"},{"location":"BROWSER_RUNNERS/#cost-management","title":"Cost Management","text":"<p>\u26a0\ufe0f IMPORTANT: Cost tracking for browser runners currently returns $0.00 in reports. This is a placeholder - you WILL be charged by Steel based on session duration. Monitor your Steel dashboard for actual costs.</p>"},{"location":"BROWSER_RUNNERS/#browser-runner-costs","title":"Browser Runner Costs","text":"<p>Browser runners have zero LLM API cost but incur Steel charges that are not yet tracked in LLM Answer Watcher cost estimates:</p> Activity Cost Tracked in Reports? API runners Per token (normal rates) \u2705 Yes Browser runners $0.10-0.30/hour via Steel \u274c No (shows $0.00) CAPTCHA solving $1-3 per 1,000 solves \u274c No"},{"location":"BROWSER_RUNNERS/#cost-optimization","title":"Cost Optimization","text":"<ol> <li>Enable Session Reuse: Reuse sessions across intents</li> <li>Reduce Timeouts: Lower <code>session_timeout</code> if possible</li> <li>Selective Screenshots: Disable screenshots if not needed</li> <li>Batch Intents: Run multiple intents per session</li> </ol> <pre><code>config:\n  session_reuse: true  # Reuse sessions (big cost savings)\n  take_screenshots: false  # Skip if not needed\n  session_timeout: 180  # Shorter timeout = lower cost\n</code></pre>"},{"location":"BROWSER_RUNNERS/#limitations","title":"Limitations","text":""},{"location":"BROWSER_RUNNERS/#current-limitations-beta-status","title":"Current Limitations (BETA Status)","text":"<ol> <li>\u274c No Cost Tracking: Browser runner costs show $0.00 (placeholder - Steel charges not calculated)</li> <li>\u274c No Token Tracking: Browser responses don't expose token counts</li> <li>\u274c No Model Detection: Can't determine which ChatGPT model is used</li> <li>\u26a0\ufe0f Slower than API: Browser automation adds 10-30s overhead per query</li> <li>\u26a0\ufe0f Rate Limits: Subject to web UI rate limits (not API limits)</li> <li>\u26a0\ufe0f Fragile Selectors: CSS selectors may break if ChatGPT/Perplexity UI changes</li> <li>\u26a0\ufe0f Limited CDP Implementation: Full Steel CDP commands need implementation for advanced features</li> </ol>"},{"location":"BROWSER_RUNNERS/#future-enhancements-planned-for-v030","title":"Future Enhancements (Planned for v0.3.0)","text":"<ul> <li> Implement accurate cost tracking based on Steel session duration (HIGH PRIORITY)</li> <li> Implement full Steel CDP commands for navigation/extraction</li> <li> Add web source extraction (citations, search results)</li> <li> Support Claude and Gemini web UIs</li> <li> Add browser action recording (interaction_steps)</li> <li> Add authentication documentation for ChatGPT login</li> <li> Add browser pool for parallel execution</li> <li> Support custom browser configurations (extensions, etc.)</li> <li> Implement retry logic for selector failures</li> </ul>"},{"location":"BROWSER_RUNNERS/#plugin-development","title":"Plugin Development","text":"<p>Want to add a new browser runner? See Plugin Development Guide.</p> <p>Example:</p> <pre><code>from llm_answer_watcher.llm_runner.browser.steel_base import SteelBaseRunner\nfrom llm_answer_watcher.llm_runner.plugin_registry import RunnerRegistry\n\n@RunnerRegistry.register\nclass MyCustomBrowserPlugin:\n    @classmethod\n    def plugin_name(cls) -&gt; str:\n        return \"my-browser\"\n\n    @classmethod\n    def runner_type(cls) -&gt; str:\n        return \"browser\"\n\n    # Implement other required methods...\n</code></pre>"},{"location":"BROWSER_RUNNERS/#resources","title":"Resources","text":"<ul> <li>Steel Documentation</li> <li>Chrome DevTools Protocol</li> <li>Plugin Development Guide</li> <li>API vs Browser Comparison</li> </ul>"},{"location":"BROWSER_RUNNERS/#support","title":"Support","text":"<p>For issues with browser runners:</p> <ol> <li>Check Steel API status: https://status.steel.dev</li> <li>Review logs with <code>--verbose</code> flag</li> <li>Report issues: https://github.com/nibzard/llm-answer-watcher/issues</li> <li>Tag with <code>browser-runner</code> label</li> </ol>"},{"location":"NEW_MODEL_COMPATIBILITY/","title":"New Model Compatibility Guide","text":""},{"location":"NEW_MODEL_COMPATIBILITY/#overview","title":"Overview","text":"<p>LLM Answer Watcher is designed to be compatible with new OpenAI models as they're released. However, newer models may have different parameter requirements than existing ones.</p> <p>\ud83d\udcc5 Updated November 2025: GPT-5 models (<code>gpt-5-mini</code>, <code>gpt-5-nano</code>) are now available and have different parameter requirements than GPT-4 models.</p>"},{"location":"NEW_MODEL_COMPATIBILITY/#the-temperature-parameter-issue","title":"The Temperature Parameter Issue","text":""},{"location":"NEW_MODEL_COMPATIBILITY/#problem","title":"Problem","text":"<p>Current OpenAI GPT-5 models (<code>gpt-5-mini</code>, <code>gpt-5-nano</code>) don't support custom temperature values and only work with the model's default temperature (1.0). This is a real limitation as of November 2025.</p>"},{"location":"NEW_MODEL_COMPATIBILITY/#solution","title":"Solution","text":"<p>The OpenAI client has been updated with dynamic parameter handling:</p> <pre><code># Models that require fixed temperature (don't support custom temperature)\nTEMPERATURE_FIXED_MODELS = {\n    \"gpt-5-mini\",\n    \"gpt-5-nano\",\n    # Add other models as they're released\n}\n</code></pre>"},{"location":"NEW_MODEL_COMPATIBILITY/#how-it-works","title":"How It Works","text":"<ol> <li>Check Model Support: The client checks if the model is in <code>TEMPERATURE_FIXED_MODELS</code></li> <li>Conditional Parameters: Temperature is only added if the model supports it</li> <li>Graceful Fallback: Uses model default temperature for restricted models</li> </ol>"},{"location":"NEW_MODEL_COMPATIBILITY/#adding-support-for-new-models","title":"Adding Support for New Models","text":""},{"location":"NEW_MODEL_COMPATIBILITY/#step-1-identify-model-requirements","title":"Step 1: Identify Model Requirements","text":"<p>When a new OpenAI model is released (beyond GPT-5-mini/nano), check: - Does it support custom temperature values? - Are there other parameter restrictions? - Is it available via the standard OpenAI API?</p>"},{"location":"NEW_MODEL_COMPATIBILITY/#step-2-update-model-configuration","title":"Step 2: Update Model Configuration","text":"<p>If the model has parameter restrictions (like GPT-5 models), add it to the appropriate configuration:</p> <pre><code># For models that don't support custom temperature\nTEMPERATURE_FIXED_MODELS.add(\"new-model-name\")\n\n# For models with other parameter restrictions\nMODEL_PARAMS = {\n    \"default\": {\n        \"temperature\": 0.7,\n    },\n    \"temperature_fixed\": {\n        # No temperature parameter\n    },\n    \"custom_category\": {\n        # Add model-specific parameters\n    },\n}\n</code></pre>"},{"location":"NEW_MODEL_COMPATIBILITY/#step-3-test-the-model","title":"Step 3: Test the Model","text":"<p>Create a test configuration:</p> <pre><code># test-new-model.yaml\nmodels:\n  - provider: \"openai\"\n    model_name: \"new-model-name\"\n    env_api_key: \"OPENAI_API_KEY\"\n\nbrands:\n  mine:\n    - \"YourBrand\"\n\nintents:\n  - id: \"test-intent\"\n    prompt: \"Test prompt for new model\"\n</code></pre> <p>Run: <code>llm-answer-watcher run --config test-new-model.yaml --yes</code></p>"},{"location":"NEW_MODEL_COMPATIBILITY/#step-4-update-documentation","title":"Step 4: Update Documentation","text":"<p>Add the new model to: - Configuration examples - Documentation - Default model lists (if appropriate)</p>"},{"location":"NEW_MODEL_COMPATIBILITY/#current-model-support-november-2025","title":"Current Model Support (November 2025)","text":""},{"location":"NEW_MODEL_COMPATIBILITY/#fully-supported-custom-temperature","title":"Fully Supported (Custom Temperature)","text":"<ul> <li><code>gpt-4o-mini</code> - Supports custom temperature (0.0-2.0)</li> <li><code>gpt-4o</code> - Supports custom temperature (0.0-2.0)</li> <li><code>gpt-4-turbo</code> - Supports custom temperature (0.0-2.0)</li> <li><code>gpt-3.5-turbo</code> - Supports custom temperature (0.0-2.0)</li> <li>Most GPT-4 and earlier models</li> </ul>"},{"location":"NEW_MODEL_COMPATIBILITY/#fixed-temperature-models-gpt-5-series","title":"Fixed Temperature Models (GPT-5 Series)","text":"<p>These models are LIVE as of November 2025: - <code>gpt-5-mini</code> - Requires temperature=1.0 (default) - Cannot customize - <code>gpt-5-nano</code> - Requires temperature=1.0 (default) - Cannot customize</p> <p>The OpenAI client automatically detects these models and excludes the temperature parameter.</p>"},{"location":"NEW_MODEL_COMPATIBILITY/#testing-new-models","title":"Testing New Models","text":"<p>Always test new models with a simple configuration before adding them to production configs. Future GPT-5 models may have similar restrictions.</p>"},{"location":"NEW_MODEL_COMPATIBILITY/#error-handling","title":"Error Handling","text":""},{"location":"NEW_MODEL_COMPATIBILITY/#common-errors-and-solutions","title":"Common Errors and Solutions","text":""},{"location":"NEW_MODEL_COMPATIBILITY/#1-temperature-parameter-error","title":"1. Temperature Parameter Error","text":"<p><pre><code>Unsupported value: 'temperature' does not support 0.7 with this model. Only the default (1) value is supported.\n</code></pre> Solution: Add model to <code>TEMPERATURE_FIXED_MODELS</code></p>"},{"location":"NEW_MODEL_COMPATIBILITY/#2-model-not-found","title":"2. Model Not Found","text":"<p><pre><code>Model not found: gpt-new-model\n</code></pre> Solution: Model may not be available yet or requires special access</p>"},{"location":"NEW_MODEL_COMPATIBILITY/#3-api-timeouts","title":"3. API Timeouts","text":"<p><pre><code>The read operation timed out\n</code></pre> Solution: Model may be in preview, slow, or unavailable</p>"},{"location":"NEW_MODEL_COMPATIBILITY/#best-practices","title":"Best Practices","text":"<ol> <li>Test First: Always test new models with simple configs</li> <li>Monitor Costs: New models may have different pricing</li> <li>Check Performance: Response times can vary significantly</li> <li>Review Documentation: Check OpenAI's API docs for model-specific requirements</li> <li>Gradual Rollout: Test in development before production use</li> </ol>"},{"location":"NEW_MODEL_COMPATIBILITY/#future-enhancements","title":"Future Enhancements","text":"<p>Potential improvements for model compatibility:</p> <ol> <li>Dynamic Model Detection: Automatically detect model capabilities</li> <li>API Response Parsing: Parse API responses to determine supported parameters</li> <li>Configuration Validation: Validate model availability before runs</li> <li>Model-Specific Cost Calculation: Different pricing for different models</li> <li>Fallback Mechanisms: Gracefully fallback to compatible models</li> </ol>"},{"location":"NEW_MODEL_COMPATIBILITY/#troubleshooting","title":"Troubleshooting","text":"<p>If a new model doesn't work:</p> <ol> <li>Check the error logs for specific parameter issues</li> <li>Verify the model name spelling and availability</li> <li>Test with a minimal configuration</li> <li>Check OpenAI's status page for model availability</li> <li>Ensure your API key has access to the new model</li> </ol>"},{"location":"NEW_MODEL_COMPATIBILITY/#contributing","title":"Contributing","text":"<p>When adding support for new models:</p> <ol> <li>Update the <code>TEMPERATURE_FIXED_MODELS</code> set if needed</li> <li>Add any model-specific parameters to <code>MODEL_PARAMS</code></li> <li>Update test cases</li> <li>Document any special requirements</li> <li>Test with the complete configuration</li> </ol>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to LLM Answer Watcher will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#planned","title":"Planned","text":"<ul> <li>Additional browser runners (Claude, Gemini web UIs)</li> <li>Enhanced cost tracking for browser runners</li> <li>DeepEval integration for quality metrics</li> <li>Trends command for historical analysis</li> </ul>"},{"location":"changelog/#020-2025-11-08","title":"[0.2.0] - 2025-11-08","text":""},{"location":"changelog/#added-major-features","title":"Added - Major Features","text":"<ul> <li>\ud83c\udf10 Browser Runners (BETA): Steel API integration for web-based LLM interfaces</li> <li>ChatGPT web UI runner with session management</li> <li>Perplexity web UI runner with citation extraction</li> <li>Screenshot capture and HTML snapshot support</li> <li>Session reuse for cost optimization</li> <li>Plugin system for extensible browser automation</li> <li> <p>See Browser Runners Guide for details</p> </li> <li> <p>\u26a1 Async/Await Parallelization: 3-4x performance improvement</p> </li> <li>Parallel query execution across models</li> <li>Async progress callbacks</li> <li> <p>RuntimeWarning fixes for async operations</p> </li> <li> <p>\ud83d\udd0d Google Search Grounding: Enhanced Gemini model support</p> </li> <li>Google Search grounding for Gemini models</li> <li>Accurate web search cost calculation</li> <li> <p>Grounded responses with citations</p> </li> <li> <p>\ud83c\udfaf Post-Intent Operations: Dynamic workflow support</p> </li> <li>Configurable operations to run after each intent</li> <li>Operation models with validation</li> <li>Config filename tracking in reports</li> <li> <p>Model capability detection</p> </li> <li> <p>\ud83d\udcca Advanced Analysis Features:</p> </li> <li>Sentiment Analysis: Analyze tone (positive/neutral/negative) and context of each brand mention</li> <li>Intent Classification: Classify user queries by intent type, buyer journey stage, and urgency signals<ul> <li>Intent types: transactional, informational, navigational, commercial_investigation</li> <li>Buyer stages: awareness, consideration, decision</li> <li>Urgency signals: high, medium, low</li> <li>Confidence scoring and reasoning explanations</li> </ul> </li> <li>Brand visibility score in reports</li> <li> <p>HTML report filtering and web search badges</p> </li> <li> <p>\ud83d\udcda Documentation Expansion:</p> </li> <li>Comprehensive MkDocs documentation with Material theme (60+ pages)</li> <li>Browser runners guide with Steel integration</li> <li>Google Search grounding documentation</li> <li>44 example configurations across 8 directories</li> </ul>"},{"location":"changelog/#added-database-storage","title":"Added - Database &amp; Storage","text":"<ul> <li>New database tables and columns for sentiment and intent data</li> <li><code>mentions</code> table: <code>sentiment</code> and <code>mention_context</code> columns</li> <li><code>intent_classifications</code> table with query hash caching</li> <li>5 new indexes for filtering by sentiment, context, intent type, buyer stage, and urgency</li> <li>SQLite schema version 5 (migration support included)</li> </ul>"},{"location":"changelog/#added-configuration","title":"Added - Configuration","text":"<ul> <li>Configuration options: <code>enable_sentiment_analysis</code> and <code>enable_intent_classification</code> (both default true)</li> <li>Runner plugin configuration system</li> <li>Browser runner specific settings (Steel API, screenshots, sessions)</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Breaking: Configuration format updated to support runner plugins</li> <li>Improved test coverage to 100% for core modules</li> <li>Enhanced error messages for better debugging</li> <li>Function calling extraction schema expanded with sentiment/context fields</li> <li>Correct Responses API format with required type field</li> <li>Improved validation, error handling, and config validation</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Database schema mismatches and exception handling in CLI</li> <li>Rank display in HTML reports (shows actual positions not match positions)</li> <li>GPT-4.1 model support in OpenAI client</li> <li>Code review findings (validation, error handling, config)</li> <li>RuntimeWarnings for async operations</li> <li>Indentation in runner loop to process all models</li> </ul>"},{"location":"changelog/#cost-impact","title":"Cost Impact","text":"<ul> <li>Intent classification: ~$0.00012 per query (one-time per unique query, cached)</li> <li>Sentiment extraction: ~33% increase per extraction call (integrated into function calling)</li> <li>Browser runners: $0.10-0.30/hour via Steel (not yet tracked in cost estimates)</li> </ul>"},{"location":"changelog/#known-limitations-v020","title":"Known Limitations (v0.2.0)","text":"<ul> <li>Browser runner cost tracking returns $0.00 (placeholder - actual Steel costs not calculated)</li> <li>Browser runners are BETA quality (added Nov 8, 2025)</li> <li>CSS selectors for browser runners may break if web UIs change</li> <li>No authentication handling documented for ChatGPT login</li> <li>Response completion detection is heuristic-based</li> </ul>"},{"location":"changelog/#010-2025-11-05","title":"[0.1.0] - 2025-11-05","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Initial release of LLM Answer Watcher</li> <li>Multi-provider support: OpenAI, Anthropic, Mistral, X.AI Grok, Google Gemini, Perplexity</li> <li>Brand mention detection with word-boundary matching</li> <li>Rank extraction (pattern-based and LLM-assisted)</li> <li>SQLite database for historical tracking</li> <li>HTML report generation with Jinja2</li> <li>Dual-mode CLI (human-friendly Rich output, structured JSON for automation)</li> <li>Budget controls and cost estimation</li> <li>Dynamic pricing from llm-prices.com with 24-hour caching</li> <li>Web search cost calculation for OpenAI models</li> <li>Retry logic with exponential backoff</li> <li>Evaluation framework for extraction accuracy</li> <li>Configuration validation with Pydantic</li> <li>Exit codes for automation (0-4)</li> <li>Example configurations</li> <li>Comprehensive test suite (750+ tests)</li> <li>GitHub Actions CI/CD pipeline</li> </ul>"},{"location":"changelog/#core-modules","title":"Core Modules","text":"<ul> <li><code>config/</code>: YAML loading and Pydantic validation</li> <li><code>llm_runner/</code>: Multi-provider LLM client abstraction</li> <li><code>extractor/</code>: Brand mention detection and rank extraction</li> <li><code>storage/</code>: SQLite schema and JSON writers</li> <li><code>report/</code>: HTML report generation</li> <li><code>utils/</code>: Time utilities, logging, cost estimation, Rich console</li> <li><code>evals/</code>: Evaluation framework</li> </ul>"},{"location":"changelog/#supported-models","title":"Supported Models","text":"<ul> <li>OpenAI: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo</li> <li>Anthropic: claude-3-5-sonnet, claude-3-5-haiku, claude-3-opus</li> <li>Mistral: mistral-large-latest, mistral-small-latest</li> <li>X.AI: grok-beta, grok-2-1212, grok-2-latest, grok-3</li> <li>Google: gemini-2.0-flash-exp, gemini-1.5-pro, gemini-1.5-flash</li> <li>Perplexity: sonar, sonar-pro, sonar-reasoning</li> </ul>"},{"location":"changelog/#documentation","title":"Documentation","text":"<ul> <li>README with quick start and examples</li> <li>CLAUDE.md with development guidelines</li> <li>CONTRIBUTING.md with contribution guidelines</li> <li>SPECS.md with complete engineering specification</li> <li>TODO.md with milestone tracking</li> </ul>"},{"location":"changelog/#security","title":"Security","text":"<ul> <li>Environment variable-based API key management</li> <li>SQL injection prevention (parameterized queries)</li> <li>XSS prevention (Jinja2 autoescaping)</li> <li>No API key logging</li> </ul>"},{"location":"changelog/#release-notes","title":"Release Notes","text":""},{"location":"changelog/#version-010-production-ready","title":"Version 0.1.0 - Production Ready","text":"<p>This is the first production-ready release of LLM Answer Watcher. The tool is feature-complete for core brand monitoring use cases:</p> <p>Highlights: - \u2705 8,200 lines of production Python code - \u2705 17,400 lines of test code (750+ tests) - \u2705 100% coverage on critical paths - \u2705 6 LLM providers supported - \u2705 Complete evaluation framework - \u2705 Full documentation</p> <p>What's Working: - All core features tested and validated - Multi-provider queries with retry logic - Accurate brand mention detection (90%+ precision) - Historical tracking in SQLite - Professional HTML reports - Budget protection - CI/CD integration</p> <p>Known Limitations (v0.1.0 - resolved in v0.2.0): - No async support (intentionally - keeping it simple) - ADDED in v0.2.0 - Web search only for OpenAI models - Google Search grounding added in v0.2.0 - Perplexity request fees not yet in cost estimates - Trends command not yet implemented (data collection works)</p> <p>Upgrade Notes: - This is the initial release - no upgrades needed - SQLite schema version 1 - Configuration format stable</p>"},{"location":"changelog/#future-roadmap","title":"Future Roadmap","text":""},{"location":"changelog/#planned-features","title":"Planned Features","text":"<p>v0.2.0 - \u2705 RELEASED 2025-11-08: - \u2705 Async support for parallel queries (3-4x faster) - \u2705 Enhanced web search support (Google Search grounding) - \u2705 Browser runners (BETA) - \u23f3 <code>trends</code> command for historical analysis (moved to v0.3.0) - \u23f3 Dashboard UI for visualizing trends (moved to v0.3.0) - \u23f3 DeepEval integration for quality metrics (moved to v0.3.0)</p> <p>v0.3.0 (Q1 2025): - <code>trends</code> command for historical analysis - Dashboard UI for visualizing trends - DeepEval integration for quality metrics - Production-ready browser runners (cost tracking, authentication) - Additional browser runners (Claude, Gemini web UIs) - Cloud deployment option - HTTP API (expose internal contract) - Real-time alerts and webhooks - Advanced analytics and insights - Multi-user support</p> <p>v1.0.0 (Q3 2025): - Enterprise features - Advanced provider integrations - Custom model support - White-label options - SaaS offering</p>"},{"location":"changelog/#contributing","title":"Contributing","text":"<p>We welcome contributions! See CONTRIBUTING.md for guidelines.</p>"},{"location":"changelog/#links","title":"Links","text":"<ul> <li>Repository: github.com/nibzard/llm-answer-watcher</li> <li>Issues: github.com/nibzard/llm-answer-watcher/issues</li> <li>Documentation: This site</li> <li>License: MIT</li> </ul>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#general","title":"General","text":""},{"location":"faq/#what-is-llm-answer-watcher","title":"What is LLM Answer Watcher?","text":"<p>LLM Answer Watcher is a CLI tool that monitors how large language models (like ChatGPT, Claude) talk about your brand versus competitors when answering buyer-intent queries.</p>"},{"location":"faq/#why-should-i-use-this","title":"Why should I use this?","text":"<p>As AI-powered search becomes mainstream (ChatGPT, Perplexity, Google AI Overview), understanding your brand's presence in LLM responses is crucial for:</p> <ul> <li>Brand visibility tracking</li> <li>Competitive intelligence</li> <li>SEO for the AI era</li> <li>Market positioning</li> </ul>"},{"location":"faq/#is-it-free","title":"Is it free?","text":"<p>The tool is open source (MIT license) and free to use. However, you pay for:</p> <ul> <li>LLM API calls (typically \\(0.001-\\)0.01 per query)</li> <li>Your own compute resources</li> </ul>"},{"location":"faq/#how-much-does-it-cost-to-run","title":"How much does it cost to run?","text":"<p>Example costs per run:</p> <ul> <li>3 intents \u00d7 1 model (gpt-4o-mini): ~$0.006</li> <li>5 intents \u00d7 2 models: ~$0.020</li> <li>10 intents \u00d7 5 models: ~$0.150</li> </ul> <p>See Cost Management for details.</p>"},{"location":"faq/#installation-setup","title":"Installation &amp; Setup","text":""},{"location":"faq/#what-python-version-do-i-need","title":"What Python version do I need?","text":"<p>Python 3.12 or 3.13 is required. The tool uses modern Python features.</p>"},{"location":"faq/#can-i-use-pip-instead-of-uv","title":"Can I use pip instead of uv?","text":"<p>Yes! Both work:</p> <pre><code># With uv (recommended - faster)\nuv sync\n\n# With pip (traditional)\npip install -e .\n</code></pre>"},{"location":"faq/#which-llm-providers-are-supported","title":"Which LLM providers are supported?","text":"<ul> <li>OpenAI (GPT models)</li> <li>Anthropic (Claude models)</li> <li>Mistral AI</li> <li>X.AI (Grok models)</li> <li>Google (Gemini models)</li> <li>Perplexity</li> </ul> <p>See Providers for complete list.</p>"},{"location":"faq/#do-i-need-api-keys-for-all-providers","title":"Do I need API keys for all providers?","text":"<p>No! You only need API keys for providers you want to use. Start with just OpenAI if you want.</p>"},{"location":"faq/#configuration","title":"Configuration","text":""},{"location":"faq/#how-do-i-create-a-configuration-file","title":"How do I create a configuration file?","text":"<p>See Basic Configuration. Minimum config:</p> <pre><code>run_settings:\n  output_dir: \"./output\"\n  models:\n    - provider: \"openai\"\n      model_name: \"gpt-4o-mini\"\n      env_api_key: \"OPENAI_API_KEY\"\n\nbrands:\n  mine: [\"YourBrand\"]\n  competitors: [\"CompetitorA\"]\n\nintents:\n  - id: \"best-tools\"\n    prompt: \"What are the best tools?\"\n</code></pre>"},{"location":"faq/#how-many-brands-should-i-track","title":"How many brands should I track?","text":"<p>Your brands: Include all variations (e.g., \"HubSpot\", \"HubSpot CRM\", \"hubspot.com\")</p> <p>Competitors: Start with top 5-10 direct competitors. You can always add more.</p>"},{"location":"faq/#what-makes-a-good-intent-prompt","title":"What makes a good intent prompt?","text":"<p>Good prompts are:</p> <ul> <li>Natural: How real users ask</li> <li>Buyer-intent: Imply evaluation/purchase</li> <li>Specific: Target a use case</li> </ul> <p>Examples:</p> <ul> <li>\u2705 \"What are the best email warmup tools for startups?\"</li> <li>\u274c \"Tell me about email\"</li> </ul>"},{"location":"faq/#can-i-use-the-same-config-for-multiple-runs","title":"Can I use the same config for multiple runs?","text":"<p>Yes! Configs are reusable. All data is timestamped and stored separately.</p>"},{"location":"faq/#usage","title":"Usage","text":""},{"location":"faq/#why-arent-my-brands-being-detected","title":"Why aren't my brands being detected?","text":"<p>Common causes:</p> <ol> <li>Name mismatch: LLM used \"HubSpot CRM\" but you only configured \"HubSpot\"</li> <li> <p>Solution: Add all brand variations</p> </li> <li> <p>Brand not mentioned: LLM didn't include your brand</p> </li> <li> <p>Solution: This is valuable data! Your brand isn't top-of-mind for that query</p> </li> <li> <p>Word boundary issue: \"Hub\" won't match in \"GitHub\"</p> </li> <li>Solution: This is intentional to prevent false positives</li> </ol>"},{"location":"faq/#how-do-i-track-historical-trends","title":"How do I track historical trends?","text":"<p>All data is stored in SQLite at <code>./output/watcher.db</code>:</p> <pre><code>SELECT DATE(timestamp_utc), AVG(rank_position)\nFROM mentions\nWHERE normalized_name = 'yourbrand'\nGROUP BY DATE(timestamp_utc);\n</code></pre> <p>See Data Analytics.</p>"},{"location":"faq/#can-i-run-this-in-cicd","title":"Can I run this in CI/CD?","text":"<p>Yes! Use <code>--yes --format json</code> for automation:</p> <pre><code>llm-answer-watcher run --config config.yaml --yes --format json\n</code></pre> <p>See Automation Guide.</p>"},{"location":"faq/#what-are-the-exit-codes","title":"What are the exit codes?","text":"<ul> <li><code>0</code>: Success</li> <li><code>1</code>: Configuration error</li> <li><code>2</code>: Database error</li> <li><code>3</code>: Partial failure (acceptable)</li> <li><code>4</code>: Complete failure</li> </ul> <p>See Exit Codes.</p>"},{"location":"faq/#features","title":"Features","text":""},{"location":"faq/#whats-the-difference-between-regex-and-llm-extraction","title":"What's the difference between regex and LLM extraction?","text":"<p>Regex extraction (default): - Fast and cheap - Pattern-based matching - 90%+ accuracy</p> <p>LLM extraction (<code>use_llm_rank_extraction: true</code>): - More accurate for complex cases - Costs extra (additional LLM calls) - 95%+ accuracy</p> <p>Start with regex. Only use LLM if needed.</p>"},{"location":"faq/#what-is-function-calling","title":"What is function calling?","text":"<p>Function calling uses LLM's built-in structured output feature for extraction. More accurate than regex.</p> <p>Enable it:</p> <pre><code>extraction_settings:\n  method: \"function_calling\"\n  extraction_model:\n    provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n</code></pre> <p>See Function Calling.</p>"},{"location":"faq/#how-do-budget-controls-work","title":"How do budget controls work?","text":"<p>Set spending limits:</p> <pre><code>budget:\n  enabled: true\n  max_per_run_usd: 1.00\n  max_per_intent_usd: 0.10\n</code></pre> <p>Tool validates before running and aborts if estimated cost exceeds limits.</p> <p>See Budget Controls.</p>"},{"location":"faq/#can-i-use-web-search","title":"Can I use web search?","text":"<p>Yes, but it increases costs significantly (\\(10-\\)25 per 1,000 calls):</p> <pre><code>web_search:\n  enabled: true\n  max_results: 10\n</code></pre> <p>See Web Search.</p>"},{"location":"faq/#data-privacy","title":"Data &amp; Privacy","text":""},{"location":"faq/#where-is-my-data-stored","title":"Where is my data stored?","text":"<p>Locally on your machine:</p> <ul> <li>SQLite database: <code>./output/watcher.db</code></li> <li>JSON files: <code>./output/YYYY-MM-DDTHH-MM-SSZ/</code></li> <li>HTML reports: <code>./output/YYYY-MM-DDTHH-MM-SSZ/report.html</code></li> </ul> <p>No data leaves your machine except LLM API calls.</p>"},{"location":"faq/#is-my-data-sent-anywhere","title":"Is my data sent anywhere?","text":"<p>Only to configured LLM providers (OpenAI, Anthropic, etc.) for query processing. We don't collect any data.</p>"},{"location":"faq/#are-api-keys-secure","title":"Are API keys secure?","text":"<p>API keys are:</p> <ul> <li>Loaded from environment variables</li> <li>Never logged or written to disk</li> <li>Never sent anywhere except the respective LLM provider</li> </ul> <p>See Security.</p>"},{"location":"faq/#can-i-delete-old-data","title":"Can I delete old data?","text":"<p>Yes! Simply delete directories or database records:</p> <pre><code># Delete runs older than 90 days\nfind output/ -name \"2024-*\" -type d -mtime +90 -exec rm -rf {} +\n</code></pre>"},{"location":"faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"faq/#configuration-error-api-key-not-found","title":"\"Configuration error: API key not found\"","text":"<p>Solution:</p> <pre><code># Check if key is set\necho $OPENAI_API_KEY\n\n# If empty, export it\nexport OPENAI_API_KEY=sk-your-key-here\n</code></pre>"},{"location":"faq/#rate-limit-exceeded","title":"\"Rate limit exceeded\"","text":"<p>Solution: LLM provider rate limit hit. Options:</p> <ol> <li>Wait and retry</li> <li>Reduce number of queries</li> <li>Use slower model tiers</li> <li>Upgrade API plan</li> </ol>"},{"location":"faq/#no-brands-detected","title":"\"No brands detected\"","text":"<p>Causes:</p> <ol> <li>Brand not mentioned by LLM</li> <li>Brand name mismatch (add aliases)</li> <li>Case sensitivity (should work - file a bug)</li> </ol>"},{"location":"faq/#database-locked","title":"\"Database locked\"","text":"<p>Solution: Another process is using the database:</p> <pre><code># Find process\nlsof output/watcher.db\n\n# Kill if needed\nkill -9 &lt;PID&gt;\n</code></pre>"},{"location":"faq/#buildimport-errors","title":"Build/Import Errors","text":"<p>Solution:</p> <pre><code># Reinstall\npip install -e .\n\n# Check Python version\npython --version  # Should be 3.12+\n</code></pre>"},{"location":"faq/#advanced","title":"Advanced","text":""},{"location":"faq/#can-i-extend-it-with-new-providers","title":"Can I extend it with new providers?","text":"<p>Yes! See Extending Providers.</p>"},{"location":"faq/#can-i-customize-system-prompts","title":"Can I customize system prompts?","text":"<p>Yes! See Custom System Prompts.</p>"},{"location":"faq/#is-there-a-python-api","title":"Is there a Python API?","text":"<p>Yes! See Python API Reference.</p>"},{"location":"faq/#can-i-contribute","title":"Can I contribute?","text":"<p>Absolutely! See Contributing Guide.</p>"},{"location":"faq/#still-have-questions","title":"Still Have Questions?","text":"<ul> <li>GitHub Issues: Report bugs or ask questions</li> <li>Documentation: Browse this site</li> <li>Examples: Check <code>examples/</code> directory in the repository</li> </ul>"},{"location":"advanced/api-contract/","title":"API Contract","text":"<p>Internal API designed for future HTTP exposure.</p>"},{"location":"advanced/api-contract/#core-contract","title":"Core Contract","text":"<pre><code>def run_all(config: RuntimeConfig) -&gt; dict:\n    \"\"\"\n    Execute monitoring run.\n\n    Args:\n        config: Validated runtime configuration\n\n    Returns:\n        {\n            \"run_id\": \"YYYY-MM-DDTHH-MM-SSZ\",\n            \"status\": \"success\" | \"partial\" | \"failed\",\n            \"queries_completed\": int,\n            \"queries_failed\": int,\n            \"total_cost_usd\": float,\n            \"output_dir\": str,\n            \"brands_detected\": {...}\n        }\n    \"\"\"\n</code></pre>"},{"location":"advanced/api-contract/#future-http-api","title":"Future HTTP API","text":"<p>The internal contract is designed to become an HTTP API:</p> <pre><code>POST /api/v1/run\nContent-Type: application/json\n\n{\n  \"config\": {...},\n  \"return_format\": \"json\"\n}\n</code></pre>"},{"location":"advanced/api-contract/#provider-interface","title":"Provider Interface","text":"<pre><code>@dataclass\nclass LLMResponse:\n    answer_text: str\n    tokens_used: int\n    cost_usd: float\n    provider: str\n    model_name: str\n    timestamp_utc: str\n</code></pre> <p>See Architecture for overall design.</p>"},{"location":"advanced/architecture/","title":"Architecture","text":"<p>LLM Answer Watcher follows Domain-Driven Design principles with strict separation of concerns.</p>"},{"location":"advanced/architecture/#core-domains","title":"Core Domains","text":"<pre><code>llm_answer_watcher/\n\u251c\u2500\u2500 config/         # Configuration loading and validation\n\u251c\u2500\u2500 llm_runner/     # LLM client abstraction\n\u251c\u2500\u2500 extractor/      # Brand mention detection\n\u251c\u2500\u2500 storage/        # SQLite and JSON persistence\n\u251c\u2500\u2500 report/         # HTML report generation\n\u251c\u2500\u2500 utils/          # Shared utilities\n\u2514\u2500\u2500 cli.py          # CLI interface\n</code></pre>"},{"location":"advanced/architecture/#design-patterns","title":"Design Patterns","text":""},{"location":"advanced/architecture/#1-provider-abstraction","title":"1. Provider Abstraction","text":"<pre><code>class LLMClient(Protocol):\n    def generate_answer(self, prompt: str) -&gt; LLMResponse:\n        ...\n\ndef build_client(provider: str, model: str) -&gt; LLMClient:\n    ...\n</code></pre>"},{"location":"advanced/architecture/#2-api-first-contract","title":"2. API-First Contract","text":"<pre><code>def run_all(config: RuntimeConfig) -&gt; dict:\n    # Internal \"POST /run\" contract\n    # OSS CLI calls in-process\n    # Cloud will expose over HTTP\n    return {\"run_id\": \"...\", \"cost_usd\": 0.01}\n</code></pre>"},{"location":"advanced/architecture/#3-dual-mode-cli","title":"3. Dual-Mode CLI","text":"<pre><code>class OutputMode(Enum):\n    HUMAN = \"human\"  # Rich formatting\n    JSON = \"json\"    # Structured output\n    QUIET = \"quiet\"  # Minimal output\n</code></pre> <p>See API Contract for internal API details.</p>"},{"location":"advanced/custom-system-prompts/","title":"Custom System Prompts","text":"<p>Customize system prompts for LLM queries.</p>"},{"location":"advanced/custom-system-prompts/#built-in-prompts","title":"Built-in Prompts","text":"<p>Located in <code>llm_answer_watcher/system_prompts/</code>:</p> <pre><code>system_prompts/\n\u251c\u2500\u2500 openai/\n\u2502   \u251c\u2500\u2500 gpt-4-default.json\n\u2502   \u2514\u2500\u2500 extraction-default.json\n\u251c\u2500\u2500 anthropic/\n\u2502   \u2514\u2500\u2500 default.json\n\u2514\u2500\u2500 mistral/\n    \u2514\u2500\u2500 default.json\n</code></pre>"},{"location":"advanced/custom-system-prompts/#using-custom-prompts","title":"Using Custom Prompts","text":"<pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    system_prompt: \"openai/custom-prompt\"\n</code></pre>"},{"location":"advanced/custom-system-prompts/#creating-custom-prompts","title":"Creating Custom Prompts","text":"<ol> <li>Create JSON file in <code>system_prompts/provider/</code>:</li> </ol> <pre><code>{\n  \"role\": \"system\",\n  \"content\": \"You are a helpful assistant focused on...\"\n}\n</code></pre> <ol> <li>Reference in config:</li> </ol> <pre><code>system_prompt: \"openai/custom-prompt\"\n</code></pre>"},{"location":"advanced/custom-system-prompts/#prompt-guidelines","title":"Prompt Guidelines","text":"<ul> <li>Keep prompts neutral (avoid biasing toward your brand)</li> <li>Be concise yet comprehensive</li> <li>Test with evaluation framework</li> </ul> <p>See API Contract for technical details.</p>"},{"location":"advanced/extending-providers/","title":"Extending Providers","text":"<p>Add support for new LLM providers.</p>"},{"location":"advanced/extending-providers/#provider-interface","title":"Provider Interface","text":"<pre><code>from llm_answer_watcher.llm_runner.models import LLMClient, LLMResponse\n\nclass MyCustomClient:\n    def __init__(self, model_name: str, api_key: str, system_prompt: str):\n        self.model = model_name\n        self.api_key = api_key\n        self.system_prompt = system_prompt\n\n    def generate_answer(self, prompt: str) -&gt; LLMResponse:\n        # Call your LLM API\n        response = call_my_llm_api(prompt)\n\n        return LLMResponse(\n            answer_text=response.text,\n            tokens_used=response.tokens,\n            cost_usd=calculate_cost(response),\n            provider=\"my_provider\",\n            model_name=self.model,\n            timestamp_utc=utc_timestamp()\n        )\n</code></pre>"},{"location":"advanced/extending-providers/#registering-provider","title":"Registering Provider","text":"<pre><code># llm_runner/models.py\ndef build_client(provider: str, model_name: str, ...) -&gt; LLMClient:\n    if provider == \"my_provider\":\n        return MyCustomClient(...)\n    # ...\n</code></pre>"},{"location":"advanced/extending-providers/#testing-your-provider","title":"Testing Your Provider","text":"<pre><code>def test_my_provider(httpx_mock):\n    httpx_mock.add_response(...)\n    client = MyCustomClient(...)\n    response = client.generate_answer(\"test\")\n    assert response.provider == \"my_provider\"\n</code></pre> <p>See Architecture for design patterns.</p>"},{"location":"advanced/performance/","title":"Performance","text":"<p>Optimizing LLM Answer Watcher for speed and efficiency.</p>"},{"location":"advanced/performance/#query-performance","title":"Query Performance","text":""},{"location":"advanced/performance/#parallel-queries-future","title":"Parallel Queries (Future)","text":"<p>Currently synchronous. Async support planned:</p> <pre><code># Future: parallel execution\nawait asyncio.gather(*[\n    query_model(intent, model)\n    for intent in intents\n    for model in models\n])\n</code></pre>"},{"location":"advanced/performance/#current-sequential","title":"Current: Sequential","text":"<pre><code># Current: one at a time\nfor intent in intents:\n    for model in models:\n        query_model(intent, model)\n</code></pre>"},{"location":"advanced/performance/#cost-optimization","title":"Cost Optimization","text":""},{"location":"advanced/performance/#use-cheaper-models","title":"Use Cheaper Models","text":"<pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"  # $0.15/1M vs $2.50/1M\n</code></pre>"},{"location":"advanced/performance/#regex-vs-llm-extraction","title":"Regex vs LLM Extraction","text":"<pre><code># Fast and cheap (recommended)\nuse_llm_rank_extraction: false\n\n# Accurate but costly\nuse_llm_rank_extraction: true\n</code></pre>"},{"location":"advanced/performance/#database-performance","title":"Database Performance","text":""},{"location":"advanced/performance/#indexes","title":"Indexes","text":"<p>SQLite indexes on: - <code>timestamp_utc</code> - <code>intent_id</code> - <code>brand</code> - <code>rank_position</code></p>"},{"location":"advanced/performance/#vacuum","title":"Vacuum","text":"<p>Periodically compact database:</p> <pre><code>sqlite3 output/watcher.db \"VACUUM;\"\n</code></pre>"},{"location":"advanced/performance/#caching","title":"Caching","text":""},{"location":"advanced/performance/#pricing-cache","title":"Pricing Cache","text":"<p>LLM prices cached for 24 hours to reduce API calls.</p>"},{"location":"advanced/performance/#future-caching","title":"Future Caching","text":"<p>Planned: - Response caching (identical queries) - Extracted data caching</p> <p>See Architecture for design details.</p>"},{"location":"advanced/security/","title":"Security","text":"<p>Security best practices for LLM Answer Watcher.</p>"},{"location":"advanced/security/#api-key-management","title":"API Key Management","text":""},{"location":"advanced/security/#do-this","title":"\u2705 Do This","text":"<pre><code># Use environment variables\nexport OPENAI_API_KEY=sk-your-key\n\n# Use secrets management\nOPENAI_API_KEY=$(aws secretsmanager get-secret-value ...)\n\n# Use .env files (add to .gitignore)\necho \"OPENAI_API_KEY=sk-...\" &gt; .env\necho \".env\" &gt;&gt; .gitignore\n</code></pre>"},{"location":"advanced/security/#dont-do-this","title":"\u274c Don't Do This","text":"<pre><code># NEVER hardcode API keys in config files\nmodels:\n  - provider: \"openai\"\n    api_key: \"sk-hardcoded-key\"  # DON'T DO THIS!\n</code></pre>"},{"location":"advanced/security/#sql-injection-prevention","title":"SQL Injection Prevention","text":"<p>The tool uses parameterized queries:</p> <pre><code># \u2705 Safe - parameterized\ncursor.execute(\"SELECT * FROM runs WHERE id=?\", (run_id,))\n\n# \u274c Never done - string concatenation\ncursor.execute(f\"SELECT * FROM runs WHERE id='{run_id}'\")\n</code></pre>"},{"location":"advanced/security/#xss-prevention","title":"XSS Prevention","text":"<p>Jinja2 autoescaping enabled:</p> <pre><code># \u2705 Safe - autoescaping on\nenv = Environment(loader=..., autoescape=True)\n</code></pre>"},{"location":"advanced/security/#best-practices","title":"Best Practices","text":"<ol> <li>Never commit secrets</li> <li>Rotate API keys regularly</li> <li>Use read-only file permissions for configs</li> <li>Review logs before sharing</li> <li>Keep dependencies updated</li> </ol>"},{"location":"advanced/security/#reporting-security-issues","title":"Reporting Security Issues","text":"<p>Email: [security contact] (replace with actual contact)</p> <p>See Contributing.</p>"},{"location":"contributing/code-standards/","title":"Code Standards","text":"<p>Coding standards and best practices.</p>"},{"location":"contributing/code-standards/#python-style","title":"Python Style","text":""},{"location":"contributing/code-standards/#modern-type-hints-python-312","title":"Modern Type Hints (Python 3.12+)","text":"<pre><code># \u2705 Good - use | for unions\ndef process(data: dict | None = None) -&gt; str | None:\n    pass\n\n# \u274c Bad - old style\nfrom typing import Union, Optional\ndef process(data: Optional[dict] = None) -&gt; Union[str, None]:\n    pass\n</code></pre>"},{"location":"contributing/code-standards/#docstrings","title":"Docstrings","text":"<pre><code>def detect_mentions(text: str, brands: list[str]) -&gt; list[Mention]:\n    \"\"\"\n    Detect brand mentions in text.\n\n    Args:\n        text: Text to search\n        brands: List of brand names\n\n    Returns:\n        List of detected mentions\n    \"\"\"\n</code></pre>"},{"location":"contributing/code-standards/#word-boundaries","title":"Word Boundaries","text":"<pre><code># \u2705 Good - word boundary matching\npattern = r'\\b' + re.escape(brand) + r'\\b'\n\n# \u274c Bad - substring matching\nif brand.lower() in text.lower():\n    ...\n</code></pre>"},{"location":"contributing/code-standards/#testing","title":"Testing","text":""},{"location":"contributing/code-standards/#coverage-requirements","title":"Coverage Requirements","text":"<ul> <li>Core modules: 80%+ coverage</li> <li>Critical paths: 100% coverage</li> </ul>"},{"location":"contributing/code-standards/#test-structure","title":"Test Structure","text":"<pre><code>def test_feature():\n    # Arrange\n    config = create_test_config()\n\n    # Act\n    result = run_feature(config)\n\n    # Assert\n    assert result.status == \"success\"\n</code></pre>"},{"location":"contributing/code-standards/#commits","title":"Commits","text":"<p>Use Conventional Commits:</p> <pre><code>feat: add new provider\nfix: correct rank extraction\ndocs: update README\ntest: add coverage for extractor\nchore: update dependencies\n</code></pre> <p>See Testing for testing guidelines.</p>"},{"location":"contributing/development-setup/","title":"Development Setup","text":"<p>Set up your development environment for contributing.</p>"},{"location":"contributing/development-setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.12 or 3.13</li> <li>Git</li> <li>uv or pip</li> </ul>"},{"location":"contributing/development-setup/#clone-and-install","title":"Clone and Install","text":"<pre><code># Clone repository\ngit clone https://github.com/nibzard/llm-answer-watcher.git\ncd llm-answer-watcher\n\n# Install with uv (recommended)\nuv sync --dev\n\n# Or with pip\npip install -e \".[dev]\"\n</code></pre>"},{"location":"contributing/development-setup/#development-tools","title":"Development Tools","text":""},{"location":"contributing/development-setup/#testing","title":"Testing","text":"<pre><code># Run all tests\npytest\n\n# Run with coverage\npytest --cov=llm_answer_watcher --cov-report=html\n\n# Run specific test\npytest tests/test_config_loader.py::test_load_valid_config\n</code></pre>"},{"location":"contributing/development-setup/#linting","title":"Linting","text":"<pre><code># Check code quality\nruff check .\n\n# Auto-fix issues\nruff check . --fix\n\n# Format code\nruff format .\n</code></pre>"},{"location":"contributing/development-setup/#documentation","title":"Documentation","text":"<pre><code># Build docs\nmkdocs build\n\n# Serve docs locally\nmkdocs serve\n</code></pre>"},{"location":"contributing/development-setup/#making-changes","title":"Making Changes","text":"<ol> <li>Create a branch: <code>git checkout -b feature/my-feature</code></li> <li>Make changes</li> <li>Run tests: <code>pytest</code></li> <li>Run linting: <code>ruff check .</code></li> <li>Commit: <code>git commit -m \"feat: add feature\"</code></li> <li>Push: <code>git push origin feature/my-feature</code></li> <li>Create Pull Request</li> </ol> <p>See Code Standards for coding guidelines.</p>"},{"location":"contributing/documentation/","title":"Documentation Guidelines","text":"<p>Contributing to documentation.</p>"},{"location":"contributing/documentation/#documentation-structure","title":"Documentation Structure","text":"<pre><code>docs/\n\u251c\u2500\u2500 index.md\n\u251c\u2500\u2500 getting-started/\n\u251c\u2500\u2500 user-guide/\n\u251c\u2500\u2500 providers/\n\u251c\u2500\u2500 examples/\n\u251c\u2500\u2500 data-analytics/\n\u251c\u2500\u2500 evaluation/\n\u251c\u2500\u2500 advanced/\n\u251c\u2500\u2500 reference/\n\u2514\u2500\u2500 contributing/\n</code></pre>"},{"location":"contributing/documentation/#writing-guidelines","title":"Writing Guidelines","text":""},{"location":"contributing/documentation/#style","title":"Style","text":"<ul> <li>Use clear, concise language</li> <li>Write in active voice</li> <li>Include code examples</li> <li>Add links to related pages</li> </ul>"},{"location":"contributing/documentation/#formatting","title":"Formatting","text":"<pre><code># Page Title\n\nBrief introduction paragraph.\n\n## Section Heading\n\nContent with examples:\n\n\\`\\`\\`python\n# Code example\nconfig = load_config(\"config.yaml\")\n\\`\\`\\`\n\n### Subsection\n\nMore detailed content.\n</code></pre>"},{"location":"contributing/documentation/#material-for-mkdocs-features","title":"Material for MkDocs Features","text":"<pre><code>!!! tip \"Pro Tip\"\n    Use this feature for better results.\n\n!!! warning\n    This operation costs money.\n\n=== \"Python\"\n    \\`\\`\\`python\n    import module\n    \\`\\`\\`\n\n=== \"Bash\"\n    \\`\\`\\`bash\n    command --flag\n    \\`\\`\\`\n</code></pre>"},{"location":"contributing/documentation/#building-docs","title":"Building Docs","text":"<pre><code># Install dependencies\nuv sync --dev\n\n# Build docs\nmkdocs build\n\n# Serve locally\nmkdocs serve\n\n# Open browser to http://localhost:8000\n</code></pre>"},{"location":"contributing/documentation/#previewing-changes","title":"Previewing Changes","text":"<pre><code>mkdocs serve --watch docs/\n</code></pre> <p>See Development Setup for environment setup.</p>"},{"location":"contributing/testing-utilities/","title":"Testing Utilities","text":"<p>LLM Answer Watcher provides specialized testing utilities to help you write reliable tests without making real API calls or dealing with brittle HTTP mocking.</p>"},{"location":"contributing/testing-utilities/#overview","title":"Overview","text":"<p>The testing utilities follow patterns inspired by modern LLM abstraction layers:</p> <ul> <li>MockLLMClient: Deterministic responses for testing extraction logic</li> <li>ChaosLLMClient: Resilience testing with controlled failure injection</li> <li>Protocol-based: Both implement the <code>LLMClient</code> protocol</li> </ul>"},{"location":"contributing/testing-utilities/#mockllmclient","title":"MockLLMClient","text":""},{"location":"contributing/testing-utilities/#basic-usage","title":"Basic Usage","text":"<p>The <code>MockLLMClient</code> provides deterministic responses without making real API calls:</p> <pre><code>from llm_answer_watcher.llm_runner.mock_client import MockLLMClient\n\n# Create client with configured responses\nclient = MockLLMClient(\n    responses={\n        \"What are the best CRM tools?\": \"HubSpot and Salesforce are leading CRM platforms.\",\n        \"best email warmup\": \"Warmly, HubSpot, and Instantly are top choices.\"\n    },\n    default_response=\"No specific answer available.\",\n    tokens_per_response=300,\n    cost_per_response=0.001\n)\n\n# Use in tests\nresponse = await client.generate_answer(\"What are the best CRM tools?\")\nassert response.answer_text == \"HubSpot and Salesforce are leading CRM platforms.\"\nassert response.tokens_used == 300\nassert response.cost_usd == 0.001\n</code></pre>"},{"location":"contributing/testing-utilities/#configuration-options","title":"Configuration Options","text":"<pre><code>MockLLMClient(\n    responses={\"prompt\": \"answer\"},  # Dict mapping prompts to answers\n    default_response=\"Default answer\",  # Fallback when prompt not found\n    model_name=\"mock-gpt-4\",  # Model name in responses\n    provider=\"mock-openai\",  # Provider name in responses\n    tokens_per_response=100,  # Token count to report\n    cost_per_response=0.0,  # Cost to report\n    streaming_chunk_size=None,  # Enable streaming (see below)\n    streaming_delay_ms=50  # Delay between chunks\n)\n</code></pre>"},{"location":"contributing/testing-utilities/#integration-testing","title":"Integration Testing","text":"<p>MockLLMClient works seamlessly with the extraction pipeline:</p> <pre><code>from llm_answer_watcher.config.schema import Brands\nfrom llm_answer_watcher.extractor.parser import parse_answer\n\n# Create mock client\nclient = MockLLMClient(\n    responses={\"best CRM\": \"1. HubSpot\\n2. Salesforce\\n3. Warmly\"}\n)\n\n# Generate answer\nresponse = await client.generate_answer(\"best CRM\")\n\n# Test extraction\nbrands = Brands(mine=[\"Warmly\"], competitors=[\"HubSpot\", \"Salesforce\"])\nextraction = parse_answer(response.answer_text, brands)\n\nassert extraction.appeared_mine is True\nassert len(extraction.my_mentions) == 1\nassert len(extraction.competitor_mentions) == 2\n</code></pre>"},{"location":"contributing/testing-utilities/#streaming-support","title":"Streaming Support","text":"<p>MockLLMClient supports optional streaming for testing streaming workflows:</p> <pre><code>chunks = []\n\nclient = MockLLMClient(\n    responses={\"test\": \"Hello world from LLM\"},\n    streaming_chunk_size=5,  # Stream in 5-char chunks\n    streaming_delay_ms=10  # 10ms delay between chunks\n)\n\nresponse = await client.generate_answer(\n    \"test\",\n    on_chunk=lambda chunk: chunks.append(chunk)\n)\n\n# Chunks received during streaming\nassert chunks == ['Hello', ' worl', 'd fro', 'm LLM']\n\n# Full response still returned\nassert response.answer_text == \"Hello world from LLM\"\n</code></pre>"},{"location":"contributing/testing-utilities/#chaosllmclient","title":"ChaosLLMClient","text":""},{"location":"contributing/testing-utilities/#basic-usage_1","title":"Basic Usage","text":"<p>The <code>ChaosLLMClient</code> wraps any <code>LLMClient</code> and probabilistically injects failures:</p> <pre><code>from llm_answer_watcher.llm_runner.chaos_client import ChaosLLMClient\n\n# Wrap a base client (e.g., MockLLMClient)\nbase = MockLLMClient(responses={\"test\": \"answer\"})\n\nchaos = ChaosLLMClient(\n    base_client=base,\n    success_rate=0.7,  # 70% success, 30% failure\n    rate_limit_prob=0.1,  # 10% chance of 429 error\n    server_error_prob=0.1,  # 10% chance of 5xx error\n    timeout_prob=0.05,  # 5% chance of timeout\n    auth_error_prob=0.05,  # 5% chance of 401 error\n    seed=42  # Optional: reproducible chaos\n)\n\n# May succeed or fail\ntry:\n    response = await chaos.generate_answer(\"test\")\n    print(\"Success!\")\nexcept RuntimeError as e:\n    print(f\"Chaos injected: {e}\")\n</code></pre>"},{"location":"contributing/testing-utilities/#factory-function","title":"Factory Function","text":"<p>Use <code>create_chaos_client()</code> for balanced error distribution:</p> <pre><code>from llm_answer_watcher.llm_runner.chaos_client import create_chaos_client\n\nchaos = create_chaos_client(\n    base_client=base,\n    failure_rate=0.3,  # 30% overall failures\n    seed=42\n)\n\n# Failures distributed evenly:\n# - 7.5% rate limit (429)\n# - 7.5% server errors (500/502/503)\n# - 7.5% timeout\n# - 7.5% auth error (401)\n</code></pre>"},{"location":"contributing/testing-utilities/#testing-retry-logic","title":"Testing Retry Logic","text":"<p>Validate your retry logic handles transient failures:</p> <pre><code># High failure rate to force retries\nchaos = ChaosLLMClient(\n    base_client=base,\n    success_rate=0.3,  # 70% failure rate\n    seed=42\n)\n\n# Retry loop\nmax_attempts = 3\nfor attempt in range(max_attempts):\n    try:\n        response = await chaos.generate_answer(\"test\")\n        break  # Success!\n    except RuntimeError as e:\n        if attempt == max_attempts - 1:\n            raise  # Give up after max attempts\n        # Otherwise retry\n</code></pre>"},{"location":"contributing/testing-utilities/#reproducible-chaos","title":"Reproducible Chaos","text":"<p>Use <code>seed</code> for deterministic test runs:</p> <pre><code># Two clients with same seed produce identical behavior\nchaos1 = ChaosLLMClient(base_client=base, success_rate=0.5, seed=123)\nchaos2 = ChaosLLMClient(base_client=base, success_rate=0.5, seed=123)\n\n# Same sequence of successes/failures\nfor i in range(10):\n    result1 = await chaos1.generate_answer(\"test\")\n    result2 = await chaos2.generate_answer(\"test\")\n    # Both succeed or both fail identically\n</code></pre>"},{"location":"contributing/testing-utilities/#error-types-injected","title":"Error Types Injected","text":"<p>ChaosLLMClient injects realistic errors:</p> Error Type Status Code Description Retryable? Rate Limit 429 Too many requests Yes Server Error 500/502/503 Server-side issues Yes Timeout - Network timeout Yes Auth Error 401 Invalid API key No"},{"location":"contributing/testing-utilities/#best-practices","title":"Best Practices","text":""},{"location":"contributing/testing-utilities/#1-use-mockllmclient-for-logic-tests","title":"1. Use MockLLMClient for Logic Tests","text":"<p>Test extraction, parsing, and business logic:</p> <pre><code>def test_brand_detection():\n    client = MockLLMClient(\n        responses={\"test\": \"Warmly and HubSpot are great tools.\"}\n    )\n    # Test extraction logic\n</code></pre>"},{"location":"contributing/testing-utilities/#2-use-chaosllmclient-for-resilience-tests","title":"2. Use ChaosLLMClient for Resilience Tests","text":"<p>Test error handling and retry logic:</p> <pre><code>def test_retry_on_rate_limit():\n    chaos = ChaosLLMClient(\n        base_client=base,\n        rate_limit_prob=1.0  # Always 429\n    )\n    # Test retry behavior\n</code></pre>"},{"location":"contributing/testing-utilities/#3-avoid-http-mocking","title":"3. Avoid HTTP Mocking","text":"<p>Instead of:</p> <pre><code># \u274c Brittle HTTP mocking\nhttpx_mock.add_response(\n    url=\"https://api.openai.com/...\",\n    json={\"choices\": [{\"message\": {\"content\": \"...\"}}]}\n)\n</code></pre> <p>Use:</p> <pre><code># \u2705 Clean protocol-based mocking\nclient = MockLLMClient(responses={\"prompt\": \"answer\"})\n</code></pre>"},{"location":"contributing/testing-utilities/#4-test-statistical-distribution","title":"4. Test Statistical Distribution","text":"<p>For chaos testing, validate statistical properties:</p> <pre><code>successes = 0\nfailures = 0\ntrials = 1000\n\nchaos = ChaosLLMClient(base_client=base, success_rate=0.7, seed=42)\n\nfor _ in range(trials):\n    try:\n        await chaos.generate_answer(\"test\")\n        successes += 1\n    except RuntimeError:\n        failures += 1\n\nsuccess_rate = successes / trials\nassert 0.65 &lt;= success_rate &lt;= 0.75  # Allow 5% tolerance\n</code></pre>"},{"location":"contributing/testing-utilities/#migration-from-http-mocking","title":"Migration from HTTP Mocking","text":""},{"location":"contributing/testing-utilities/#before-pytest-httpx","title":"Before (pytest-httpx)","text":"<pre><code>def test_openai_client(httpx_mock):\n    httpx_mock.add_response(\n        method=\"POST\",\n        url=\"https://api.openai.com/v1/chat/completions\",\n        json={\n            \"choices\": [{\"message\": {\"content\": \"test answer\"}}],\n            \"usage\": {\"total_tokens\": 100}\n        }\n    )\n\n    client = OpenAIClient(...)\n    response = await client.generate_answer(\"test\")\n    assert response.answer_text == \"test answer\"\n</code></pre>"},{"location":"contributing/testing-utilities/#after-mockllmclient","title":"After (MockLLMClient)","text":"<pre><code>def test_extraction_pipeline():\n    client = MockLLMClient(responses={\"test\": \"test answer\"})\n\n    response = await client.generate_answer(\"test\")\n    assert response.answer_text == \"test answer\"\n\n    # Now test the entire pipeline\n    extraction = parse_answer(response.answer_text, brands)\n    # ... test extraction logic\n</code></pre>"},{"location":"contributing/testing-utilities/#see-also","title":"See Also","text":"<ul> <li>Development Setup - Setting up your dev environment</li> <li>Testing Guide - Overall testing strategy</li> <li>Code Standards - Code quality requirements</li> </ul>"},{"location":"contributing/testing/","title":"Testing Guidelines","text":"<p>Writing and running tests.</p>"},{"location":"contributing/testing/#test-structure","title":"Test Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 test_config_loader.py\n\u251c\u2500\u2500 test_openai_client.py\n\u251c\u2500\u2500 test_mention_detector.py\n\u251c\u2500\u2500 test_rank_extractor.py\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"contributing/testing/#writing-tests","title":"Writing Tests","text":""},{"location":"contributing/testing/#unit-tests","title":"Unit Tests","text":"<pre><code>def test_brand_detection():\n    text = \"Use HubSpot for CRM\"\n    brands = [\"HubSpot\", \"Salesforce\"]\n\n    mentions = detect_mentions(text, brands)\n\n    assert len(mentions) == 1\n    assert mentions[0].brand == \"HubSpot\"\n</code></pre>"},{"location":"contributing/testing/#mocking-llm-apis","title":"Mocking LLM APIs","text":"<pre><code>def test_openai_client(httpx_mock):\n    httpx_mock.add_response(\n        method=\"POST\",\n        url=\"https://api.openai.com/v1/chat/completions\",\n        json={\"choices\": [{\"message\": {\"content\": \"...\"}}]}\n    )\n\n    client = OpenAIClient(...)\n    response = client.generate_answer(\"test\")\n\n    assert response.provider == \"openai\"\n</code></pre>"},{"location":"contributing/testing/#time-mocking","title":"Time Mocking","text":"<pre><code>from freezegun import freeze_time\n\n@freeze_time(\"2025-11-01 08:00:00\")\ndef test_timestamp():\n    run_id = run_id_from_timestamp()\n    assert run_id == \"2025-11-01T08-00-00Z\"\n</code></pre>"},{"location":"contributing/testing/#running-tests","title":"Running Tests","text":"<pre><code># All tests\npytest\n\n# With coverage\npytest --cov=llm_answer_watcher\n\n# Specific test\npytest tests/test_config_loader.py\n\n# Verbose\npytest -v\n\n# Skip slow tests\npytest -m \"not slow\"\n</code></pre>"},{"location":"contributing/testing/#coverage-requirements","title":"Coverage Requirements","text":"<ul> <li>Core modules: 80%+</li> <li>Critical paths: 100%</li> </ul> <pre><code>pytest --cov=llm_answer_watcher --cov-report=html\nopen htmlcov/index.html\n</code></pre> <p>See Code Standards for style guidelines.</p>"},{"location":"data-analytics/output-structure/","title":"Output Structure","text":"<p>Understanding the file and directory structure of monitoring runs.</p>"},{"location":"data-analytics/output-structure/#directory-layout","title":"Directory Layout","text":"<pre><code>output/\n\u251c\u2500\u2500 watcher.db                          # SQLite database\n\u2514\u2500\u2500 YYYY-MM-DDTHH-MM-SSZ/              # Run directory\n    \u251c\u2500\u2500 run_meta.json                   # Run summary\n    \u251c\u2500\u2500 report.html                     # HTML report\n    \u251c\u2500\u2500 intent_*_raw_*.json            # Raw LLM responses\n    \u251c\u2500\u2500 intent_*_parsed_*.json         # Extracted data\n    \u2514\u2500\u2500 intent_*_error_*.json          # Errors (if any)\n</code></pre>"},{"location":"data-analytics/output-structure/#file-descriptions","title":"File Descriptions","text":""},{"location":"data-analytics/output-structure/#run_metajson","title":"<code>run_meta.json</code>","text":"<p>Summary of the entire run with costs and stats.</p>"},{"location":"data-analytics/output-structure/#reporthtml","title":"<code>report.html</code>","text":"<p>Interactive HTML report with visualizations.</p>"},{"location":"data-analytics/output-structure/#intent__raw_json","title":"<code>intent_*_raw_*.json</code>","text":"<p>Raw LLM response with metadata.</p>"},{"location":"data-analytics/output-structure/#intent__parsed_json","title":"<code>intent_*_parsed_*.json</code>","text":"<p>Extracted brand mentions and ranks.</p>"},{"location":"data-analytics/output-structure/#watcherdb","title":"<code>watcher.db</code>","text":"<p>SQLite database with all historical data.</p> <p>See SQLite Database for database schema.</p>"},{"location":"data-analytics/query-examples/","title":"SQL Query Examples","text":"<p>Useful SQL queries for analyzing monitoring data.</p>"},{"location":"data-analytics/query-examples/#brand-performance","title":"Brand Performance","text":"<pre><code>-- Your brand's mention rate\nSELECT\n  COUNT(DISTINCT run_id) as total_runs,\n  COUNT(*) as total_mentions,\n  CAST(COUNT(*) AS FLOAT) / COUNT(DISTINCT run_id) as mentions_per_run\nFROM mentions\nWHERE normalized_name = 'yourbrand';\n</code></pre>"},{"location":"data-analytics/query-examples/#competitor-analysis","title":"Competitor Analysis","text":"<pre><code>-- Top mentioned competitors\nSELECT\n  brand,\n  COUNT(*) as mentions,\n  AVG(rank_position) as avg_rank\nFROM mentions\nWHERE normalized_name != 'yourbrand'\nGROUP BY brand\nORDER BY mentions DESC\nLIMIT 10;\n</code></pre>"},{"location":"data-analytics/query-examples/#trends-over-time","title":"Trends Over Time","text":"<pre><code>-- Weekly mention trends\nSELECT\n  strftime('%Y-W%W', timestamp_utc) as week,\n  COUNT(*) as mentions\nFROM mentions\nWHERE normalized_name = 'yourbrand'\nGROUP BY week\nORDER BY week DESC;\n</code></pre> <p>See SQLite Database for schema details.</p>"},{"location":"data-analytics/sqlite-database/","title":"SQLite Database","text":"<p>LLM Answer Watcher stores all monitoring data in a local SQLite database for historical tracking and trend analysis.</p>"},{"location":"data-analytics/sqlite-database/#database-location","title":"Database Location","text":"<p>Default path: <code>./output/watcher.db</code></p> <p>Configure in <code>watcher.config.yaml</code>:</p> <pre><code>run_settings:\n  sqlite_db_path: \"./output/watcher.db\"\n</code></pre>"},{"location":"data-analytics/sqlite-database/#schema-overview","title":"Schema Overview","text":"<p>The database has 4 main tables plus schema versioning:</p> <pre><code>schema_version  \u2192 Track database migrations\nruns            \u2192 One row per CLI execution\nanswers_raw     \u2192 Full LLM responses with metadata\nmentions        \u2192 Exploded brand mentions for analysis\noperations      \u2192 Post-intent operation results (optional)\n</code></pre>"},{"location":"data-analytics/sqlite-database/#schema-details","title":"Schema Details","text":""},{"location":"data-analytics/sqlite-database/#table-runs","title":"Table: runs","text":"<p>One row per <code>llm-answer-watcher run</code> execution.</p> <p>Columns:</p> <pre><code>CREATE TABLE runs (\n    run_id TEXT PRIMARY KEY,              -- YYYY-MM-DDTHH-MM-SSZ\n    timestamp_utc TEXT NOT NULL,          -- ISO 8601 with Z suffix\n    config_file TEXT,                     -- Path to config file used\n    total_cost_usd REAL NOT NULL,         -- Sum of all query costs\n    queries_completed INTEGER NOT NULL,   -- Successful queries\n    queries_failed INTEGER NOT NULL,      -- Failed queries\n    status TEXT NOT NULL,                 -- \"success\", \"partial\", \"failed\"\n    output_dir TEXT NOT NULL             -- Directory with run artifacts\n);\n</code></pre> <p>Example Query:</p> <pre><code>-- View recent runs\nSELECT run_id, timestamp_utc, status, total_cost_usd, queries_completed\nFROM runs\nORDER BY timestamp_utc DESC\nLIMIT 10;\n</code></pre>"},{"location":"data-analytics/sqlite-database/#table-answers_raw","title":"Table: answers_raw","text":"<p>One row per intent \u00d7 model combination.</p> <p>Columns:</p> <pre><code>CREATE TABLE answers_raw (\n    run_id TEXT NOT NULL,\n    intent_id TEXT NOT NULL,\n    model_provider TEXT NOT NULL,         -- \"openai\", \"anthropic\", etc.\n    model_name TEXT NOT NULL,             -- \"gpt-4o-mini\", etc.\n    timestamp_utc TEXT NOT NULL,\n    answer_text TEXT NOT NULL,            -- Full LLM response\n    tokens_used INTEGER,                  -- Total tokens (input + output)\n    estimated_cost_usd REAL,              -- Query cost\n    extraction_method TEXT,               -- \"regex\" or \"function_calling\"\n    web_search_count INTEGER DEFAULT 0,   -- Number of web searches\n    error_message TEXT,                   -- NULL if successful\n\n    PRIMARY KEY (run_id, intent_id, model_provider, model_name),\n    FOREIGN KEY (run_id) REFERENCES runs(run_id)\n);\n</code></pre> <p>Example Query:</p> <pre><code>-- Cost by provider\nSELECT\n    model_provider,\n    COUNT(*) as queries,\n    SUM(estimated_cost_usd) as total_cost,\n    AVG(estimated_cost_usd) as avg_cost_per_query\nFROM answers_raw\nWHERE timestamp_utc &gt;= datetime('now', '-30 days')\nGROUP BY model_provider\nORDER BY total_cost DESC;\n</code></pre>"},{"location":"data-analytics/sqlite-database/#table-mentions","title":"Table: mentions","text":"<p>One row per brand mention. Denormalized for fast queries.</p> <p>Columns:</p> <pre><code>CREATE TABLE mentions (\n    run_id TEXT NOT NULL,\n    intent_id TEXT NOT NULL,\n    model_provider TEXT NOT NULL,\n    model_name TEXT NOT NULL,\n    timestamp_utc TEXT NOT NULL,\n    brand TEXT NOT NULL,                  -- Original brand name\n    normalized_name TEXT NOT NULL,        -- Lowercase, hyphenated\n    is_mine INTEGER NOT NULL,             -- 1 = your brand, 0 = competitor\n    rank_position INTEGER,                -- 1, 2, 3... or NULL\n    detection_method TEXT NOT NULL,       -- \"regex\" or \"function_calling\"\n    confidence REAL DEFAULT 1.0,          -- 0.0-1.0 confidence score\n\n    PRIMARY KEY (run_id, intent_id, model_provider, model_name, normalized_name),\n    FOREIGN KEY (run_id) REFERENCES runs(run_id)\n);\n\nCREATE INDEX idx_mentions_timestamp ON mentions(timestamp_utc);\nCREATE INDEX idx_mentions_brand ON mentions(brand);\nCREATE INDEX idx_mentions_normalized ON mentions(normalized_name);\nCREATE INDEX idx_mentions_rank ON mentions(rank_position);\nCREATE INDEX idx_mentions_is_mine ON mentions(is_mine);\n</code></pre> <p>Example Query:</p> <pre><code>-- Brand mentions over time\nSELECT\n    DATE(timestamp_utc) as date,\n    brand,\n    COUNT(*) as mentions,\n    AVG(rank_position) as avg_rank\nFROM mentions\nWHERE normalized_name = 'warmly'\n  AND timestamp_utc &gt;= datetime('now', '-30 days')\nGROUP BY DATE(timestamp_utc), brand\nORDER BY date DESC;\n</code></pre>"},{"location":"data-analytics/sqlite-database/#table-schema_version","title":"Table: schema_version","text":"<p>Tracks database migrations.</p> <p>Columns:</p> <pre><code>CREATE TABLE schema_version (\n    version INTEGER PRIMARY KEY,\n    applied_at TEXT NOT NULL\n);\n</code></pre> <p>Current version: 3</p>"},{"location":"data-analytics/sqlite-database/#common-queries","title":"Common Queries","text":""},{"location":"data-analytics/sqlite-database/#basic-analytics","title":"Basic Analytics","text":"<p>Your brand visibility:</p> <pre><code>-- How often do we appear?\nSELECT\n    COUNT(DISTINCT run_id) as runs_appeared,\n    COUNT(*) as total_mentions,\n    AVG(rank_position) as average_rank\nFROM mentions\nWHERE is_mine = 1\n  AND timestamp_utc &gt;= datetime('now', '-30 days');\n</code></pre> <p>Competitor comparison:</p> <pre><code>SELECT\n    brand,\n    COUNT(*) as mentions,\n    COUNT(DISTINCT intent_id) as intents_appeared,\n    AVG(rank_position) as avg_rank,\n    MIN(rank_position) as best_rank,\n    COUNT(CASE WHEN rank_position = 1 THEN 1 END) as first_place_count\nFROM mentions\nWHERE rank_position IS NOT NULL\n  AND timestamp_utc &gt;= datetime('now', '-30 days')\nGROUP BY brand\nORDER BY mentions DESC;\n</code></pre>"},{"location":"data-analytics/sqlite-database/#trend-analysis","title":"Trend Analysis","text":"<p>Daily brand mentions:</p> <pre><code>SELECT\n    DATE(timestamp_utc) as date,\n    COUNT(CASE WHEN is_mine = 1 THEN 1 END) as my_mentions,\n    COUNT(CASE WHEN is_mine = 0 THEN 1 END) as competitor_mentions,\n    COUNT(*) as total_mentions\nFROM mentions\nWHERE timestamp_utc &gt;= datetime('now', '-30 days')\nGROUP BY DATE(timestamp_utc)\nORDER BY date DESC;\n</code></pre> <p>Ranking trends:</p> <pre><code>SELECT\n    DATE(timestamp_utc) as date,\n    AVG(CASE WHEN is_mine = 1 THEN rank_position END) as my_avg_rank,\n    AVG(CASE WHEN is_mine = 0 THEN rank_position END) as competitor_avg_rank\nFROM mentions\nWHERE rank_position IS NOT NULL\n  AND timestamp_utc &gt;= datetime('now', '-30 days')\nGROUP BY DATE(timestamp_utc)\nORDER BY date DESC;\n</code></pre>"},{"location":"data-analytics/sqlite-database/#intent-analysis","title":"Intent Analysis","text":"<p>Which intents work best for your brand?</p> <pre><code>SELECT\n    intent_id,\n    COUNT(*) as total_mentions,\n    COUNT(DISTINCT model_provider) as providers,\n    AVG(rank_position) as avg_rank\nFROM mentions\nWHERE is_mine = 1\n  AND timestamp_utc &gt;= datetime('now', '-30 days')\nGROUP BY intent_id\nORDER BY total_mentions DESC;\n</code></pre> <p>Intents where you're NOT mentioned:</p> <pre><code>-- Get all intent IDs from recent runs\nWITH recent_intents AS (\n    SELECT DISTINCT intent_id\n    FROM answers_raw\n    WHERE timestamp_utc &gt;= datetime('now', '-7 days')\n),\n-- Get intents where you appeared\nappeared_intents AS (\n    SELECT DISTINCT intent_id\n    FROM mentions\n    WHERE is_mine = 1\n      AND timestamp_utc &gt;= datetime('now', '-7 days')\n)\n-- Find the difference\nSELECT ri.intent_id\nFROM recent_intents ri\nLEFT JOIN appeared_intents ai ON ri.intent_id = ai.intent_id\nWHERE ai.intent_id IS NULL;\n</code></pre>"},{"location":"data-analytics/sqlite-database/#cost-analysis","title":"Cost Analysis","text":"<p>Total spending:</p> <pre><code>SELECT\n    SUM(total_cost_usd) as total_spent,\n    COUNT(*) as total_runs,\n    AVG(total_cost_usd) as avg_cost_per_run\nFROM runs\nWHERE timestamp_utc &gt;= datetime('now', '-30 days');\n</code></pre> <p>Cost by provider:</p> <pre><code>SELECT\n    model_provider,\n    model_name,\n    COUNT(*) as queries,\n    SUM(estimated_cost_usd) as total_cost,\n    AVG(estimated_cost_usd) as avg_cost\nFROM answers_raw\nWHERE timestamp_utc &gt;= datetime('now', '-30 days')\nGROUP BY model_provider, model_name\nORDER BY total_cost DESC;\n</code></pre> <p>Cost per brand mention:</p> <pre><code>SELECT\n    r.run_id,\n    r.total_cost_usd,\n    COUNT(m.brand) as mentions,\n    r.total_cost_usd / COUNT(m.brand) as cost_per_mention\nFROM runs r\nJOIN mentions m ON r.run_id = m.run_id\nWHERE r.timestamp_utc &gt;= datetime('now', '-30 days')\n  AND m.is_mine = 1\nGROUP BY r.run_id, r.total_cost_usd\nORDER BY cost_per_mention ASC;\n</code></pre>"},{"location":"data-analytics/sqlite-database/#provider-comparison","title":"Provider Comparison","text":"<p>Which provider mentions you more?</p> <pre><code>SELECT\n    model_provider,\n    COUNT(CASE WHEN is_mine = 1 THEN 1 END) as my_mentions,\n    COUNT(*) as total_mentions,\n    CAST(COUNT(CASE WHEN is_mine = 1 THEN 1 END) AS REAL) / COUNT(*) * 100 as my_mention_rate\nFROM mentions\nWHERE timestamp_utc &gt;= datetime('now', '-30 days')\nGROUP BY model_provider\nORDER BY my_mention_rate DESC;\n</code></pre> <p>Average ranking by provider:</p> <pre><code>SELECT\n    model_provider,\n    model_name,\n    COUNT(*) as mentions,\n    AVG(rank_position) as avg_rank\nFROM mentions\nWHERE is_mine = 1\n  AND rank_position IS NOT NULL\n  AND timestamp_utc &gt;= datetime('now', '-30 days')\nGROUP BY model_provider, model_name\nORDER BY avg_rank ASC;\n</code></pre>"},{"location":"data-analytics/sqlite-database/#exporting-data","title":"Exporting Data","text":""},{"location":"data-analytics/sqlite-database/#csv-export","title":"CSV Export","text":"<pre><code># Export mentions to CSV\nsqlite3 -header -csv output/watcher.db \\\n  \"SELECT * FROM mentions WHERE timestamp_utc &gt;= datetime('now', '-30 days')\" \\\n  &gt; mentions_30days.csv\n\n# Export runs summary\nsqlite3 -header -csv output/watcher.db \\\n  \"SELECT * FROM runs ORDER BY timestamp_utc DESC\" \\\n  &gt; runs_summary.csv\n</code></pre>"},{"location":"data-analytics/sqlite-database/#json-export","title":"JSON Export","text":"<pre><code># Export as JSON Lines\nsqlite3 output/watcher.db &lt;&lt;SQL | jq -c '.'\nSELECT json_object(\n  'brand', brand,\n  'timestamp', timestamp_utc,\n  'rank', rank_position,\n  'is_mine', is_mine\n) as json_data\nFROM mentions\nWHERE timestamp_utc &gt;= datetime('now', '-7 days');\nSQL\n</code></pre>"},{"location":"data-analytics/sqlite-database/#excelgoogle-sheets","title":"Excel/Google Sheets","text":"<ol> <li>Export to CSV:</li> </ol> <pre><code>sqlite3 -header -csv output/watcher.db \\\n  \"SELECT * FROM mentions\" &gt; mentions.csv\n</code></pre> <ol> <li>Import CSV into Excel or Google Sheets</li> </ol>"},{"location":"data-analytics/sqlite-database/#database-maintenance","title":"Database Maintenance","text":""},{"location":"data-analytics/sqlite-database/#vacuum-database","title":"Vacuum Database","text":"<p>Reclaim space after deletions:</p> <pre><code>sqlite3 output/watcher.db \"VACUUM;\"\n</code></pre>"},{"location":"data-analytics/sqlite-database/#delete-old-data","title":"Delete Old Data","text":"<pre><code>-- Delete runs older than 90 days\nDELETE FROM runs\nWHERE timestamp_utc &lt; datetime('now', '-90 days');\n\n-- Vacuum to reclaim space\nVACUUM;\n</code></pre>"},{"location":"data-analytics/sqlite-database/#check-database-size","title":"Check Database Size","text":"<pre><code>ls -lh output/watcher.db\n# Example: -rw-r--r-- 1 user user 2.5M Nov 5 14:30 watcher.db\n</code></pre>"},{"location":"data-analytics/sqlite-database/#backup-database","title":"Backup Database","text":"<pre><code># Simple copy\ncp output/watcher.db output/watcher.backup.db\n\n# Or use SQLite backup command\nsqlite3 output/watcher.db \".backup output/watcher.backup.db\"\n\n# Compress backup\ngzip output/watcher.backup.db\n</code></pre>"},{"location":"data-analytics/sqlite-database/#schema-migrations","title":"Schema Migrations","text":""},{"location":"data-analytics/sqlite-database/#check-schema-version","title":"Check Schema Version","text":"<pre><code>SELECT * FROM schema_version ORDER BY version DESC;\n</code></pre> <p>Output:</p> <pre><code>version | applied_at\n--------|---------------------\n3       | 2025-11-05T14:30:00Z\n2       | 2025-10-25T10:15:00Z\n1       | 2025-10-20T09:00:00Z\n</code></pre>"},{"location":"data-analytics/sqlite-database/#migration-process","title":"Migration Process","text":"<p>Migrations run automatically on startup. No manual intervention needed.</p> <p>What happens:</p> <ol> <li>Check current schema version</li> <li>Compare to required version</li> <li>Apply migrations sequentially</li> <li>Update schema_version table</li> </ol>"},{"location":"data-analytics/sqlite-database/#manual-migration-advanced","title":"Manual Migration (Advanced)","text":"<p>If needed, manually upgrade:</p> <pre><code>from llm_answer_watcher.storage.db import init_db_if_needed\n\ninit_db_if_needed(\"./output/watcher.db\")\n</code></pre>"},{"location":"data-analytics/sqlite-database/#connecting-with-bi-tools","title":"Connecting with BI Tools","text":""},{"location":"data-analytics/sqlite-database/#metabase","title":"Metabase","text":"<ol> <li>Add SQLite database</li> <li>Point to <code>./output/watcher.db</code></li> <li>Create dashboards</li> </ol>"},{"location":"data-analytics/sqlite-database/#tableau","title":"Tableau","text":"<ol> <li>Use SQLite connector</li> <li>Connect to <code>watcher.db</code></li> <li>Create visualizations</li> </ol>"},{"location":"data-analytics/sqlite-database/#pythonpandas","title":"Python/Pandas","text":"<pre><code>import sqlite3\nimport pandas as pd\n\n# Connect to database\nconn = sqlite3.connect(\"output/watcher.db\")\n\n# Load mentions into DataFrame\ndf = pd.read_sql_query(\n    \"SELECT * FROM mentions WHERE timestamp_utc &gt;= datetime('now', '-30 days')\",\n    conn\n)\n\n# Analyze\nprint(df.groupby('brand')['rank_position'].mean())\n\n# Close connection\nconn.close()\n</code></pre>"},{"location":"data-analytics/sqlite-database/#r","title":"R","text":"<pre><code>library(DBI)\nlibrary(RSQLite)\n\n# Connect\nconn &lt;- dbConnect(RSQLite::SQLite(), \"output/watcher.db\")\n\n# Query\nmentions &lt;- dbGetQuery(conn,\n  \"SELECT * FROM mentions WHERE timestamp_utc &gt;= datetime('now', '-30 days')\"\n)\n\n# Analyze\naggregate(rank_position ~ brand, data=mentions, FUN=mean)\n\n# Disconnect\ndbDisconnect(conn)\n</code></pre>"},{"location":"data-analytics/sqlite-database/#performance-tips","title":"Performance Tips","text":""},{"location":"data-analytics/sqlite-database/#indexes","title":"Indexes","text":"<p>Indexes already exist on:</p> <ul> <li><code>timestamp_utc</code></li> <li><code>brand</code></li> <li><code>normalized_name</code></li> <li><code>rank_position</code></li> <li><code>is_mine</code></li> </ul>"},{"location":"data-analytics/sqlite-database/#query-optimization","title":"Query Optimization","text":"<p>Use indexed columns in WHERE:</p> <pre><code>-- \u2705 Fast - uses index\nWHERE timestamp_utc &gt;= datetime('now', '-30 days')\n\n-- \u274c Slow - no index\nWHERE DATE(timestamp_utc) = '2025-11-05'\n</code></pre> <p>Limit result sets:</p> <pre><code>-- \u2705 Good - only get what you need\nSELECT brand, rank_position FROM mentions\nWHERE is_mine = 1\nLIMIT 100;\n\n-- \u274c Bad - retrieves all columns\nSELECT * FROM mentions;\n</code></pre>"},{"location":"data-analytics/sqlite-database/#analyze-query-plans","title":"Analyze Query Plans","text":"<pre><code>EXPLAIN QUERY PLAN\nSELECT brand, COUNT(*) FROM mentions\nWHERE timestamp_utc &gt;= datetime('now', '-30 days')\nGROUP BY brand;\n</code></pre>"},{"location":"data-analytics/sqlite-database/#troubleshooting","title":"Troubleshooting","text":""},{"location":"data-analytics/sqlite-database/#database-locked","title":"Database Locked","text":"<p>Problem: <code>database is locked</code></p> <p>Solution:</p> <pre><code># Check for locks\nlsof output/watcher.db\n\n# Kill process if safe\nkill -9 &lt;PID&gt;\n\n# Or wait and retry\n</code></pre>"},{"location":"data-analytics/sqlite-database/#corrupted-database","title":"Corrupted Database","text":"<p>Problem: Database errors on queries</p> <p>Solution:</p> <pre><code># Check integrity\nsqlite3 output/watcher.db \"PRAGMA integrity_check;\"\n\n# If corrupted, restore from backup\ncp output/watcher.backup.db output/watcher.db\n</code></pre>"},{"location":"data-analytics/sqlite-database/#schema-version-mismatch","title":"Schema Version Mismatch","text":"<p>Problem: \"Schema version X is newer than expected Y\"</p> <p>Solution: Update LLM Answer Watcher to latest version:</p> <pre><code>pip install --upgrade llm-answer-watcher\n</code></pre>"},{"location":"data-analytics/sqlite-database/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Query Examples</p> <p>More SQL query examples</p> <p>Query Examples \u2192</p> </li> <li> <p> Trends Analysis</p> <p>Track changes over time</p> <p>Trends Analysis \u2192</p> </li> <li> <p> Output Structure</p> <p>Understand JSON output files</p> <p>Output Structure \u2192</p> </li> <li> <p> Database Schema</p> <p>Complete schema reference</p> <p>Schema Reference \u2192</p> </li> </ul>"},{"location":"data-analytics/trends-analysis/","title":"Trends Analysis","text":"<p>Analyze brand visibility trends over time.</p>"},{"location":"data-analytics/trends-analysis/#time-series-analysis","title":"Time-Series Analysis","text":"<pre><code>-- Daily mention count\nSELECT\n  DATE(timestamp_utc) as date,\n  COUNT(*) as mentions,\n  AVG(rank_position) as avg_rank\nFROM mentions\nWHERE normalized_name = 'yourbrand'\nGROUP BY DATE(timestamp_utc)\nORDER BY date DESC;\n</code></pre>"},{"location":"data-analytics/trends-analysis/#comparative-trends","title":"Comparative Trends","text":"<pre><code>-- Your brand vs top competitor\nSELECT\n  DATE(m.timestamp_utc) as date,\n  m.brand,\n  COUNT(*) as mentions\nFROM mentions m\nWHERE m.normalized_name IN ('yourbrand', 'competitor')\nGROUP BY DATE(m.timestamp_utc), m.brand\nORDER BY date DESC, mentions DESC;\n</code></pre>"},{"location":"data-analytics/trends-analysis/#export-to-csv","title":"Export to CSV","text":"<pre><code>sqlite3 -header -csv output/watcher.db   \"SELECT * FROM mentions WHERE normalized_name = 'yourbrand'\"   &gt; brand_data.csv\n</code></pre> <p>See Query Examples for more queries.</p>"},{"location":"evaluation/ci-integration/","title":"CI Integration","text":"<p>Run evaluations in continuous integration.</p>"},{"location":"evaluation/ci-integration/#github-actions","title":"GitHub Actions","text":"<pre><code>- name: Run Evaluation Suite\n  run: |\n    uv run llm-answer-watcher eval       --fixtures evals/testcases/fixtures.yaml       --format json\n\n- name: Check Results\n  run: |\n    if [ $? -ne 0 ]; then\n      echo \"Evaluations failed\"\n      exit 1\n    fi\n</code></pre>"},{"location":"evaluation/ci-integration/#exit-codes","title":"Exit Codes","text":"<ul> <li><code>0</code>: All tests passed</li> <li><code>1</code>: Tests failed (below thresholds)</li> <li><code>2</code>: Configuration error</li> </ul> <p>See Running Evals for usage details.</p>"},{"location":"evaluation/metrics/","title":"Evaluation Metrics","text":"<p>Understanding evaluation metrics and thresholds.</p>"},{"location":"evaluation/metrics/#core-metrics","title":"Core Metrics","text":""},{"location":"evaluation/metrics/#mention-precision","title":"Mention Precision","text":"<p>Ratio of correct mentions to total mentions found.</p> <p>Threshold: \u2265 90%</p>"},{"location":"evaluation/metrics/#mention-recall","title":"Mention Recall","text":"<p>Ratio of correct mentions to expected mentions.</p> <p>Threshold: \u2265 80%</p>"},{"location":"evaluation/metrics/#mention-f1","title":"Mention F1","text":"<p>Harmonic mean of precision and recall.</p> <p>Threshold: \u2265 85%</p>"},{"location":"evaluation/metrics/#rank-accuracy","title":"Rank Accuracy","text":"<p>Percentage of correctly ranked brands.</p> <p>Threshold: \u2265 85%</p>"},{"location":"evaluation/metrics/#interpreting-results","title":"Interpreting Results","text":"<ul> <li>High precision, low recall: Missing mentions</li> <li>Low precision, high recall: False positives</li> <li>Low both: Extraction needs improvement</li> </ul> <p>See Test Cases for creating fixtures.</p>"},{"location":"evaluation/overview/","title":"Evaluation Framework","text":"<p>Quality control and accuracy testing for brand extraction.</p>"},{"location":"evaluation/overview/#purpose","title":"Purpose","text":"<p>The evaluation framework validates:</p> <ul> <li>Mention detection accuracy</li> <li>Rank extraction correctness</li> <li>False positive/negative rates</li> </ul>"},{"location":"evaluation/overview/#running-evaluations","title":"Running Evaluations","text":"<pre><code>llm-answer-watcher eval --fixtures evals/testcases/fixtures.yaml\n</code></pre>"},{"location":"evaluation/overview/#metrics-tracked","title":"Metrics Tracked","text":"<ul> <li>Mention Precision: Correct mentions / total found</li> <li>Mention Recall: Correct mentions / expected mentions</li> <li>Rank Accuracy: Correctly ranked brands</li> <li>F1 Score: Harmonic mean of precision/recall</li> </ul> <p>See Running Evals for detailed usage.</p>"},{"location":"evaluation/running-evals/","title":"Running Evaluations","text":"<p>How to run the evaluation suite and interpret results.</p>"},{"location":"evaluation/running-evals/#basic-usage","title":"Basic Usage","text":"<pre><code>llm-answer-watcher eval --fixtures evals/testcases/fixtures.yaml\n</code></pre>"},{"location":"evaluation/running-evals/#command-options","title":"Command Options","text":"<pre><code>llm-answer-watcher eval   --fixtures fixtures.yaml   --db eval_results.db   --format json\n</code></pre>"},{"location":"evaluation/running-evals/#example-output","title":"Example Output","text":"<pre><code>\u2705 Evaluation completed\n\u251c\u2500\u2500 Test cases: 15\n\u251c\u2500\u2500 Passed: 14\n\u251c\u2500\u2500 Failed: 1\n\u2514\u2500\u2500 Pass rate: 93.3%\n\nMetrics:\n\u251c\u2500\u2500 Mention Precision: 95.2%\n\u251c\u2500\u2500 Mention Recall: 91.8%\n\u251c\u2500\u2500 Rank Accuracy: 88.5%\n\u2514\u2500\u2500 F1 Score: 93.5%\n</code></pre> <p>See Metrics for metric definitions.</p>"},{"location":"evaluation/test-cases/","title":"Test Cases","text":"<p>Creating evaluation test fixtures.</p>"},{"location":"evaluation/test-cases/#fixture-format","title":"Fixture Format","text":"<pre><code>test_cases:\n  - description: \"Brand detection test\"\n    intent_id: \"test-intent\"\n    llm_answer_text: |\n      The best tools are:\n      1. YourBrand\n      2. CompetitorA\n\n    brands_mine: [\"YourBrand\"]\n    brands_competitors: [\"CompetitorA\"]\n\n    expected_my_mentions: [\"YourBrand\"]\n    expected_competitor_mentions: [\"CompetitorA\"]\n\n    expected_ranked_list:\n      - \"YourBrand\"\n      - \"CompetitorA\"\n</code></pre>"},{"location":"evaluation/test-cases/#running-custom-fixtures","title":"Running Custom Fixtures","text":"<pre><code>llm-answer-watcher eval --fixtures my_tests.yaml\n</code></pre> <p>See CI Integration for automated testing.</p>"},{"location":"examples/basic-monitoring/","title":"Basic Monitoring Example","text":"<p>A complete, production-ready example for monitoring brand visibility across multiple LLM providers.</p>"},{"location":"examples/basic-monitoring/#use-case","title":"Use Case","text":"<p>Monitor how LLMs recommend your email warmup tool versus competitors.</p> <p>Goals:</p> <ul> <li>Track if your brand appears in recommendations</li> <li>See which competitors are mentioned alongside you</li> <li>Understand ranking positions</li> <li>Keep costs low (&lt;$0.05 per run)</li> </ul>"},{"location":"examples/basic-monitoring/#complete-configuration","title":"Complete Configuration","text":"<p>Create <code>watcher.config.yaml</code>:</p> <pre><code>run_settings:\n  output_dir: \"./output\"\n  sqlite_db_path: \"./output/watcher.db\"\n\n  models:\n    # Primary model - cheap and fast\n    - provider: \"openai\"\n      model_name: \"gpt-4o-mini\"\n      env_api_key: \"OPENAI_API_KEY\"\n      system_prompt: \"openai/gpt-4-default\"\n\n    # Secondary model - different provider for comparison\n    - provider: \"anthropic\"\n      model_name: \"claude-3-5-haiku-20241022\"\n      env_api_key: \"ANTHROPIC_API_KEY\"\n\n  # Use regex extraction (free)\n  use_llm_rank_extraction: false\n\n  # Budget protection\n  budget:\n    enabled: true\n    max_per_run_usd: 0.10  # 10 cents max\n    warn_threshold_usd: 0.05  # Warn at 5 cents\n\nbrands:\n  # Your brand (with variations)\n  mine:\n    - \"Warmly\"\n    - \"Warmly.io\"\n    - \"Warmly AI\"\n\n  # Top competitors\n  competitors:\n    - \"Instantly\"\n    - \"Instantly.ai\"\n    - \"Lemwarm\"\n    - \"Lemlist\"\n    - \"HubSpot\"\n    - \"Apollo.io\"\n    - \"Woodpecker\"\n    - \"Mailshake\"\n\nintents:\n  # Core buyer-intent queries\n  - id: \"best-email-warmup-tools\"\n    prompt: \"What are the best email warmup tools?\"\n\n  - id: \"email-warmup-for-cold-email\"\n    prompt: \"Which email warmup tools are best for cold email campaigns?\"\n\n  - id: \"improve-email-deliverability\"\n    prompt: \"What tools help improve email deliverability?\"\n\n  - id: \"hubspot-alternatives\"\n    prompt: \"What are good alternatives to HubSpot for email warmup?\"\n</code></pre>"},{"location":"examples/basic-monitoring/#environment-setup","title":"Environment Setup","text":"<p>Create <code>.env</code> file:</p> <pre><code># API Keys\nOPENAI_API_KEY=sk-your-openai-key-here\nANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here\n\n# Optional: Custom output directory\n# LLM_WATCHER_OUTPUT_DIR=./custom-output\n</code></pre> <p>Load environment:</p> <pre><code>source .env\n</code></pre>"},{"location":"examples/basic-monitoring/#run-monitoring","title":"Run Monitoring","text":""},{"location":"examples/basic-monitoring/#first-run","title":"First Run","text":"<pre><code># Validate configuration\nllm-answer-watcher validate --config watcher.config.yaml\n\n# Run monitoring\nllm-answer-watcher run --config watcher.config.yaml\n</code></pre> <p>Expected Output:</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503         \ud83d\udd0d LLM Answer Watcher - Run Started         \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n\n\u2713 Configuration loaded from watcher.config.yaml\n  \u251c\u2500\u2500 Intents: 4\n  \u251c\u2500\u2500 Models: 2 (OpenAI, Anthropic)\n  \u251c\u2500\u2500 Brands: 3 monitored, 8 competitors\n  \u2514\u2500\u2500 Estimated cost: $0.0168\n\n\u280b Querying OpenAI gpt-4o-mini: \"What are the best email warmup tools?\"\n\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\nProgress: 8/8 queries completed (100%)\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503                  \u2705 Run Complete                    \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n\n\ud83d\udcca Results Summary\n  \u251c\u2500\u2500 Run ID: 2025-11-05T14-30-00Z\n  \u251c\u2500\u2500 Queries completed: 8/8\n  \u251c\u2500\u2500 Total cost: $0.0168 USD\n  \u251c\u2500\u2500 Your brands found: 6 mentions across 4 intents\n  \u2514\u2500\u2500 Competitors found: 15 mentions\n\n\ud83d\udcb0 Cost Breakdown\n  \u251c\u2500\u2500 OpenAI gpt-4o-mini: $0.0084 (4 queries)\n  \u2514\u2500\u2500 Anthropic claude-3-5-haiku: $0.0084 (4 queries)\n\n\ud83d\udcc2 Output Location\n  \u251c\u2500\u2500 Directory: ./output/2025-11-05T14-30-00Z/\n  \u251c\u2500\u2500 Database: ./output/watcher.db\n  \u2514\u2500\u2500 HTML Report: ./output/2025-11-05T14-30-00Z/report.html\n\n\ud83c\udf10 Open report in browser:\n  open ./output/2025-11-05T14-30-00Z/report.html\n</code></pre>"},{"location":"examples/basic-monitoring/#view-results","title":"View Results","text":""},{"location":"examples/basic-monitoring/#html-report","title":"HTML Report","text":"<pre><code>open ./output/2025-11-05T14-30-00Z/report.html\n</code></pre> <p>Report includes:</p> <ul> <li>Summary statistics</li> <li>Brand mention tables</li> <li>Rank distribution charts</li> <li>Cost breakdown</li> <li>Raw LLM responses</li> </ul>"},{"location":"examples/basic-monitoring/#json-results","title":"JSON Results","text":"<pre><code># View run summary\ncat ./output/2025-11-05T14-30-00Z/run_meta.json | jq '.'\n\n# View specific intent results\ncat ./output/2025-11-05T14-30-00Z/intent_best-email-warmup-tools_parsed_openai_gpt-4o-mini.json | jq '.'\n</code></pre>"},{"location":"examples/basic-monitoring/#sqlite-database","title":"SQLite Database","text":"<pre><code>sqlite3 ./output/watcher.db\n\n# View latest run\nSELECT * FROM runs ORDER BY timestamp_utc DESC LIMIT 1;\n\n# View your brand mentions\nSELECT * FROM mentions WHERE is_mine = 1 ORDER BY timestamp_utc DESC;\n\n# Compare competitors\nSELECT brand, COUNT(*) as mentions, AVG(rank_position) as avg_rank\nFROM mentions\nWHERE is_mine = 0 AND rank_position IS NOT NULL\nGROUP BY brand\nORDER BY mentions DESC;\n</code></pre>"},{"location":"examples/basic-monitoring/#analyzing-results","title":"Analyzing Results","text":""},{"location":"examples/basic-monitoring/#check-brand-visibility","title":"Check Brand Visibility","text":"<pre><code>-- Did we appear in any responses?\nSELECT\n    intent_id,\n    model_provider,\n    model_name,\n    brand,\n    rank_position\nFROM mentions\nWHERE is_mine = 1\n  AND run_id = '2025-11-05T14-30-00Z'\nORDER BY intent_id, rank_position;\n</code></pre> <p>Example Output:</p> <pre><code>best-email-warmup-tools | openai | gpt-4o-mini | Warmly | 2\nbest-email-warmup-tools | anthropic | claude-3-5-haiku | Warmly.io | 3\nemail-warmup-for-cold-email | openai | gpt-4o-mini | Warmly | 1\nimprove-email-deliverability | anthropic | claude-3-5-haiku | Warmly | 4\n</code></pre> <p>Interpretation:</p> <ul> <li>\u2705 Appeared in 4/8 responses (50% visibility)</li> <li>\ud83e\udd47 Ranked #1 once</li> <li>\ud83d\udcca Average rank: 2.5</li> </ul>"},{"location":"examples/basic-monitoring/#compare-vs-competitors","title":"Compare vs Competitors","text":"<pre><code>-- How do we rank vs competitors?\nSELECT\n    brand,\n    COUNT(*) as total_mentions,\n    COUNT(DISTINCT intent_id) as intents_appeared,\n    AVG(rank_position) as avg_rank,\n    MIN(rank_position) as best_rank\nFROM mentions\nWHERE run_id = '2025-11-05T14-30-00Z'\n  AND rank_position IS NOT NULL\nGROUP BY brand\nORDER BY total_mentions DESC, avg_rank ASC;\n</code></pre> <p>Example Output:</p> <pre><code>Instantly | 7 | 4 | 1.4 | 1\nWarmly | 6 | 4 | 2.5 | 1\nLemwarm | 5 | 3 | 3.2 | 2\nHubSpot | 4 | 2 | 2.0 | 1\n</code></pre> <p>Interpretation:</p> <ul> <li>\ud83e\udd48 2<sup>nd</sup> most mentioned (6 mentions)</li> <li>\ud83d\udcc8 Average rank 2.5 (middle of pack)</li> <li>\ud83c\udfaf Opportunity: Improve from #2-3 to #1</li> </ul>"},{"location":"examples/basic-monitoring/#identify-gaps","title":"Identify Gaps","text":"<pre><code>-- Which intents didn't mention us?\nSELECT intent_id\nFROM (SELECT DISTINCT intent_id FROM mentions WHERE run_id = '2025-11-05T14-30-00Z')\nWHERE intent_id NOT IN (\n    SELECT DISTINCT intent_id\n    FROM mentions\n    WHERE run_id = '2025-11-05T14-30-00Z'\n      AND is_mine = 1\n);\n</code></pre> <p>Example Output:</p> <pre><code>hubspot-alternatives\n</code></pre> <p>Interpretation:</p> <ul> <li>\u274c Not mentioned in \"HubSpot alternatives\" query</li> <li>\ud83c\udfaf Action: Optimize content for \"alternative\" queries</li> </ul>"},{"location":"examples/basic-monitoring/#schedule-regular-monitoring","title":"Schedule Regular Monitoring","text":""},{"location":"examples/basic-monitoring/#daily-cron-job","title":"Daily Cron Job","text":"<p>Create <code>/home/user/bin/run-brand-monitoring.sh</code>:</p> <pre><code>#!/bin/bash\nset -e\n\nPROJECT_DIR=\"/home/user/projects/llm-answer-watcher\"\ncd \"$PROJECT_DIR\"\n\n# Load API keys\nsource .env\n\n# Activate virtualenv\nsource .venv/bin/activate\n\n# Run monitoring\nllm-answer-watcher run --config watcher.config.yaml --yes --quiet &gt;&gt; logs/monitoring.log 2&gt;&amp;1\n\n# Alert if no brand mentions\nif [ $? -eq 0 ]; then\n    MENTIONS=$(sqlite3 output/watcher.db \"SELECT COUNT(*) FROM mentions WHERE is_mine=1 AND timestamp_utc &gt;= datetime('now', '-1 hour')\")\n\n    if [ \"$MENTIONS\" -eq 0 ]; then\n        echo \"\u26a0\ufe0f WARNING: No brand mentions in latest run\" | mail -s \"Brand Visibility Alert\" admin@example.com\n    fi\nfi\n</code></pre> <p>Add to crontab:</p> <pre><code># Run daily at 9 AM\n0 9 * * * /home/user/bin/run-brand-monitoring.sh\n</code></pre>"},{"location":"examples/basic-monitoring/#cost-analysis","title":"Cost Analysis","text":""},{"location":"examples/basic-monitoring/#actual-costs","title":"Actual Costs","text":"<pre><code>-- Total cost last 30 days\nSELECT SUM(total_cost_usd) as total_cost\nFROM runs\nWHERE timestamp_utc &gt;= datetime('now', '-30 days');\n\n-- Cost by provider\nSELECT\n    model_provider,\n    SUM(estimated_cost_usd) as provider_cost,\n    COUNT(*) as queries\nFROM answers_raw\nWHERE timestamp_utc &gt;= datetime('now', '-30 days')\nGROUP BY model_provider;\n</code></pre>"},{"location":"examples/basic-monitoring/#cost-optimization","title":"Cost Optimization","text":"<p>Current config costs:</p> <ul> <li>4 intents \u00d7 2 models = 8 queries</li> <li>~$0.001-0.002 per query</li> <li>Total: ~$0.016 per run</li> </ul> <p>Monthly (daily runs):</p> <ul> <li>30 runs \u00d7 \\(0.016 = **\\)0.48/month**</li> </ul> <p>To reduce costs:</p> <ol> <li>Use only 1 model:</li> </ol> <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"  # $0.008/run\n</code></pre> <ol> <li>Reduce intents:</li> </ol> <pre><code>intents:\n  - id: \"best-email-warmup-tools\"  # Only 1 intent\n    prompt: \"What are the best email warmup tools?\"\n</code></pre> <p>Optimized cost: $0.002/run = $0.06/month</p>"},{"location":"examples/basic-monitoring/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/basic-monitoring/#no-brand-mentions","title":"No Brand Mentions","text":"<p>Problem: Your brand never appears</p> <p>Solutions:</p> <ol> <li>Check brand aliases:</li> </ol> <pre><code># View raw responses\ncat output/*/intent_*_raw_*.json | jq '.answer_text' | grep -i \"warmly\"\n</code></pre> <ol> <li>Add more aliases:</li> </ol> <pre><code>brands:\n  mine:\n    - \"Warmly\"\n    - \"Warmly.io\"\n    - \"Warmly AI\"\n    - \"Warmly Email\"  # Add this\n</code></pre> <ol> <li>Update intent prompts:</li> </ol> <pre><code># More specific prompt\n- id: \"warmly-vs-competitors\"\n  prompt: \"Compare Warmly vs Instantly vs Lemwarm for email warmup\"\n</code></pre>"},{"location":"examples/basic-monitoring/#high-costs","title":"High Costs","text":"<p>Problem: Costs exceed budget</p> <p>Solutions:</p> <ol> <li>Enable budget warnings:</li> </ol> <pre><code>budget:\n  enabled: true\n  max_per_run_usd: 0.05  # Abort if &gt; 5 cents\n</code></pre> <ol> <li>Reduce models or intents</li> <li>Switch to cheaper models</li> </ol>"},{"location":"examples/basic-monitoring/#rate-limiting","title":"Rate Limiting","text":"<p>Problem: API rate limits hit</p> <p>Solution: Add delays:</p> <pre><code>run_settings:\n  delay_between_queries: 2  # 2 second delay\n</code></pre>"},{"location":"examples/basic-monitoring/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Multi-Provider</p> <p>Compare multiple LLM providers</p> <p>Multi-Provider Example \u2192</p> </li> <li> <p> Competitor Analysis</p> <p>Deep dive into competitor positioning</p> <p>Competitor Analysis \u2192</p> </li> <li> <p> Historical Trends</p> <p>Track changes over time</p> <p>Trends Analysis \u2192</p> </li> <li> <p> Automation</p> <p>Set up scheduled monitoring</p> <p>Automation Guide \u2192</p> </li> </ul>"},{"location":"examples/budget-constrained/","title":"Budget-Constrained Monitoring","text":"<p>Minimize costs while maintaining monitoring quality.</p>"},{"location":"examples/budget-constrained/#cost-optimized-configuration","title":"Cost-Optimized Configuration","text":"<pre><code>run_settings:\n  output_dir: \"./output\"\n\n  models:\n    # Use cheapest effective model\n    - provider: \"openai\"\n      model_name: \"gpt-4o-mini\"\n      env_api_key: \"OPENAI_API_KEY\"\n\n  # Regex extraction (no extra LLM calls)\n  use_llm_rank_extraction: false\n\n  # Set budget limits\n  budget:\n    enabled: true\n    max_per_run_usd: 0.10\n    max_per_intent_usd: 0.02\n\nbrands:\n  mine: [\"YourBrand\"]\n  # Focus on top 3 competitors only\n  competitors: [\"TopCompetitor1\", \"TopCompetitor2\", \"TopCompetitor3\"]\n\nintents:\n  # Single most valuable intent\n  - id: \"main-query\"\n    prompt: \"What are the best tools?\"\n</code></pre>"},{"location":"examples/budget-constrained/#estimated-costs","title":"Estimated Costs","text":"<ul> <li>1 intent \u00d7 1 model: ~$0.002 per run</li> <li>3 intents \u00d7 1 model: ~$0.006 per run</li> <li>Run daily for a month: ~$0.18/month</li> </ul> <p>See Cost Management.</p>"},{"location":"examples/ci-cd-integration/","title":"CI/CD Integration","text":"<p>Integrate brand monitoring into your continuous integration pipeline.</p>"},{"location":"examples/ci-cd-integration/#github-actions-example","title":"GitHub Actions Example","text":"<p><code>.github/workflows/monitoring.yml</code>:</p> <pre><code>name: Brand Monitoring\n\non:\n  schedule:\n    - cron: '0 9 * * *'  # Daily at 9 AM\n  workflow_dispatch:\n\njobs:\n  monitor:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.12'\n\n      - name: Install\n        run: |\n          pip install uv\n          uv sync\n\n      - name: Run monitoring\n        env:\n          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n        run: |\n          uv run llm-answer-watcher run             --config config.yaml             --yes             --format json\n\n      - name: Upload results\n        uses: actions/upload-artifact@v4\n        with:\n          name: results\n          path: output/\n</code></pre>"},{"location":"examples/ci-cd-integration/#exit-code-handling","title":"Exit Code Handling","text":"<pre><code>- name: Run monitoring\n  id: monitor\n  run: |\n    uv run llm-answer-watcher run --config config.yaml --yes\n    echo \"exit_code=$?\" &gt;&gt; $GITHUB_OUTPUT\n  continue-on-error: true\n\n- name: Check result\n  run: |\n    if [ \"${{ steps.monitor.outputs.exit_code }}\" == \"0\" ]; then\n      echo \"Success\"\n    else\n      echo \"Failed\"\n      exit 1\n    fi\n</code></pre> <p>See Automation for more examples.</p>"},{"location":"examples/competitor-analysis/","title":"Competitor Analysis","text":"<p>Track competitors comprehensively across multiple queries.</p>"},{"location":"examples/competitor-analysis/#example-configuration","title":"Example Configuration","text":"<pre><code>run_settings:\n  output_dir: \"./output\"\n  models:\n    - provider: \"openai\"\n      model_name: \"gpt-4o-mini\"\n      env_api_key: \"OPENAI_API_KEY\"\n\nbrands:\n  mine: [\"YourBrand\"]\n  competitors:\n    - \"TopCompetitor\"\n    - \"RisingStartup\"\n    - \"IndustryLeader\"\n    - \"NichePlayer\"\n    - \"AlternativeTool\"\n\nintents:\n  - id: \"best-overall\"\n    prompt: \"What are the best tools in the category?\"\n  - id: \"for-startups\"\n    prompt: \"Best tools for startups?\"\n  - id: \"for-enterprise\"\n    prompt: \"Best enterprise tools?\"\n  - id: \"affordable-options\"\n    prompt: \"Most affordable tools?\"\n</code></pre>"},{"location":"examples/competitor-analysis/#analyzing-results","title":"Analyzing Results","text":"<pre><code>-- Competitor appearance frequency\nSELECT brand, COUNT(*) as mentions\nFROM mentions\nWHERE normalized_name != 'yourbrand'\nGROUP BY brand\nORDER BY mentions DESC;\n</code></pre>"},{"location":"examples/multi-provider/","title":"Multi-Provider Monitoring","text":"<p>Compare how different LLM providers represent your brand.</p>"},{"location":"examples/multi-provider/#example-configuration","title":"Example Configuration","text":"<pre><code>run_settings:\n  output_dir: \"./output\"\n  sqlite_db_path: \"./output/watcher.db\"\n\n  models:\n    - provider: \"openai\"\n      model_name: \"gpt-4o-mini\"\n      env_api_key: \"OPENAI_API_KEY\"\n\n    - provider: \"anthropic\"\n      model_name: \"claude-3-5-haiku-20241022\"\n      env_api_key: \"ANTHROPIC_API_KEY\"\n\n    - provider: \"mistral\"\n      model_name: \"mistral-small-latest\"\n      env_api_key: \"MISTRAL_API_KEY\"\n\nbrands:\n  mine: [\"YourBrand\"]\n  competitors: [\"CompetitorA\", \"CompetitorB\"]\n\nintents:\n  - id: \"best-tools\"\n    prompt: \"What are the best tools?\"\n</code></pre>"},{"location":"examples/multi-provider/#benefits","title":"Benefits","text":"<ul> <li>See which providers favor your brand</li> <li>Identify provider-specific biases</li> <li>Optimize for specific LLM platforms</li> </ul> <p>See Basic Monitoring for simpler setup.</p>"},{"location":"getting-started/basic-configuration/","title":"Basic Configuration","text":"<p>Learn how to create your first custom configuration file for LLM Answer Watcher.</p>"},{"location":"getting-started/basic-configuration/#configuration-file-structure","title":"Configuration File Structure","text":"<p>LLM Answer Watcher uses YAML configuration files with three main sections:</p> <pre><code>run_settings:    # How and where to run\nbrands:          # What brands to monitor\nintents:         # What questions to ask\n</code></pre>"},{"location":"getting-started/basic-configuration/#minimal-configuration","title":"Minimal Configuration","text":"<p>The simplest possible configuration:</p> <pre><code>run_settings:\n  output_dir: \"./output\"\n  sqlite_db_path: \"./output/watcher.db\"\n\n  models:\n    - provider: \"openai\"\n      model_name: \"gpt-4o-mini\"\n      env_api_key: \"OPENAI_API_KEY\"\n\nbrands:\n  mine:\n    - \"YourBrand\"\n\n  competitors:\n    - \"CompetitorA\"\n    - \"CompetitorB\"\n\nintents:\n  - id: \"best-tools\"\n    prompt: \"What are the best [your category] tools?\"\n</code></pre> <p>This configuration:</p> <ul> <li>Uses OpenAI's <code>gpt-4o-mini</code> (cost-effective)</li> <li>Monitors 1 brand vs 2 competitors</li> <li>Asks 1 intent question</li> <li>Stores results in <code>./output/</code></li> </ul>"},{"location":"getting-started/basic-configuration/#run-settings-section","title":"Run Settings Section","text":""},{"location":"getting-started/basic-configuration/#basic-run-settings","title":"Basic Run Settings","text":"<pre><code>run_settings:\n  # Where to store output files\n  output_dir: \"./output\"\n\n  # SQLite database path for historical tracking\n  sqlite_db_path: \"./output/watcher.db\"\n\n  # Use LLM for rank extraction (more accurate but costs more)\n  use_llm_rank_extraction: false\n</code></pre>"},{"location":"getting-started/basic-configuration/#model-configuration","title":"Model Configuration","text":"<p>Define which LLM providers and models to use:</p> <pre><code>run_settings:\n  models:\n    # OpenAI configuration\n    - provider: \"openai\"\n      model_name: \"gpt-4o-mini\"\n      env_api_key: \"OPENAI_API_KEY\"\n\n    # Anthropic configuration\n    - provider: \"anthropic\"\n      model_name: \"claude-3-5-haiku-20241022\"\n      env_api_key: \"ANTHROPIC_API_KEY\"\n</code></pre> <p>Key Points:</p> <ul> <li>provider: Must be one of: <code>openai</code>, <code>anthropic</code>, <code>mistral</code>, <code>grok</code>, <code>google</code>, <code>perplexity</code></li> <li>model_name: Specific model identifier (see Provider Guide)</li> <li>env_api_key: Environment variable name containing your API key</li> </ul> <p>Model Selection</p> <p>Start with cost-effective models:</p> <ul> <li>OpenAI: <code>gpt-4o-mini</code> ($0.15/1M input tokens)</li> <li>Anthropic: <code>claude-3-5-haiku-20241022</code> ($0.80/1M input tokens)</li> <li>Mistral: <code>mistral-small-latest</code> ($0.20/1M input tokens)</li> <li>Grok: <code>grok-2-1212</code> ($2.00/1M input tokens)</li> <li>Google: <code>gemini-2.0-flash-exp</code> (free tier available)</li> </ul>"},{"location":"getting-started/basic-configuration/#optional-system-prompts","title":"Optional System Prompts","text":"<p>Customize the system prompt for each model:</p> <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    system_prompt: \"openai/gpt-4-default\"  # Uses built-in prompt\n</code></pre> <p>If not specified, uses the provider's default prompt.</p>"},{"location":"getting-started/basic-configuration/#brands-section","title":"Brands Section","text":""},{"location":"getting-started/basic-configuration/#your-brands","title":"Your Brands","text":"<p>Define all variations of your brand name:</p> <pre><code>brands:\n  mine:\n    - \"YourBrand\"\n    - \"YourBrand.io\"\n    - \"YourBrand CRM\"\n    - \"yourbrand.com\"\n</code></pre> <p>Why multiple aliases?</p> <p>LLMs might reference your brand differently:</p> <ul> <li>\"HubSpot\" vs \"HubSpot CRM\"</li> <li>\"Lemwarm\" vs \"Lemwarm.io\"</li> <li>Domain names: \"acme.com\"</li> </ul> <p>Word Boundary Matching</p> <p>Brands are matched using word boundaries. \"Hub\" will NOT match in \"GitHub\". Add specific variations if needed.</p>"},{"location":"getting-started/basic-configuration/#competitors","title":"Competitors","text":"<p>List all competitors to track:</p> <pre><code>brands:\n  competitors:\n    - \"CompetitorA\"\n    - \"CompetitorB\"\n    - \"IndustryTool\"\n    - \"Alternative.io\"\n    - \"BigPlayer CRM\"\n</code></pre> <p>Tips:</p> <ul> <li>Include direct competitors (same category)</li> <li>Include indirect competitors (adjacent use cases)</li> <li>Use specific names, not generic terms</li> <li>Add variations for well-known competitors</li> </ul>"},{"location":"getting-started/basic-configuration/#complete-brands-example","title":"Complete Brands Example","text":"<pre><code>brands:\n  mine:\n    - \"Lemwarm\"\n    - \"Lemwarm.io\"\n    - \"Lemlist\"\n    - \"Lemlist.com\"\n\n  competitors:\n    - \"Instantly\"\n    - \"Instantly.ai\"\n    - \"Warmbox\"\n    - \"Warmbox.ai\"\n    - \"MailReach\"\n    - \"HubSpot\"\n    - \"Apollo.io\"\n    - \"Woodpecker\"\n</code></pre>"},{"location":"getting-started/basic-configuration/#intents-section","title":"Intents Section","text":"<p>Intents are the questions you want to ask LLMs.</p>"},{"location":"getting-started/basic-configuration/#basic-intent","title":"Basic Intent","text":"<pre><code>intents:\n  - id: \"best-tools\"\n    prompt: \"What are the best email warmup tools?\"\n</code></pre> <p>Intent Structure:</p> <ul> <li>id: Unique identifier (used in filenames and database)</li> <li>prompt: The exact question to ask the LLM</li> </ul>"},{"location":"getting-started/basic-configuration/#multiple-intents","title":"Multiple Intents","text":"<p>Test different question types:</p> <pre><code>intents:\n  # Direct question\n  - id: \"best-email-warmup-tools\"\n    prompt: \"What are the best email warmup tools?\"\n\n  # Comparison query\n  - id: \"comparison-warmup-tools\"\n    prompt: \"Compare the top email warmup tools for improving deliverability\"\n\n  # Specific use case\n  - id: \"cold-outreach-tools\"\n    prompt: \"Which email warmup tools are best for cold outreach campaigns?\"\n\n  # Alternative phrasing\n  - id: \"recommended-warmup-services\"\n    prompt: \"What email warmup services do you recommend for startups?\"\n</code></pre>"},{"location":"getting-started/basic-configuration/#intent-id-best-practices","title":"Intent ID Best Practices","text":"<p>Use descriptive, URL-safe IDs:</p> <p>\u2705 Good IDs: - <code>best-crm-tools</code> - <code>email-automation-comparison</code> - <code>startup-friendly-options</code></p> <p>\u274c Avoid: - <code>query1</code> (not descriptive) - <code>best CRM tools</code> (spaces) - <code>what's-best?</code> (special characters)</p>"},{"location":"getting-started/basic-configuration/#crafting-effective-prompts","title":"Crafting Effective Prompts","text":"<p>Good prompts are:</p> <ol> <li>Natural: How a real user would ask</li> <li>Specific: Target a particular use case or category</li> <li>Open-ended: Allow for varied responses</li> <li>Buyer-intent: Imply readiness to evaluate/purchase</li> </ol> <p>Examples:</p> <pre><code>intents:\n  # \u2705 Good: Natural buyer-intent query\n  - id: \"saas-analytics-tools\"\n    prompt: \"What are the best analytics tools for SaaS companies?\"\n\n  # \u2705 Good: Specific use case\n  - id: \"startup-crm-budget\"\n    prompt: \"Which CRM is best for startups on a tight budget?\"\n\n  # \u274c Too broad\n  - id: \"software\"\n    prompt: \"Tell me about software\"\n\n  # \u274c Not buyer-intent\n  - id: \"history\"\n    prompt: \"What is the history of CRM software?\"\n</code></pre>"},{"location":"getting-started/basic-configuration/#complete-basic-configuration-example","title":"Complete Basic Configuration Example","text":"<p>Here's a complete, production-ready configuration:</p> <pre><code>run_settings:\n  output_dir: \"./output\"\n  sqlite_db_path: \"./output/watcher.db\"\n\n  models:\n    # Cost-effective model for regular monitoring\n    - provider: \"openai\"\n      model_name: \"gpt-4o-mini\"\n      env_api_key: \"OPENAI_API_KEY\"\n\n    # High-quality model for comparison\n    - provider: \"anthropic\"\n      model_name: \"claude-3-5-haiku-20241022\"\n      env_api_key: \"ANTHROPIC_API_KEY\"\n\n  # Use regex-based extraction (faster, cheaper)\n  use_llm_rank_extraction: false\n\nbrands:\n  mine:\n    - \"Lemwarm\"\n    - \"Lemwarm.io\"\n    - \"Lemlist\"\n    - \"lemlist.com\"\n\n  competitors:\n    - \"Instantly\"\n    - \"Instantly.ai\"\n    - \"Warmbox\"\n    - \"Warmbox.ai\"\n    - \"MailReach\"\n    - \"HubSpot\"\n    - \"Apollo.io\"\n    - \"Woodpecker\"\n\nintents:\n  # Direct question\n  - id: \"best-email-warmup-tools\"\n    prompt: \"What are the best email warmup tools?\"\n\n  # Comparison query\n  - id: \"warmup-tools-comparison\"\n    prompt: \"Compare the top email warmup tools for improving email deliverability\"\n\n  # Use case specific\n  - id: \"cold-outreach-warmup\"\n    prompt: \"Which email warmup tools are best for cold outreach campaigns?\"\n\n  # Budget-conscious\n  - id: \"affordable-warmup-tools\"\n    prompt: \"What are the most affordable email warmup tools for startups?\"\n</code></pre>"},{"location":"getting-started/basic-configuration/#testing-your-configuration","title":"Testing Your Configuration","text":"<p>Always validate before running:</p> <pre><code>llm-answer-watcher validate --config my-config.yaml\n</code></pre> <p>Expected output:</p> <pre><code>\u2705 Configuration valid\n\u251c\u2500\u2500 Models: 2 configured\n\u2502   \u251c\u2500\u2500 openai: gpt-4o-mini\n\u2502   \u2514\u2500\u2500 anthropic: claude-3-5-haiku-20241022\n\u251c\u2500\u2500 Brands: 4 mine, 8 competitors\n\u251c\u2500\u2500 Intents: 4 queries\n\u2514\u2500\u2500 Estimated cost: $0.016 (8 queries total)\n</code></pre>"},{"location":"getting-started/basic-configuration/#next-steps","title":"Next Steps","text":"<p>Now that you understand basic configuration:</p> <ul> <li> <p> Advanced Configuration</p> <p>Budget controls, web search, custom operations</p> <p>Configuration Guide \u2192</p> </li> <li> <p> Run Your Config</p> <p>Execute monitoring with your custom configuration</p> <p>First Run \u2192</p> </li> <li> <p> Add More Providers</p> <p>Learn about Mistral, Grok, Google, Perplexity</p> <p>Providers \u2192</p> </li> <li> <p> See Examples</p> <p>Browse real-world configuration examples</p> <p>Examples \u2192</p> </li> </ul>"},{"location":"getting-started/first-run/","title":"Your First Run","text":"<p>This guide walks you through running LLM Answer Watcher for the first time and understanding the results.</p>"},{"location":"getting-started/first-run/#before-you-start","title":"Before You Start","text":"<p>Ensure you have:</p> <ul> <li>\u2705 Installed LLM Answer Watcher (Installation Guide)</li> <li>\u2705 Set up at least one API key</li> <li>\u2705 Activated your virtual environment</li> </ul>"},{"location":"getting-started/first-run/#step-1-verify-installation","title":"Step 1: Verify Installation","text":"<p>Check that everything is working:</p> <pre><code># Verify the CLI is available\nllm-answer-watcher --version\n\n# Check help documentation\nllm-answer-watcher --help\n</code></pre>"},{"location":"getting-started/first-run/#step-2-validate-example-configuration","title":"Step 2: Validate Example Configuration","text":"<p>Before running, validate the configuration file:</p> <pre><code>llm-answer-watcher validate --config examples/watcher.config.yaml\n</code></pre> <p>Expected output:</p> <pre><code>\u2705 Configuration valid\n\u251c\u2500\u2500 Models: 1 configured (openai gpt-4o-mini)\n\u251c\u2500\u2500 Brands: 2 mine, 4 competitors\n\u251c\u2500\u2500 Intents: 2 queries\n\u2514\u2500\u2500 Estimated cost: $0.004\n</code></pre> <p>If validation fails, you'll see specific error messages about what needs to be fixed.</p>"},{"location":"getting-started/first-run/#step-3-run-your-first-monitoring-job","title":"Step 3: Run Your First Monitoring Job","text":"<p>Execute a monitoring run:</p> <pre><code>llm-answer-watcher run --config examples/watcher.config.yaml\n</code></pre>"},{"location":"getting-started/first-run/#what-happens-during-a-run","title":"What Happens During a Run","text":""},{"location":"getting-started/first-run/#1-configuration-loading","title":"1. Configuration Loading","text":"<pre><code>\ud83d\udd0d Loading configuration from examples/watcher.config.yaml...\n\u251c\u2500\u2500 \u2705 YAML syntax valid\n\u251c\u2500\u2500 \u2705 Schema validation passed\n\u251c\u2500\u2500 \u2705 API keys found\n\u2514\u2500\u2500 \u2705 Output directory accessible\n</code></pre>"},{"location":"getting-started/first-run/#2-cost-estimation","title":"2. Cost Estimation","text":"<pre><code>\ud83d\udcb0 Estimated cost breakdown:\n\u251c\u2500\u2500 OpenAI gpt-4o-mini: $0.002 \u00d7 2 intents = $0.004\n\u2514\u2500\u2500 Total estimated cost: $0.004\n\nContinue with this run? [Y/n]:\n</code></pre> <p>Press <code>Y</code> to continue, or <code>n</code> to abort.</p> <p>Skip confirmation prompts</p> <p>Use <code>--yes</code> flag to auto-confirm in automated scripts: <pre><code>llm-answer-watcher run --config config.yaml --yes\n</code></pre></p>"},{"location":"getting-started/first-run/#3-query-execution","title":"3. Query Execution","text":"<p>You'll see progress for each query:</p> <pre><code>\ud83d\udce4 Query 1/2: \"What are the best email warmup tools?\"\n\u251c\u2500\u2500 Provider: OpenAI (gpt-4o-mini)\n\u251c\u2500\u2500 Sending request... \u23f3\n\u251c\u2500\u2500 \u2705 Response received (1.2s)\n\u251c\u2500\u2500 Tokens: 145 input, 387 output\n\u251c\u2500\u2500 Cost: $0.002\n\u2514\u2500\u2500 Brands detected: 3 found (Lemwarm, Instantly, HubSpot)\n\n\ud83d\udce4 Query 2/2: \"Compare the top email warmup tools\"\n\u251c\u2500\u2500 Provider: OpenAI (gpt-4o-mini)\n\u251c\u2500\u2500 Sending request... \u23f3\n\u251c\u2500\u2500 \u2705 Response received (1.4s)\n\u251c\u2500\u2500 Tokens: 152 input, 421 output\n\u251c\u2500\u2500 Cost: $0.002\n\u2514\u2500\u2500 Brands detected: 4 found (Lemwarm, Lemlist, Instantly, Apollo.io)\n</code></pre>"},{"location":"getting-started/first-run/#4-results-summary","title":"4. Results Summary","text":"<pre><code>\u2705 Run completed successfully!\n\n\ud83d\udcca Summary:\n\u251c\u2500\u2500 Run ID: 2025-11-05T14-30-00Z\n\u251c\u2500\u2500 Queries: 2/2 completed (100%)\n\u251c\u2500\u2500 Total cost: $0.004\n\u251c\u2500\u2500 Brands found: 5 unique\n\u251c\u2500\u2500 Your brands mentioned: 2/2 queries\n\u251c\u2500\u2500 Competitor mentions: 4/2 queries\n\u2514\u2500\u2500 Output: ./output/2025-11-05T14-30-00Z/\n\n\ud83d\udcc1 Artifacts created:\n\u251c\u2500\u2500 report.html - Interactive HTML report\n\u251c\u2500\u2500 run_meta.json - Run summary and metadata\n\u251c\u2500\u2500 *.raw.json - Raw LLM responses\n\u251c\u2500\u2500 *.parsed.json - Extracted brand mentions\n\u2514\u2500\u2500 watcher.db - Historical SQLite database\n</code></pre>"},{"location":"getting-started/first-run/#step-4-explore-the-results","title":"Step 4: Explore the Results","text":""},{"location":"getting-started/first-run/#html-report","title":"HTML Report","text":"<p>Open the interactive report:</p> <pre><code># macOS\nopen ./output/2025-11-05T14-30-00Z/report.html\n\n# Linux\nxdg-open ./output/2025-11-05T14-30-00Z/report.html\n\n# Windows\nstart ./output/2025-11-05T14-30-00Z/report.html\n</code></pre> <p>The report contains:</p>"},{"location":"getting-started/first-run/#summary-section","title":"Summary Section","text":"<ul> <li>Total cost breakdown</li> <li>Queries completed vs failed</li> <li>Unique brands detected</li> <li>Your brand mention rate</li> </ul>"},{"location":"getting-started/first-run/#brand-mentions-table","title":"Brand Mentions Table","text":"Intent Model Your Brand Competitors Rank best-email-warmup-tools gpt-4o-mini Lemwarm (#1) Instantly (#2), HubSpot (#3) 1 email-warmup-comparison gpt-4o-mini Lemwarm (#1), Lemlist (#2) Instantly (#3), Apollo.io (#4) 1"},{"location":"getting-started/first-run/#rank-distribution-chart","title":"Rank Distribution Chart","text":"<p>Visual representation of where your brand appears in ranked lists.</p>"},{"location":"getting-started/first-run/#historical-trends","title":"Historical Trends","text":"<p>If you've run multiple times, you'll see trend charts showing: - Brand mention frequency over time - Average ranking position changes - Competitor appearance patterns</p>"},{"location":"getting-started/first-run/#raw-responses","title":"Raw Responses","text":"<p>Expandable sections showing the full LLM response for each query.</p>"},{"location":"getting-started/first-run/#json-artifacts","title":"JSON Artifacts","text":"<p>Each run creates structured JSON files:</p>"},{"location":"getting-started/first-run/#run_metajson","title":"<code>run_meta.json</code>","text":"<p>Summary of the entire run:</p> <pre><code>{\n  \"run_id\": \"2025-11-05T14-30-00Z\",\n  \"timestamp_utc\": \"2025-11-05T14:30:00Z\",\n  \"config_path\": \"examples/watcher.config.yaml\",\n  \"total_cost_usd\": 0.004,\n  \"queries_completed\": 2,\n  \"queries_failed\": 0,\n  \"brands_detected\": {\n    \"mine\": [\"Lemwarm\", \"Lemlist\"],\n    \"competitors\": [\"Instantly\", \"HubSpot\", \"Apollo.io\"]\n  }\n}\n</code></pre>"},{"location":"getting-started/first-run/#intent__raw_json","title":"<code>intent_*_raw_*.json</code>","text":"<p>Raw LLM response with metadata:</p> <pre><code>{\n  \"intent_id\": \"best-email-warmup-tools\",\n  \"provider\": \"openai\",\n  \"model_name\": \"gpt-4o-mini\",\n  \"prompt\": \"What are the best email warmup tools?\",\n  \"answer_text\": \"Here are the best email warmup tools:\\n\\n1. Lemwarm...\",\n  \"tokens_used\": 532,\n  \"cost_usd\": 0.002,\n  \"timestamp_utc\": \"2025-11-05T14:30:00Z\"\n}\n</code></pre>"},{"location":"getting-started/first-run/#intent__parsed_json","title":"<code>intent_*_parsed_*.json</code>","text":"<p>Extracted brand mentions and ranks:</p> <pre><code>{\n  \"intent_id\": \"best-email-warmup-tools\",\n  \"provider\": \"openai\",\n  \"model_name\": \"gpt-4o-mini\",\n  \"brands_found\": {\n    \"mine\": [\n      {\n        \"brand\": \"Lemwarm\",\n        \"normalized\": \"lemwarm\",\n        \"rank_position\": 1,\n        \"context\": \"1. Lemwarm - Best for automated warmup\"\n      }\n    ],\n    \"competitors\": [\n      {\n        \"brand\": \"Instantly\",\n        \"normalized\": \"instantly\",\n        \"rank_position\": 2,\n        \"context\": \"2. Instantly - Great deliverability features\"\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"getting-started/first-run/#sqlite-database","title":"SQLite Database","text":"<p>All data is stored in <code>./output/watcher.db</code> for historical tracking:</p> <pre><code># Open the database\nsqlite3 ./output/watcher.db\n\n# View recent runs\nSELECT run_id, timestamp_utc, total_cost_usd, queries_completed\nFROM runs\nORDER BY timestamp_utc DESC\nLIMIT 5;\n</code></pre>"},{"location":"getting-started/first-run/#step-5-run-with-different-modes","title":"Step 5: Run with Different Modes","text":""},{"location":"getting-started/first-run/#agent-mode-structured-json-output","title":"Agent Mode (Structured JSON Output)","text":"<p>Perfect for automation and AI agents:</p> <pre><code>llm-answer-watcher run --config examples/watcher.config.yaml --format json\n</code></pre> <p>Output:</p> <pre><code>{\n  \"run_id\": \"2025-11-05T14-30-00Z\",\n  \"status\": \"success\",\n  \"queries_completed\": 2,\n  \"queries_failed\": 0,\n  \"total_cost_usd\": 0.004,\n  \"output_dir\": \"./output/2025-11-05T14-30-00Z\",\n  \"brands_detected\": {\n    \"mine\": [\"Lemwarm\", \"Lemlist\"],\n    \"competitors\": [\"Instantly\", \"HubSpot\", \"Apollo.io\"]\n  }\n}\n</code></pre>"},{"location":"getting-started/first-run/#quiet-mode-minimal-output","title":"Quiet Mode (Minimal Output)","text":"<p>For scripts and pipelines:</p> <pre><code>llm-answer-watcher run --config examples/watcher.config.yaml --quiet\n</code></pre> <p>Output (tab-separated):</p> <pre><code>2025-11-05T14-30-00Z    success 2   0.004   ./output/2025-11-05T14-30-00Z\n</code></pre>"},{"location":"getting-started/first-run/#automation-mode-no-prompts","title":"Automation Mode (No Prompts)","text":"<p>Skip confirmation prompts:</p> <pre><code>llm-answer-watcher run --config examples/watcher.config.yaml --yes --format json\n</code></pre>"},{"location":"getting-started/first-run/#understanding-exit-codes","title":"Understanding Exit Codes","text":"<p>LLM Answer Watcher uses exit codes for automation:</p> <pre><code>llm-answer-watcher run --config config.yaml\necho $?  # Print exit code\n</code></pre> Exit Code Meaning When It Happens 0 Success All queries completed successfully 1 Configuration Error Invalid YAML, missing API keys, bad schema 2 Database Error Cannot create/access SQLite database 3 Partial Failure Some queries failed, but run completed 4 Complete Failure No queries succeeded <p>Use in scripts:</p> <pre><code>#!/bin/bash\nllm-answer-watcher run --config config.yaml --yes\n\ncase $? in\n    0) echo \"\u2705 Success!\" ;;\n    1) echo \"\u274c Configuration error\" &amp;&amp; exit 1 ;;\n    2) echo \"\u274c Database error\" &amp;&amp; exit 1 ;;\n    3) echo \"\u26a0\ufe0f  Partial failure\" ;;\n    4) echo \"\u274c Complete failure\" &amp;&amp; exit 1 ;;\nesac\n</code></pre>"},{"location":"getting-started/first-run/#common-first-run-issues","title":"Common First-Run Issues","text":""},{"location":"getting-started/first-run/#issue-api-key-not-found","title":"Issue: \"API key not found\"","text":"<p>Solution: Ensure API keys are exported:</p> <pre><code>echo $OPENAI_API_KEY  # Should print your key\nexport OPENAI_API_KEY=sk-your-key-here\n</code></pre>"},{"location":"getting-started/first-run/#issue-permission-denied-output","title":"Issue: \"Permission denied: ./output/\"","text":"<p>Solution: Create output directory with correct permissions:</p> <pre><code>mkdir -p output\nchmod 755 output\n</code></pre>"},{"location":"getting-started/first-run/#issue-no-brands-detected","title":"Issue: \"No brands detected\"","text":"<p>Possible causes:</p> <ol> <li>Brand name mismatch: LLM used different name (e.g., \"HubSpot CRM\" vs \"HubSpot\")</li> <li>Not mentioned: Brand wasn't included in LLM response</li> <li>Word boundary issue: Brand name contains special characters</li> </ol> <p>Solution: Check raw response and add brand aliases:</p> <pre><code>brands:\n  mine:\n    - \"YourBrand\"\n    - \"YourBrand.io\"\n    - \"YourBrand CRM\"  # Add variations\n</code></pre>"},{"location":"getting-started/first-run/#issue-rate-limit-exceeded","title":"Issue: \"Rate limit exceeded\"","text":"<p>Solution: LLM API rate limit hit. Wait and retry, or add retry configuration:</p> <pre><code>run_settings:\n  retry_max_attempts: 5\n  retry_wait_exponential_multiplier: 2\n</code></pre>"},{"location":"getting-started/first-run/#next-steps","title":"Next Steps","text":"<p>Now that you've completed your first run:</p> <ul> <li> <p> Customize Configuration</p> <p>Create your own config with your brands and intents</p> <p>Basic Configuration \u2192</p> </li> <li> <p> Query Your Data</p> <p>Use SQL to analyze results and track trends</p> <p>Data Analytics \u2192</p> </li> <li> <p> Add More Providers</p> <p>Compare results across OpenAI, Anthropic, Mistral, and more</p> <p>Provider Guide \u2192</p> </li> <li> <p>:material-calendar-repeat: Automate Runs</p> <p>Set up scheduled monitoring with cron or GitHub Actions</p> <p>Automation \u2192</p> </li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide covers all installation methods for LLM Answer Watcher.</p>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":""},{"location":"getting-started/installation/#python-version","title":"Python Version","text":"<p>LLM Answer Watcher requires Python 3.12 or 3.13. It uses modern Python features including:</p> <ul> <li>Native union type syntax (<code>|</code> instead of <code>Union</code>)</li> <li>Improved type hints</li> <li>Performance optimizations</li> </ul> <p>Check your Python version:</p> <pre><code>python3 --version\n# Should output: Python 3.12.x or Python 3.13.x\n</code></pre>"},{"location":"getting-started/installation/#installing-python-312","title":"Installing Python 3.12+","text":"macOSUbuntu/DebianWindows <pre><code># Using Homebrew\nbrew install python@3.12\n\n# Verify installation\npython3.12 --version\n</code></pre> <pre><code># Add deadsnakes PPA\nsudo add-apt-repository ppa:deadsnakes/ppa\nsudo apt update\n\n# Install Python 3.12\nsudo apt install python3.12 python3.12-venv python3.12-dev\n\n# Verify installation\npython3.12 --version\n</code></pre> <p>Download Python 3.12 from python.org</p> <p>During installation:</p> <ul> <li>\u2705 Check \"Add Python to PATH\"</li> <li>\u2705 Check \"Install pip\"</li> </ul> <p>Verify installation: <pre><code>python --version\n</code></pre></p>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#method-1-uv-recommended","title":"Method 1: uv (Recommended)","text":"<p>uv is a fast, modern Python package installer written in Rust. It's significantly faster than pip and handles virtual environments automatically.</p>"},{"location":"getting-started/installation/#install-uv","title":"Install uv","text":"macOS/LinuxWindowsWith pip <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <pre><code>powershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre> <pre><code>pip install uv\n</code></pre>"},{"location":"getting-started/installation/#install-llm-answer-watcher","title":"Install LLM Answer Watcher","text":"<pre><code># Clone the repository\ngit clone https://github.com/nibzard/llm-answer-watcher.git\ncd llm-answer-watcher\n\n# Install all dependencies (creates .venv automatically)\nuv sync\n\n# For development with extra dependencies\nuv sync --dev\n</code></pre>"},{"location":"getting-started/installation/#activate-virtual-environment","title":"Activate Virtual Environment","text":"<p>uv creates a <code>.venv</code> directory automatically. You can optionally activate it:</p> <pre><code># macOS/Linux\nsource .venv/bin/activate\n\n# Windows\n.venv\\Scripts\\activate\n</code></pre> <p>uv handles activation automatically</p> <p>When you run <code>uv run llm-answer-watcher</code>, uv automatically uses the virtual environment. Explicit activation is optional.</p>"},{"location":"getting-started/installation/#method-2-pip","title":"Method 2: pip","text":"<p>Traditional pip installation with manual virtual environment management.</p> <pre><code># Clone the repository\ngit clone https://github.com/nibzard/llm-answer-watcher.git\ncd llm-answer-watcher\n\n# Create virtual environment\npython3.12 -m venv .venv\n\n# Activate virtual environment\nsource .venv/bin/activate  # macOS/Linux\n# or\n.venv\\Scripts\\activate     # Windows\n\n# Install package in editable mode\npip install -e .\n\n# For development with extra dependencies\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#method-3-pypi-coming-soon","title":"Method 3: PyPI (Coming Soon)","text":"<p>Once published to PyPI, you'll be able to install directly:</p> <pre><code># Future installation method\npip install llm-answer-watcher\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>Check that the installation was successful:</p> <pre><code>llm-answer-watcher --version\n</code></pre> <p>You should see output like:</p> <pre><code>llm-answer-watcher version 0.1.0\n</code></pre> <p>Test the CLI help:</p> <pre><code>llm-answer-watcher --help\n</code></pre>"},{"location":"getting-started/installation/#api-keys-setup","title":"API Keys Setup","text":"<p>LLM Answer Watcher requires API keys from LLM providers. You need at least one provider configured.</p>"},{"location":"getting-started/installation/#supported-providers","title":"Supported Providers","text":"Provider Environment Variable Get API Key OpenAI <code>OPENAI_API_KEY</code> platform.openai.com Anthropic <code>ANTHROPIC_API_KEY</code> console.anthropic.com Mistral <code>MISTRAL_API_KEY</code> console.mistral.ai X.AI (Grok) <code>XAI_API_KEY</code> x.ai/api Google (Gemini) <code>GOOGLE_API_KEY</code> aistudio.google.com Perplexity <code>PERPLEXITY_API_KEY</code> www.perplexity.ai/settings/api"},{"location":"getting-started/installation/#setting-api-keys","title":"Setting API Keys","text":""},{"location":"getting-started/installation/#temporary-current-session","title":"Temporary (Current Session)","text":"<pre><code>export OPENAI_API_KEY=sk-your-key-here\nexport ANTHROPIC_API_KEY=sk-ant-your-key-here\n</code></pre>"},{"location":"getting-started/installation/#persistent-env-file","title":"Persistent (<code>.env</code> file)","text":"<p>Create a <code>.env</code> file in your project directory:</p> <pre><code># .env file\nOPENAI_API_KEY=sk-your-openai-key\nANTHROPIC_API_KEY=sk-ant-your-anthropic-key\nMISTRAL_API_KEY=mistral-your-key\nXAI_API_KEY=xai-your-grok-key\nGOOGLE_API_KEY=AIza-your-google-key\nPERPLEXITY_API_KEY=pplx-your-perplexity-key\n</code></pre> <p>Load the file before running:</p> <pre><code>source .env\n</code></pre> <p>Security: Never commit API keys</p> <p>Add <code>.env</code> to <code>.gitignore</code>: <pre><code>echo \".env\" &gt;&gt; .gitignore\n</code></pre></p>"},{"location":"getting-started/installation/#using-direnv-recommended-for-development","title":"Using direnv (Recommended for Development)","text":"<p>direnv automatically loads <code>.env</code> when you enter the directory:</p> <pre><code># Install direnv\nbrew install direnv  # macOS\n# or\nsudo apt install direnv  # Ubuntu/Debian\n\n# Create .envrc file\necho 'source .env' &gt; .envrc\n\n# Allow direnv to load it\ndirenv allow\n</code></pre> <p>Now your keys load automatically when you <code>cd</code> into the directory.</p>"},{"location":"getting-started/installation/#development-dependencies","title":"Development Dependencies","text":"<p>If you're contributing or want to run tests:</p> <pre><code># With uv\nuv sync --dev\n\n# With pip\npip install -e \".[dev]\"\n</code></pre> <p>This installs additional tools:</p> <ul> <li>pytest - Test runner</li> <li>pytest-httpx - HTTP mocking for tests</li> <li>pytest-cov - Coverage reporting</li> <li>pytest-mock - Advanced mocking</li> <li>freezegun - Time mocking</li> <li>ruff - Fast Python linter and formatter</li> <li>mkdocs - Documentation builder</li> <li>mkdocs-material - Material theme for MkDocs</li> </ul>"},{"location":"getting-started/installation/#docker-installation-optional","title":"Docker Installation (Optional)","text":"<p>For containerized deployment:</p> <pre><code># Dockerfile\nFROM python:3.12-slim\n\nWORKDIR /app\n\n# Install uv\nRUN pip install uv\n\n# Copy project files\nCOPY . .\n\n# Install dependencies\nRUN uv sync\n\n# Set entrypoint\nENTRYPOINT [\"uv\", \"run\", \"llm-answer-watcher\"]\n</code></pre> <p>Build and run:</p> <pre><code>docker build -t llm-answer-watcher .\ndocker run -e OPENAI_API_KEY=$OPENAI_API_KEY \\\n           -v $(pwd)/output:/app/output \\\n           llm-answer-watcher run --config config.yaml\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#python-version-issues","title":"Python Version Issues","text":"<p>If you have multiple Python versions:</p> <pre><code># Use specific Python version\npython3.12 -m venv .venv\nsource .venv/bin/activate\npython --version  # Verify it's 3.12.x\n</code></pre>"},{"location":"getting-started/installation/#permission-errors","title":"Permission Errors","text":"<p>If you get permission errors during installation:</p> <pre><code># Don't use sudo with pip in virtual environments\n# Instead, ensure your virtual environment is activated\nsource .venv/bin/activate\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#ssl-certificate-errors","title":"SSL Certificate Errors","text":"<p>On macOS, you might need to install certificates:</p> <pre><code>/Applications/Python\\ 3.12/Install\\ Certificates.command\n</code></pre>"},{"location":"getting-started/installation/#module-not-found-errors","title":"Module Not Found Errors","text":"<p>If you get <code>ModuleNotFoundError</code> after installation:</p> <pre><code># Ensure you're in the virtual environment\nwhich python  # Should point to .venv/bin/python\n\n# Re-install the package\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#uv-installation-issues","title":"uv Installation Issues","text":"<p>If <code>uv sync</code> fails:</p> <pre><code># Try updating uv\npip install --upgrade uv\n\n# Or fall back to pip\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Now that LLM Answer Watcher is installed:</p> <ol> <li>Run your first monitoring job</li> <li>Learn about configuration</li> <li>Explore supported providers</li> </ol>"},{"location":"getting-started/installation/#uninstallation","title":"Uninstallation","text":"<p>To remove LLM Answer Watcher:</p> <pre><code># Remove the package\npip uninstall llm-answer-watcher\n\n# Remove the virtual environment\nrm -rf .venv\n\n# Remove output data (optional)\nrm -rf output/\n</code></pre>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>Get LLM Answer Watcher up and running in 5 minutes.</p>"},{"location":"getting-started/quick-start/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have:</p> <ul> <li>Python 3.12 or 3.13 installed</li> <li>API keys for at least one LLM provider (OpenAI recommended for getting started)</li> <li>Basic terminal knowledge</li> </ul>"},{"location":"getting-started/quick-start/#installation","title":"Installation","text":""},{"location":"getting-started/quick-start/#option-1-using-uv-recommended","title":"Option 1: Using uv (Recommended)","text":"<p>uv is a fast Python package installer and resolver.</p> <pre><code># Clone the repository\ngit clone https://github.com/nibzard/llm-answer-watcher.git\ncd llm-answer-watcher\n\n# Install dependencies\nuv sync\n\n# Activate virtual environment (optional, uv handles this automatically)\nsource .venv/bin/activate\n</code></pre>"},{"location":"getting-started/quick-start/#option-2-using-pip","title":"Option 2: Using pip","text":"<pre><code># Clone the repository\ngit clone https://github.com/nibzard/llm-answer-watcher.git\ncd llm-answer-watcher\n\n# Create virtual environment\npython3.12 -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install in development mode\npip install -e .\n</code></pre>"},{"location":"getting-started/quick-start/#set-up-api-keys","title":"Set Up API Keys","text":"<p>LLM Answer Watcher uses environment variables for API keys. Set up at least one:</p> <pre><code># OpenAI (recommended for getting started)\nexport OPENAI_API_KEY=sk-your-openai-key-here\n\n# Optional: Add more providers\nexport ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here\nexport MISTRAL_API_KEY=mistral-your-key-here\nexport XAI_API_KEY=xai-your-grok-key-here\nexport GOOGLE_API_KEY=AIza-your-google-api-key-here\nexport PERPLEXITY_API_KEY=pplx-your-perplexity-key-here\n</code></pre> <p>Persistent API Keys</p> <p>Create a <code>.env</code> file to persist your keys: <pre><code>echo \"OPENAI_API_KEY=sk-your-key\" &gt; .env\nsource .env\n</code></pre></p> <p>Add <code>.env</code> to your <code>.gitignore</code> to avoid accidentally committing secrets!</p>"},{"location":"getting-started/quick-start/#your-first-run","title":"Your First Run","text":"<p>LLM Answer Watcher includes example configurations you can use immediately.</p>"},{"location":"getting-started/quick-start/#1-choose-an-example-config","title":"1. Choose an Example Config","text":"<p>The repository includes several example configs in the <code>examples/</code> directory:</p> <ul> <li><code>watcher.config.yaml</code> - Basic configuration with OpenAI</li> <li><code>steel-dev-quick.config.yaml</code> - Quick test configuration</li> <li><code>watcher-with-web-search.config.yaml</code> - Configuration with web search enabled</li> </ul>"},{"location":"getting-started/quick-start/#2-run-the-tool","title":"2. Run the Tool","text":"<pre><code>llm-answer-watcher run --config examples/watcher.config.yaml\n</code></pre>"},{"location":"getting-started/quick-start/#3-view-the-output","title":"3. View the Output","text":"<p>You'll see a beautiful progress display:</p> <pre><code>\ud83d\udd0d Running LLM Answer Watcher...\n\u251c\u2500\u2500 Configuration loaded from examples/watcher.config.yaml\n\u251c\u2500\u2500 Query 1/2: \"What are the best email warmup tools?\"\n\u251c\u2500\u2500 Query 2/2: \"Compare the top email warmup tools\"\n\u251c\u2500\u2500 Models: OpenAI gpt-4o-mini\n\u251c\u2500\u2500 Brands: 2 monitored, 4 competitors\n\u2514\u2500\u2500 Output: ./output/2025-11-05T14-30-00Z/\n\n\u2705 Queries completed: 2/2\n\ud83d\udcb0 Total cost: $0.0042\n\ud83d\udcca Report: ./output/2025-11-05T14-30-00Z/report.html\n</code></pre>"},{"location":"getting-started/quick-start/#4-explore-results","title":"4. Explore Results","text":"<p>Open the HTML report in your browser:</p> <pre><code>open ./output/2025-11-05T14-30-00Z/report.html\n# Or on Linux:\nxdg-open ./output/2025-11-05T14-30-00Z/report.html\n</code></pre> <p>The report shows:</p> <ul> <li>Summary: Total costs, queries completed, brands found</li> <li>Brand Mentions: Which brands appeared in each response</li> <li>Rank Distribution: Visual charts of ranking positions</li> <li>Raw Responses: Full LLM outputs for inspection</li> </ul>"},{"location":"getting-started/quick-start/#understanding-the-output","title":"Understanding the Output","text":"<p>Each run creates a timestamped directory with:</p> <pre><code>output/2025-11-05T14-30-00Z/\n\u251c\u2500\u2500 run_meta.json                    # Run summary and stats\n\u251c\u2500\u2500 report.html                      # Interactive HTML report\n\u251c\u2500\u2500 intent_*_raw_*.json             # Raw LLM responses\n\u251c\u2500\u2500 intent_*_parsed_*.json          # Extracted brand mentions\n\u2514\u2500\u2500 intent_*_error_*.json           # Error details (if any)\n</code></pre> <p>All data is also stored in a SQLite database at <code>./output/watcher.db</code> for historical analysis.</p>"},{"location":"getting-started/quick-start/#whats-next","title":"What's Next?","text":"<p>Now that you've run your first monitoring job, here are suggested next steps:</p>"},{"location":"getting-started/quick-start/#create-your-own-configuration","title":"Create Your Own Configuration","text":"<p>Create <code>my-watcher.config.yaml</code>:</p> <pre><code>run_settings:\n  output_dir: \"./output\"\n  sqlite_db_path: \"./output/watcher.db\"\n\n  models:\n    - provider: \"openai\"\n      model_name: \"gpt-4o-mini\"\n      env_api_key: \"OPENAI_API_KEY\"\n\nbrands:\n  mine:\n    - \"YourBrand\"\n    - \"YourBrand.io\"\n\n  competitors:\n    - \"CompetitorA\"\n    - \"CompetitorB\"\n    - \"IndustryTool\"\n\nintents:\n  - id: \"best-tools-in-category\"\n    prompt: \"What are the best [your category] tools?\"\n\n  - id: \"comparison-query\"\n    prompt: \"Compare the top [your category] tools\"\n</code></pre> <p>Then run:</p> <pre><code>llm-answer-watcher run --config my-watcher.config.yaml\n</code></pre>"},{"location":"getting-started/quick-start/#explore-more-features","title":"Explore More Features","text":"<ul> <li> <p> Configuration Deep Dive</p> <p>Learn about all configuration options</p> <p>Configuration Guide \u2192</p> </li> <li> <p> Multiple Providers</p> <p>Add Anthropic, Mistral, Grok, and more</p> <p>Provider Guide \u2192</p> </li> <li> <p> Query Your Data</p> <p>Use SQL to analyze historical trends</p> <p>Data Analytics \u2192</p> </li> <li> <p> Automate Monitoring</p> <p>Set up scheduled runs with cron or GitHub Actions</p> <p>Automation Guide \u2192</p> </li> </ul>"},{"location":"getting-started/quick-start/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/quick-start/#command-not-found-llm-answer-watcher","title":"\"Command not found: llm-answer-watcher\"","text":"<p>Make sure you've activated your virtual environment:</p> <pre><code>source .venv/bin/activate  # On macOS/Linux\n# or\n.venv\\Scripts\\activate     # On Windows\n</code></pre>"},{"location":"getting-started/quick-start/#configuration-error-api-key-not-found","title":"\"Configuration error: API key not found\"","text":"<p>Ensure your API keys are exported:</p> <pre><code>echo $OPENAI_API_KEY  # Should print your key\n</code></pre> <p>If empty, export it:</p> <pre><code>export OPENAI_API_KEY=sk-your-key-here\n</code></pre>"},{"location":"getting-started/quick-start/#importerror-no-module-named-llm_answer_watcher","title":"\"ImportError: No module named 'llm_answer_watcher'\"","text":"<p>Re-install the package:</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"getting-started/quick-start/#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: Browse this site for comprehensive guides</li> <li>Examples: Check the <code>examples/</code> directory in the repository</li> <li>Issues: Report bugs or ask questions</li> <li>Contributing: Read the contributing guide</li> </ul> <p>Ready to dive deeper? Continue to the Installation Guide for more installation options.</p>"},{"location":"providers/anthropic/","title":"Anthropic Provider","text":"<p>Integration with Anthropic's Claude models.</p>"},{"location":"providers/anthropic/#supported-models","title":"Supported Models","text":"<ul> <li><code>claude-3-5-sonnet-20241022</code> - Latest Sonnet</li> <li><code>claude-3-5-haiku-20241022</code> - Fast and affordable</li> <li><code>claude-3-opus-20240229</code> - Most capable</li> </ul>"},{"location":"providers/anthropic/#configuration","title":"Configuration","text":"<pre><code>models:\n  - provider: \"anthropic\"\n    model_name: \"claude-3-5-haiku-20241022\"\n    env_api_key: \"ANTHROPIC_API_KEY\"\n</code></pre>"},{"location":"providers/anthropic/#getting-api-key","title":"Getting API Key","text":"<ol> <li>Visit console.anthropic.com</li> <li>Get your API key</li> <li>Export: <code>export ANTHROPIC_API_KEY=sk-ant-your-key</code></li> </ol>"},{"location":"providers/anthropic/#pricing","title":"Pricing","text":"<ul> <li>Haiku: $0.80/1M input, $4/1M output</li> <li>Sonnet: $3/1M input, $15/1M output</li> </ul> <p>See Providers Overview for comparison.</p>"},{"location":"providers/google/","title":"Google Gemini Provider","text":"<p>Integration with Google's Gemini models, including support for Google Search grounding.</p>"},{"location":"providers/google/#overview","title":"Overview","text":"<p>Google Gemini is a family of multimodal AI models that excels at understanding and generating text. Gemini models are available through Google AI Studio and support Google Search grounding for real-time web information.</p> <p>Key Features:</p> <ul> <li>Google Search Grounding: Access real-time web data with no additional per-request fees</li> <li>Competitive Pricing: Among the most cost-effective LLMs with high quality</li> <li>Automatic Search Decision: Gemini intelligently decides when to use Google Search</li> <li>Grounding Metadata: Rich attribution showing which sources influenced responses</li> </ul>"},{"location":"providers/google/#supported-models","title":"Supported Models","text":""},{"location":"providers/google/#gemini-25-series-recommended","title":"Gemini 2.5 Series (Recommended)","text":"Model Speed Quality Grounding Best For <code>gemini-2.5-flash</code> Fast High \u2705 Yes Production - balanced speed/quality/cost <code>gemini-2.5-flash-lite</code> Fastest Medium \u274c No High-volume, non-grounded queries <code>gemini-2.5-pro</code> Slower Highest \u2705 Yes Complex reasoning, highest quality"},{"location":"providers/google/#gemini-20-series","title":"Gemini 2.0 Series","text":"Model Speed Quality Grounding Best For <code>gemini-2.0-flash-exp</code> Fast High \u26a0\ufe0f Experimental Testing new features <code>gemini-2.0-flash-lite</code> Fastest Medium \u274c No Fast, non-grounded queries"},{"location":"providers/google/#legacy-models-not-recommended","title":"Legacy Models (Not Recommended)","text":"<ul> <li><code>gemini-1.5-pro</code> - Superseded by 2.5-pro</li> <li><code>gemini-1.5-flash</code> - Superseded by 2.5-flash</li> </ul> <p>Recommendation: Use <code>gemini-2.5-flash</code> for production workloads. It provides excellent performance with Google Search grounding support at competitive pricing.</p>"},{"location":"providers/google/#basic-configuration","title":"Basic Configuration","text":""},{"location":"providers/google/#without-google-search-grounding","title":"Without Google Search Grounding","text":"<p>Standard Gemini usage with training data only:</p> <pre><code>models:\n  - provider: \"google\"\n    model_name: \"gemini-2.5-flash-lite\"\n    env_api_key: \"GEMINI_API_KEY\"\n</code></pre> <p>Use when: - You don't need real-time information - Faster response times are critical - Cost optimization is priority</p>"},{"location":"providers/google/#with-google-search-grounding","title":"With Google Search Grounding","text":"<p>Enable real-time web information:</p> <pre><code>models:\n  - provider: \"google\"\n    model_name: \"gemini-2.5-flash\"\n    env_api_key: \"GEMINI_API_KEY\"\n    system_prompt: \"google/gemini-grounding\"\n    tools:\n      - google_search: {}\n</code></pre> <p>Use when: - Brand monitoring requires current data - Tracking real-time competitive landscape - Need to detect recent changes - Want Google's search quality</p>"},{"location":"providers/google/#google-search-grounding","title":"Google Search Grounding","text":""},{"location":"providers/google/#configuration-format","title":"Configuration Format","text":"<p>Google uses a unique tools configuration format:</p> <pre><code>tools:\n  - google_search: {}  # Dictionary with tool name as key\n</code></pre> <p>This differs from OpenAI's format: <pre><code>tools:\n  - type: \"web_search\"  # Dictionary with 'type' field\ntool_choice: \"auto\"\n</code></pre></p> <p>Why the difference? Each provider has different API specifications. Google uses named tool objects, OpenAI uses typed specifications. The config does direct passthrough to each API.</p>"},{"location":"providers/google/#supported-models-for-grounding","title":"Supported Models for Grounding","text":"Model Grounding Support <code>gemini-2.5-flash</code> \u2705 Yes (recommended) <code>gemini-2.5-flash-lite</code> \u274c No <code>gemini-2.5-pro</code> \u2705 Yes (highest quality) <code>gemini-2.0-flash-exp</code> \u26a0\ufe0f Experimental <code>gemini-2.0-flash-lite</code> \u274c No"},{"location":"providers/google/#system-prompt-optimization","title":"System Prompt Optimization","text":"<p>Use the specialized <code>google/gemini-grounding</code> system prompt for best results:</p> <pre><code>models:\n  - provider: \"google\"\n    model_name: \"gemini-2.5-flash\"\n    env_api_key: \"GEMINI_API_KEY\"\n    system_prompt: \"google/gemini-grounding\"  # Optimized for grounding\n    tools:\n      - google_search: {}\n</code></pre> <p>This prompt: - Instructs Gemini to use Google Search when beneficial - Emphasizes grounding responses in search results - Requests comprehensive source coverage - Improves answer quality for brand monitoring</p>"},{"location":"providers/google/#how-grounding-works","title":"How Grounding Works","text":"<ol> <li>Gemini receives your query prompt</li> <li>Automatically decides if Google Search would improve the answer</li> <li>Performs search if beneficial (no <code>tool_choice</code> parameter needed)</li> <li>Grounds response in search results</li> <li>Returns answer with grounding metadata</li> </ol> <p>No explicit control: Unlike OpenAI's <code>tool_choice: \"required\"</code>, Gemini intelligently determines when grounding helps. This is intentional - Gemini optimizes for quality and cost.</p>"},{"location":"providers/google/#grounding-metadata","title":"Grounding Metadata","text":"<p>Responses include rich grounding attribution:</p> <pre><code>{\n  \"web_search_results\": {\n    \"web_search_queries\": [\"best email warmup tools 2025\"],\n    \"grounding_chunks\": [\n      {\n        \"web_source\": \"https://www.g2.com/categories/email-warmup\",\n        \"retrieved_context\": \"Top email warmup tools...\"\n      }\n    ],\n    \"grounding_supports\": [\n      {\n        \"segment\": {\n          \"text\": \"Warmly is a leading solution\"\n        },\n        \"grounding_chunk_indices\": [0],\n        \"confidence_scores\": [0.95]\n      }\n    ]\n  },\n  \"web_search_count\": 1\n}\n</code></pre> <p>Key fields: - <code>web_search_queries</code>: What Gemini searched for - <code>grounding_chunks</code>: Source URLs and context - <code>grounding_supports</code>: Which text segments came from which sources - <code>confidence_scores</code>: How confident Gemini is (0.0-1.0)</p>"},{"location":"providers/google/#pricing","title":"Pricing","text":""},{"location":"providers/google/#token-costs","title":"Token Costs","text":"Model Input Output <code>gemini-2.5-flash</code> $0.04 / 1M tokens $0.12 / 1M tokens <code>gemini-2.5-flash-lite</code> $0.02 / 1M tokens $0.06 / 1M tokens <code>gemini-2.5-pro</code> $0.60 / 1M tokens $1.80 / 1M tokens"},{"location":"providers/google/#google-search-grounding-costs","title":"Google Search Grounding Costs","text":"<p>Good news: No additional fees for grounding. You only pay token costs.</p> <p>Example (email warmup query with grounding): <pre><code>Input: 100 tokens @ $0.04/1M = $0.000004\nOutput: 300 tokens @ $0.12/1M = $0.000036\nTotal: $0.00004 per query\n</code></pre></p> <p>Comparison: - Gemini with grounding: $0.00004 per query - OpenAI web search: $0.0116 per query (~290x more) - Perplexity sonar-pro: \\(0.005-\\)0.03 per query (125-750x more)</p> <p>Cost Advantage</p> <p>Google Search grounding is significantly cheaper than alternatives. Grounding tokens are included in base pricing with no per-request fees.</p>"},{"location":"providers/google/#complete-configuration-example","title":"Complete Configuration Example","text":""},{"location":"providers/google/#multi-model-strategy","title":"Multi-Model Strategy","text":"<p>Use different models for different use cases:</p> <pre><code>run_settings:\n  output_dir: \"./output\"\n  sqlite_db_path: \"./output/watcher.db\"\n\n  models:\n    # High-volume: Fast + cheap without grounding\n    - provider: \"google\"\n      model_name: \"gemini-2.5-flash-lite\"\n      env_api_key: \"GEMINI_API_KEY\"\n\n    # Brand monitoring: Balanced with grounding\n    - provider: \"google\"\n      model_name: \"gemini-2.5-flash\"\n      env_api_key: \"GEMINI_API_KEY\"\n      system_prompt: \"google/gemini-grounding\"\n      tools:\n        - google_search: {}\n\n    # Premium: Highest quality with grounding\n    - provider: \"google\"\n      model_name: \"gemini-2.5-pro\"\n      env_api_key: \"GEMINI_API_KEY\"\n      system_prompt: \"google/gemini-grounding\"\n      tools:\n        - google_search: {}\n\nbrands:\n  mine:\n    - \"Warmly\"\n  competitors:\n    - \"HubSpot\"\n    - \"Instantly\"\n\nintents:\n  - id: \"email-warmup-tools\"\n    prompt: \"What are the best email warmup tools in 2025?\"\n</code></pre>"},{"location":"providers/google/#getting-api-key","title":"Getting API Key","text":"<ol> <li>Visit Google AI Studio</li> <li>Sign in with your Google account</li> <li>Click \"Create API key\"</li> <li>Copy the key (format: <code>AIza...</code>)</li> <li>Export to environment:    <pre><code>export GEMINI_API_KEY=AIza-your-key-here\n</code></pre></li> </ol> <p>API Key Security</p> <ul> <li>Never commit API keys to version control</li> <li>Use environment variables or secret management</li> <li>Rotate keys periodically</li> <li>Monitor usage in AI Studio dashboard</li> </ul>"},{"location":"providers/google/#when-to-use-gemini","title":"When to Use Gemini","text":""},{"location":"providers/google/#choose-gemini-when","title":"Choose Gemini When:","text":"<ul> <li>\u2705 Cost optimization: Among the cheapest high-quality LLMs</li> <li>\u2705 Google Search quality: Want Google's search coverage and accuracy</li> <li>\u2705 High-volume monitoring: Grounding with no per-request fees</li> <li>\u2705 Automatic search decision: Trust Gemini to decide when to ground</li> <li>\u2705 Grounding metadata: Need detailed source attribution</li> </ul>"},{"location":"providers/google/#choose-other-providers-when","title":"Choose Other Providers When:","text":"<p>OpenAI: - Need explicit <code>tool_choice</code> control (force/disable search) - Prefer OpenAI's reasoning quality - Already invested in OpenAI ecosystem</p> <p>Perplexity: - Need explicit source URLs in every response - Want always-on web search with citations - Prefer Perplexity's citation format</p> <p>Anthropic: - Need longest context windows (200K+) - Prefer Claude's reasoning style - Don't need web search</p>"},{"location":"providers/google/#best-practices","title":"Best Practices","text":""},{"location":"providers/google/#1-use-appropriate-model-tiers","title":"1. Use Appropriate Model Tiers","text":"<pre><code># High-volume, non-grounded queries\n- model_name: \"gemini-2.5-flash-lite\"\n\n# Production brand monitoring (recommended)\n- model_name: \"gemini-2.5-flash\"\n  tools: [google_search: {}]\n\n# Premium quality for critical queries\n- model_name: \"gemini-2.5-pro\"\n  tools: [google_search: {}]\n</code></pre>"},{"location":"providers/google/#2-enable-grounding-for-brand-monitoring","title":"2. Enable Grounding for Brand Monitoring","text":"<pre><code># \u2705 GOOD - Grounding for current brand data\n- provider: \"google\"\n  model_name: \"gemini-2.5-flash\"\n  system_prompt: \"google/gemini-grounding\"\n  tools:\n    - google_search: {}\n\nintents:\n  - id: \"current-tools\"\n    prompt: \"What are the best email tools in 2025?\"\n</code></pre>"},{"location":"providers/google/#3-skip-grounding-for-historicalgeneric-queries","title":"3. Skip Grounding for Historical/Generic Queries","text":"<pre><code># \u2705 GOOD - No grounding for general knowledge\n- provider: \"google\"\n  model_name: \"gemini-2.5-flash-lite\"\n\nintents:\n  - id: \"email-best-practices\"\n    prompt: \"What are email deliverability best practices?\"\n</code></pre>"},{"location":"providers/google/#4-use-grounding-optimized-system-prompt","title":"4. Use Grounding-Optimized System Prompt","text":"<pre><code># \u2705 GOOD\nsystem_prompt: \"google/gemini-grounding\"  # Optimized\n\n# \u274c SUBOPTIMAL\n# (no system_prompt or using \"google/default\")\n</code></pre>"},{"location":"providers/google/#5-monitor-grounding-usage","title":"5. Monitor Grounding Usage","text":"<p>Track when Gemini uses grounding:</p> <pre><code># Check if grounding was used\nif result[\"web_search_count\"] &gt; 0:\n    print(f\"Grounding used: {result['web_search_count']} searches\")\n    print(f\"Queries: {result['web_search_results']['web_search_queries']}\")\n</code></pre>"},{"location":"providers/google/#troubleshooting","title":"Troubleshooting","text":""},{"location":"providers/google/#grounding-not-working","title":"Grounding Not Working","text":"<p>Problem: <code>web_search_count</code> is always 0</p> <p>Solutions: 1. Check you're using a grounding-capable model:    <pre><code># \u2705 Grounding supported\nmodel_name: \"gemini-2.5-flash\"\n\n# \u274c Grounding NOT supported\nmodel_name: \"gemini-2.5-flash-lite\"\n</code></pre></p> <ol> <li> <p>Verify tools configuration format:    <pre><code># \u2705 Correct\ntools:\n  - google_search: {}\n\n# \u274c Wrong (OpenAI format)\ntools:\n  - type: \"web_search\"\n</code></pre></p> </li> <li> <p>Use grounding-optimized system prompt:    <pre><code>system_prompt: \"google/gemini-grounding\"\n</code></pre></p> </li> </ol>"},{"location":"providers/google/#api-authentication-errors","title":"API Authentication Errors","text":"<p>Problem: <code>401 Unauthorized</code> or <code>403 Forbidden</code></p> <p>Solutions: 1. Verify API key is correct:    <pre><code>echo $GEMINI_API_KEY  # Should show AIza...\n</code></pre></p> <ol> <li> <p>Check key is active in AI Studio</p> </li> <li> <p>Verify key has correct permissions</p> </li> </ol>"},{"location":"providers/google/#rate-limiting","title":"Rate Limiting","text":"<p>Problem: <code>429 Too Many Requests</code></p> <p>Solutions: 1. Reduce <code>max_concurrent_requests</code> in config:    <pre><code>run_settings:\n  max_concurrent_requests: 3  # Google limit\n</code></pre></p> <ol> <li> <p>Add delay between requests</p> </li> <li> <p>Upgrade to higher quota tier in AI Studio</p> </li> </ol>"},{"location":"providers/google/#further-reading","title":"Further Reading","text":"<ul> <li>Web Search Configuration - Detailed grounding setup</li> <li>Model Configuration - Model selection guide</li> <li>Providers Overview - Compare all providers</li> <li>Google AI Studio - Official documentation</li> </ul>"},{"location":"providers/grok/","title":"X.AI Grok Provider","text":"<p>Integration with X.AI's Grok models.</p>"},{"location":"providers/grok/#supported-models","title":"Supported Models","text":"<ul> <li><code>grok-beta</code></li> <li><code>grok-2-1212</code></li> <li><code>grok-2-latest</code></li> <li><code>grok-3</code></li> <li><code>grok-3-mini</code></li> </ul>"},{"location":"providers/grok/#configuration","title":"Configuration","text":"<pre><code>models:\n  - provider: \"grok\"\n    model_name: \"grok-2-1212\"\n    env_api_key: \"XAI_API_KEY\"\n</code></pre>"},{"location":"providers/grok/#getting-api-key","title":"Getting API Key","text":"<ol> <li>Visit x.ai/api</li> <li>Get API access</li> <li>Export: <code>export XAI_API_KEY=xai-your-key</code></li> </ol> <p>See Providers Overview for comparison.</p>"},{"location":"providers/mistral/","title":"Mistral AI Provider","text":"<p>Integration with Mistral's models.</p>"},{"location":"providers/mistral/#supported-models","title":"Supported Models","text":"<ul> <li><code>mistral-large-latest</code></li> <li><code>mistral-small-latest</code></li> </ul>"},{"location":"providers/mistral/#configuration","title":"Configuration","text":"<pre><code>models:\n  - provider: \"mistral\"\n    model_name: \"mistral-small-latest\"\n    env_api_key: \"MISTRAL_API_KEY\"\n</code></pre>"},{"location":"providers/mistral/#getting-api-key","title":"Getting API Key","text":"<ol> <li>Visit console.mistral.ai</li> <li>Generate API key</li> <li>Export: <code>export MISTRAL_API_KEY=your-key</code></li> </ol> <p>See Providers Overview for comparison.</p>"},{"location":"providers/openai/","title":"OpenAI Provider","text":"<p>Integration with OpenAI's GPT models.</p>"},{"location":"providers/openai/#supported-models","title":"Supported Models","text":"<ul> <li><code>gpt-4o</code> - Latest GPT-4 Optimized</li> <li><code>gpt-4o-mini</code> - Cost-effective model (recommended)</li> <li><code>gpt-4-turbo</code> - Fast GPT-4</li> <li><code>gpt-3.5-turbo</code> - Legacy model</li> </ul>"},{"location":"providers/openai/#configuration","title":"Configuration","text":"<pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n</code></pre>"},{"location":"providers/openai/#getting-api-key","title":"Getting API Key","text":"<ol> <li>Visit platform.openai.com</li> <li>Create new secret key</li> <li>Export: <code>export OPENAI_API_KEY=sk-your-key</code></li> </ol>"},{"location":"providers/openai/#pricing","title":"Pricing","text":"<ul> <li>gpt-4o-mini: $0.15/1M input, $0.60/1M output</li> <li>gpt-4o: $2.50/1M input, $10/1M output</li> </ul>"},{"location":"providers/openai/#web-search-tool","title":"Web Search Tool","text":"<p>OpenAI supports web search through the <code>web_search</code> tool in the Responses API.</p>"},{"location":"providers/openai/#basic-web-search-configuration","title":"Basic Web Search Configuration","text":"<pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    tools:\n      - type: \"web_search\"\n    tool_choice: \"auto\"  # Model decides when to search\n</code></pre>"},{"location":"providers/openai/#tool-choice-options","title":"Tool Choice Options","text":"<ul> <li><code>auto</code> (recommended): Model decides when web search is needed</li> <li><code>required</code>: Force web search for every query</li> <li><code>none</code>: Disable web search</li> </ul>"},{"location":"providers/openai/#web-search-pricing","title":"Web Search Pricing","text":"<p>Web search adds $10 per 1,000 calls plus content token costs.</p> <p>Example cost (gpt-4o-mini): <pre><code>Base query: $0.0004 (tokens only)\n+ Web search call: $0.01\n+ Search content: $0.0012 (8k tokens @ $0.15/1M)\n= Total: ~$0.0116 per query\n</code></pre></p>"},{"location":"providers/openai/#when-to-use-openai-web-search","title":"When to Use OpenAI Web Search","text":"<p>Use OpenAI when: - \u2705 Need explicit <code>tool_choice</code> control - \u2705 Prefer OpenAI's LLM reasoning quality - \u2705 Already invested in OpenAI ecosystem</p> <p>Consider alternatives: - Google Gemini grounding: 290x cheaper (~$0.00004 vs $0.0116) - Perplexity: Built-in citations, always-on search</p> <p>See Web Search Configuration for detailed setup and comparison.</p>"},{"location":"providers/openai/#further-reading","title":"Further Reading","text":"<ul> <li>Web Search Configuration - Detailed web search setup</li> <li>Model Configuration - Model selection guide</li> <li>Providers Overview - Compare all providers</li> </ul>"},{"location":"providers/overview/","title":"Provider Overview","text":"<p>LLM Answer Watcher supports 6 major LLM providers with a unified interface. Choose providers based on cost, performance, and feature requirements.</p> <p>\ud83c\udf10 New in v0.2.0: Browser Runners - Access ChatGPT and Perplexity via web UI automation to capture the true user experience. See Browser vs API Access below.</p>"},{"location":"providers/overview/#supported-providers","title":"Supported Providers","text":"Provider Models Cost Range Web Search Best For OpenAI gpt-4o-mini, gpt-4o, more \\(0.15-\\)10/1M tokens \u2705 Yes General use, cost-effective Anthropic Claude 3.5 Haiku, Sonnet, Opus \\(0.80-\\)75/1M tokens \u274c No High-quality responses Mistral mistral-large, mistral-small \\(0.30-\\)2/1M tokens \u274c No European alternative X.AI (Grok) grok-beta, grok-2, grok-3 \\(2-\\)25/1M tokens \u274c No X platform integration Google Gemini 2.0 Flash \\(0.075-\\)0.30/1M tokens \u274c No Low-cost option Perplexity Sonar, Sonar Pro \\(1-\\)15/1M tokens \u2705 Built-in Grounded responses"},{"location":"providers/overview/#quick-configuration","title":"Quick Configuration","text":""},{"location":"providers/overview/#single-provider","title":"Single Provider","text":"<pre><code>run_settings:\n  models:\n    - provider: \"openai\"\n      model_name: \"gpt-4o-mini\"\n      env_api_key: \"OPENAI_API_KEY\"\n</code></pre>"},{"location":"providers/overview/#multiple-providers","title":"Multiple Providers","text":"<pre><code>run_settings:\n  models:\n    - provider: \"openai\"\n      model_name: \"gpt-4o-mini\"\n      env_api_key: \"OPENAI_API_KEY\"\n\n    - provider: \"anthropic\"\n      model_name: \"claude-3-5-haiku-20241022\"\n      env_api_key: \"ANTHROPIC_API_KEY\"\n\n    - provider: \"perplexity\"\n      model_name: \"sonar-pro\"\n      env_api_key: \"PERPLEXITY_API_KEY\"\n</code></pre>"},{"location":"providers/overview/#provider-selection-guide","title":"Provider Selection Guide","text":""},{"location":"providers/overview/#by-budget","title":"By Budget","text":"<p>Ultra Low Cost (&lt;$0.005 per query):</p> <ul> <li>Google Gemini 2.0 Flash</li> <li>OpenAI gpt-4o-mini</li> </ul> <p>Low Cost ($0.005-0.01 per query):</p> <ul> <li>Mistral mistral-small</li> <li>Anthropic Claude 3.5 Haiku</li> </ul> <p>Medium Cost ($0.01-0.05 per query):</p> <ul> <li>OpenAI gpt-4o</li> <li>Anthropic Claude 3.5 Sonnet</li> <li>Perplexity Sonar Pro</li> </ul> <p>High Cost (&gt;$0.05 per query):</p> <ul> <li>Anthropic Claude 3.5 Opus</li> <li>Grok grok-3</li> <li>OpenAI gpt-4-turbo</li> </ul>"},{"location":"providers/overview/#by-feature","title":"By Feature","text":"<p>Web Search Required:</p> <ul> <li>\u2705 OpenAI (with tools configuration)</li> <li>\u2705 Perplexity (built-in)</li> </ul> <p>No Web Search:</p> <ul> <li>Anthropic, Mistral, Grok, Google</li> </ul> <p>Grounded Responses:</p> <ul> <li>\u2705 Perplexity (best)</li> <li>\u2705 OpenAI with web search</li> </ul> <p>High Quality:</p> <ul> <li>Anthropic Claude 3.5 Sonnet/Opus</li> <li>OpenAI gpt-4o</li> <li>Perplexity Sonar Pro</li> </ul> <p>Fast Response:</p> <ul> <li>OpenAI gpt-4o-mini</li> <li>Google Gemini Flash</li> <li>Mistral mistral-small</li> </ul>"},{"location":"providers/overview/#by-use-case","title":"By Use Case","text":"<p>Cost-Optimized Monitoring:</p> <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n</code></pre> <p>High-Quality Analysis:</p> <pre><code>models:\n  - provider: \"anthropic\"\n    model_name: \"claude-3-5-sonnet-20241022\"\n</code></pre> <p>Multi-Provider Comparison:</p> <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n  - provider: \"anthropic\"\n    model_name: \"claude-3-5-haiku-20241022\"\n  - provider: \"perplexity\"\n    model_name: \"sonar\"\n</code></pre> <p>Web Search Required:</p> <pre><code>models:\n  - provider: \"perplexity\"\n    model_name: \"sonar-pro\"\n</code></pre>"},{"location":"providers/overview/#api-key-setup","title":"API Key Setup","text":""},{"location":"providers/overview/#openai","title":"OpenAI","text":"<pre><code>export OPENAI_API_KEY=sk-your-openai-key-here\n</code></pre> <p>Get key: https://platform.openai.com/api-keys</p>"},{"location":"providers/overview/#anthropic","title":"Anthropic","text":"<pre><code>export ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here\n</code></pre> <p>Get key: https://console.anthropic.com/</p>"},{"location":"providers/overview/#mistral","title":"Mistral","text":"<pre><code>export MISTRAL_API_KEY=your-mistral-key-here\n</code></pre> <p>Get key: https://console.mistral.ai/</p>"},{"location":"providers/overview/#xai-grok","title":"X.AI (Grok)","text":"<pre><code>export XAI_API_KEY=xai-your-grok-key-here\n</code></pre> <p>Get key: https://console.x.ai/</p>"},{"location":"providers/overview/#google-gemini","title":"Google Gemini","text":"<pre><code>export GOOGLE_API_KEY=AIza-your-google-api-key-here\n</code></pre> <p>Get key: https://aistudio.google.com/apikey</p>"},{"location":"providers/overview/#perplexity","title":"Perplexity","text":"<pre><code>export PERPLEXITY_API_KEY=pplx-your-perplexity-key-here\n</code></pre> <p>Get key: https://www.perplexity.ai/settings/api</p>"},{"location":"providers/overview/#provider-comparison","title":"Provider Comparison","text":""},{"location":"providers/overview/#response-quality","title":"Response Quality","text":"<p>Best to Good:</p> <ol> <li>Anthropic Claude 3.5 Opus</li> <li>Anthropic Claude 3.5 Sonnet</li> <li>OpenAI gpt-4o</li> <li>Perplexity Sonar Pro</li> <li>Mistral mistral-large</li> <li>Grok grok-3</li> <li>Anthropic Claude 3.5 Haiku</li> <li>OpenAI gpt-4o-mini</li> <li>Google Gemini 2.0 Flash</li> <li>Mistral mistral-small</li> </ol>"},{"location":"providers/overview/#cost-efficiency","title":"Cost Efficiency","text":"<p>Best value (quality per dollar):</p> <ol> <li>OpenAI gpt-4o-mini</li> <li>Google Gemini 2.0 Flash</li> <li>Anthropic Claude 3.5 Haiku</li> <li>Mistral mistral-small</li> <li>Perplexity Sonar</li> </ol>"},{"location":"providers/overview/#speed","title":"Speed","text":"<p>Fastest to Slowest:</p> <ol> <li>Google Gemini Flash</li> <li>OpenAI gpt-4o-mini</li> <li>Mistral models</li> <li>Perplexity Sonar</li> <li>Anthropic Haiku</li> <li>OpenAI gpt-4o</li> <li>Anthropic Sonnet</li> <li>Grok models</li> <li>Anthropic Opus</li> </ol>"},{"location":"providers/overview/#rate-limits","title":"Rate Limits","text":"<p>Default rate limits (check provider docs for current limits):</p> Provider Requests/Min Tokens/Min OpenAI 500 90,000 Anthropic 50 100,000 Mistral 5-60 Varies X.AI 60 120,000 Google 15 32,000 Perplexity 20 Varies <p>Recommendation: Add delays between queries if hitting rate limits:</p> <pre><code>run_settings:\n  delay_between_queries: 2  # seconds\n</code></pre>"},{"location":"providers/overview/#provider-specific-features","title":"Provider-Specific Features","text":""},{"location":"providers/overview/#openai_1","title":"OpenAI","text":"<ul> <li>\u2705 Web search via tools</li> <li>\u2705 Function calling</li> <li>\u2705 JSON mode</li> <li>\u2705 Vision support (not used)</li> </ul> <p>See OpenAI Provider</p>"},{"location":"providers/overview/#anthropic_1","title":"Anthropic","text":"<ul> <li>\u2705 Extended context (200K tokens)</li> <li>\u2705 Function calling</li> <li>\u2705 JSON mode</li> <li>\u2705 Thinking mode (not used)</li> </ul> <p>See Anthropic Provider</p>"},{"location":"providers/overview/#mistral_1","title":"Mistral","text":"<ul> <li>\u2705 European data residency</li> <li>\u2705 Function calling</li> <li>\u2705 JSON mode</li> <li>\u2705 Competitive pricing</li> </ul> <p>See Mistral Provider</p>"},{"location":"providers/overview/#xai-grok_1","title":"X.AI (Grok)","text":"<ul> <li>\u2705 X platform integration</li> <li>\u2705 OpenAI-compatible API</li> <li>\u2705 Real-time information</li> <li>\u26a0\ufe0f Limited model selection</li> </ul> <p>See Grok Provider</p>"},{"location":"providers/overview/#google","title":"Google","text":"<ul> <li>\u2705 Very low cost</li> <li>\u2705 Fast responses</li> <li>\u2705 Long context (1M tokens)</li> <li>\u26a0\ufe0f Newer platform</li> </ul> <p>See Google Provider</p>"},{"location":"providers/overview/#perplexity_1","title":"Perplexity","text":"<ul> <li>\u2705 Built-in web search</li> <li>\u2705 Grounded responses</li> <li>\u2705 Citations included</li> <li>\u2705 Real-time information</li> <li>\u26a0\ufe0f Request fees (not in cost estimate)</li> </ul> <p>See Perplexity Provider</p>"},{"location":"providers/overview/#multi-provider-strategies","title":"Multi-Provider Strategies","text":""},{"location":"providers/overview/#strategy-1-cost-vs-quality","title":"Strategy 1: Cost vs Quality","text":"<p>Cheap model for volume, expensive for quality:</p> <pre><code>models:\n  # High volume, low cost\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n\n  # Occasional high-quality check\n  - provider: \"anthropic\"\n    model_name: \"claude-3-5-sonnet-20241022\"\n    enabled_for: [\"critical-intent\"]\n</code></pre>"},{"location":"providers/overview/#strategy-2-provider-diversity","title":"Strategy 2: Provider Diversity","text":"<p>Avoid single-provider dependency:</p> <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n\n  - provider: \"anthropic\"\n    model_name: \"claude-3-5-haiku-20241022\"\n\n  - provider: \"google\"\n    model_name: \"gemini-2.0-flash-exp\"\n</code></pre>"},{"location":"providers/overview/#strategy-3-web-search-standard","title":"Strategy 3: Web Search + Standard","text":"<pre><code>models:\n  # Standard queries\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n\n  # Web-search enabled\n  - provider: \"perplexity\"\n    model_name: \"sonar-pro\"\n</code></pre>"},{"location":"providers/overview/#common-issues","title":"Common Issues","text":""},{"location":"providers/overview/#api-key-errors","title":"API Key Errors","text":"<pre><code>\u274c API key not found: OPENAI_API_KEY\n</code></pre> <p>Solution:</p> <pre><code>export OPENAI_API_KEY=sk-your-key-here\n</code></pre>"},{"location":"providers/overview/#rate-limit-exceeded","title":"Rate Limit Exceeded","text":"<pre><code>\u26a0\ufe0f Rate limit exceeded for openai/gpt-4o-mini\n</code></pre> <p>Solutions:</p> <ol> <li>Add delay: <code>delay_between_queries: 2</code></li> <li>Reduce concurrent requests</li> <li>Upgrade API tier</li> </ol>"},{"location":"providers/overview/#model-not-found","title":"Model Not Found","text":"<pre><code>\u274c Model not found: gpt-4-mini\n</code></pre> <p>Solution: Use correct model name: <code>gpt-4o-mini</code></p> <p>See provider docs for valid models.</p>"},{"location":"providers/overview/#authentication-failed","title":"Authentication Failed","text":"<pre><code>\u274c Authentication failed: Invalid API key\n</code></pre> <p>Solutions:</p> <ol> <li>Check key spelling</li> <li>Regenerate key at provider console</li> <li>Verify key has correct permissions</li> </ol>"},{"location":"providers/overview/#browser-vs-api-access","title":"Browser vs API Access","text":""},{"location":"providers/overview/#two-ways-to-access-providers","title":"Two Ways to Access Providers","text":"<p>Starting in v0.2.0, LLM Answer Watcher supports two access methods for supported providers:</p> Access Method Providers How It Works Use Cases API Access All 6 providers Direct API calls with your API key Production monitoring, cost-optimized, fast Browser Access (BETA) ChatGPT, Perplexity Headless browser via Steel API True user experience, screenshots, web UI testing"},{"location":"providers/overview/#key-differences","title":"Key Differences","text":"<p>API Access: - \u2705 Faster (no browser overhead) - \u2705 Accurate cost tracking - \u2705 Token usage metrics - \u2705 Programmatic control - \u274c May differ from web UI behavior - \u274c No visual evidence</p> <p>Browser Access: - \u2705 Captures actual user experience - \u2705 Screenshots and HTML snapshots - \u2705 Tests web UI behavior - \u2705 Free tier usage (no API costs) - \u274c Slower (10-30s overhead) - \u274c No cost tracking yet (shows $0.00) - \u274c Subject to UI changes</p>"},{"location":"providers/overview/#when-to-use-each","title":"When to Use Each","text":"<p>Use API Access when: - You need fast, automated monitoring - Cost tracking is important - You're running high-volume queries - You need programmatic control</p> <p>Use Browser Access when: - You want to verify web UI behavior - You need visual evidence (screenshots) - You're testing free tier experience - You want to see what actual users see</p>"},{"location":"providers/overview/#example-comparing-both","title":"Example: Comparing Both","text":"<pre><code>runners:\n  # API access for production monitoring\n  - runner_plugin: \"api\"\n    config:\n      provider: \"openai\"\n      model_name: \"gpt-4o-mini\"\n      api_key: \"${OPENAI_API_KEY}\"\n\n  # Browser access to verify web UI\n  - runner_plugin: \"steel-chatgpt\"\n    config:\n      steel_api_key: \"${STEEL_API_KEY}\"\n      take_screenshots: true\n</code></pre> <p>This configuration runs the same query through both methods, letting you compare: - Does the API response match what users see in ChatGPT? - Are citations/sources displayed differently? - Does the web UI recommend different brands?</p> <p>See Browser Runners Guide for complete details.</p>"},{"location":"providers/overview/#next-steps","title":"Next Steps","text":"<ul> <li> <p> OpenAI</p> <p>Complete OpenAI provider guide</p> <p>OpenAI Provider \u2192</p> </li> <li> <p> Anthropic</p> <p>Claude models documentation</p> <p>Anthropic Provider \u2192</p> </li> <li> <p> Perplexity</p> <p>Grounded LLMs with web search</p> <p>Perplexity Provider \u2192</p> </li> <li> <p> Browser Runners</p> <p>Web UI automation guide</p> <p>Browser Runners \u2192</p> </li> </ul>"},{"location":"providers/perplexity/","title":"Perplexity Provider","text":"<p>Integration with Perplexity's search-grounded models.</p>"},{"location":"providers/perplexity/#supported-models","title":"Supported Models","text":"<ul> <li><code>sonar</code></li> <li><code>sonar-pro</code></li> <li><code>sonar-reasoning</code></li> <li><code>sonar-reasoning-pro</code></li> <li><code>sonar-deep-research</code></li> </ul>"},{"location":"providers/perplexity/#configuration","title":"Configuration","text":"<pre><code>models:\n  - provider: \"perplexity\"\n    model_name: \"sonar-pro\"\n    env_api_key: \"PERPLEXITY_API_KEY\"\n</code></pre>"},{"location":"providers/perplexity/#getting-api-key","title":"Getting API Key","text":"<ol> <li>Visit perplexity.ai/settings/api</li> <li>Generate API key</li> <li>Export: <code>export PERPLEXITY_API_KEY=pplx-your-key</code></li> </ol>"},{"location":"providers/perplexity/#features","title":"Features","text":"<ul> <li>Built-in web search</li> <li>Real-time information</li> <li>Citations included</li> </ul> <p>See Providers Overview for comparison.</p>"},{"location":"reference/cli-reference/","title":"CLI Reference","text":"<p>Complete command-line interface reference.</p>"},{"location":"reference/cli-reference/#commands","title":"Commands","text":""},{"location":"reference/cli-reference/#run","title":"<code>run</code>","text":"<p>Execute monitoring run.</p> <pre><code>llm-answer-watcher run --config PATH [OPTIONS]\n</code></pre> <p>Options: - <code>--config PATH</code> (required): Configuration file - <code>--format [human|json|quiet]</code>: Output format - <code>--yes, -y</code>: Skip prompts - <code>--force</code>: Override budget limits - <code>--verbose, -v</code>: Verbose logging</p>"},{"location":"reference/cli-reference/#validate","title":"<code>validate</code>","text":"<p>Validate configuration.</p> <pre><code>llm-answer-watcher validate --config PATH [OPTIONS]\n</code></pre>"},{"location":"reference/cli-reference/#eval","title":"<code>eval</code>","text":"<p>Run evaluation suite.</p> <pre><code>llm-answer-watcher eval --fixtures PATH [OPTIONS]\n</code></pre>"},{"location":"reference/cli-reference/#prices-show","title":"<code>prices show</code>","text":"<p>Display LLM pricing.</p> <pre><code>llm-answer-watcher prices show [OPTIONS]\n</code></pre> <p>See CLI Commands for detailed usage.</p>"},{"location":"reference/configuration-schema/","title":"Configuration Schema","text":"<p>Complete YAML configuration schema reference.</p>"},{"location":"reference/configuration-schema/#root-structure","title":"Root Structure","text":"<pre><code>run_settings:  # Required\nbrands:        # Required\nintents:       # Required\n</code></pre>"},{"location":"reference/configuration-schema/#run_settings","title":"<code>run_settings</code>","text":"<pre><code>run_settings:\n  output_dir: string           # Required\n  sqlite_db_path: string       # Required\n  models: [ModelConfig]        # Required\n  use_llm_rank_extraction: bool  # Optional, default: false\n  extraction_settings: ExtractionSettings  # Optional\n  budget: BudgetConfig         # Optional\n  web_search: WebSearchConfig  # Optional\n</code></pre>"},{"location":"reference/configuration-schema/#modelconfig","title":"<code>ModelConfig</code>","text":"<pre><code>provider: string              # Required: openai, anthropic, etc.\nmodel_name: string            # Required\nenv_api_key: string           # Required\nsystem_prompt: string         # Optional\n</code></pre>"},{"location":"reference/configuration-schema/#brands","title":"<code>brands</code>","text":"<pre><code>brands:\n  mine: [string]              # Required\n  competitors: [string]       # Required\n</code></pre>"},{"location":"reference/configuration-schema/#intents","title":"<code>intents</code>","text":"<pre><code>intents:\n  - id: string                # Required\n    prompt: string            # Required\n    operations: [Operation]   # Optional\n</code></pre> <p>See Configuration Overview.</p>"},{"location":"reference/database-schema/","title":"Database Schema","text":"<p>SQLite database schema reference.</p>"},{"location":"reference/database-schema/#tables","title":"Tables","text":""},{"location":"reference/database-schema/#schema_version","title":"<code>schema_version</code>","text":"<pre><code>CREATE TABLE schema_version (\n    version INTEGER PRIMARY KEY\n);\n</code></pre>"},{"location":"reference/database-schema/#runs","title":"<code>runs</code>","text":"<pre><code>CREATE TABLE runs (\n    run_id TEXT PRIMARY KEY,\n    timestamp_utc TEXT NOT NULL,\n    config_path TEXT,\n    total_cost_usd REAL,\n    queries_completed INTEGER,\n    queries_failed INTEGER\n);\n</code></pre>"},{"location":"reference/database-schema/#answers_raw","title":"<code>answers_raw</code>","text":"<pre><code>CREATE TABLE answers_raw (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    run_id TEXT NOT NULL,\n    intent_id TEXT NOT NULL,\n    model_provider TEXT NOT NULL,\n    model_name TEXT NOT NULL,\n    answer_text TEXT NOT NULL,\n    tokens_used INTEGER,\n    estimated_cost_usd REAL,\n    timestamp_utc TEXT NOT NULL,\n    UNIQUE(run_id, intent_id, model_provider, model_name)\n);\n</code></pre>"},{"location":"reference/database-schema/#mentions","title":"<code>mentions</code>","text":"<pre><code>CREATE TABLE mentions (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    run_id TEXT NOT NULL,\n    intent_id TEXT NOT NULL,\n    model_provider TEXT NOT NULL,\n    model_name TEXT NOT NULL,\n    brand TEXT NOT NULL,\n    normalized_name TEXT NOT NULL,\n    is_mine BOOLEAN NOT NULL,\n    rank_position INTEGER,\n    context_snippet TEXT,\n    sentiment TEXT,              -- NEW: positive/neutral/negative\n    mention_context TEXT,        -- NEW: primary_recommendation, alternative_listing, etc.\n    timestamp_utc TEXT NOT NULL,\n    UNIQUE(run_id, intent_id, model_provider, model_name, normalized_name)\n);\n</code></pre> <p>New Columns (v0.1.0+): - <code>sentiment</code>: Emotional tone - <code>positive</code>, <code>neutral</code>, <code>negative</code>, or <code>NULL</code> - <code>mention_context</code>: How brand was mentioned - <code>primary_recommendation</code>, <code>alternative_listing</code>, <code>competitor_negative</code>, <code>competitor_neutral</code>, <code>passing_reference</code>, or <code>NULL</code></p>"},{"location":"reference/database-schema/#intent_classifications","title":"<code>intent_classifications</code>","text":"<pre><code>CREATE TABLE intent_classifications (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    run_id TEXT NOT NULL,\n    intent_id TEXT NOT NULL,\n    query_text TEXT NOT NULL,\n    query_hash TEXT NOT NULL,        -- SHA256 hash for caching\n    intent_type TEXT NOT NULL,       -- transactional, informational, navigational, commercial_investigation\n    buyer_stage TEXT NOT NULL,       -- awareness, consideration, decision\n    urgency_signal TEXT NOT NULL,    -- high, medium, low\n    classification_confidence REAL NOT NULL,  -- 0.0-1.0\n    reasoning TEXT,                  -- Explanation of classification\n    timestamp_utc TEXT NOT NULL,\n    UNIQUE(run_id, intent_id)\n);\n</code></pre> <p>Purpose: Stores query intent classifications for prioritizing high-value mentions.</p> <p>Query Hash: Normalized SHA256 hash enables caching - same query text always produces same hash, avoiding redundant LLM calls.</p>"},{"location":"reference/database-schema/#indexes","title":"Indexes","text":"<pre><code>-- Original indexes\nCREATE INDEX idx_mentions_timestamp ON mentions(timestamp_utc);\nCREATE INDEX idx_mentions_brand ON mentions(normalized_name);\nCREATE INDEX idx_mentions_intent ON mentions(intent_id);\n\n-- Sentiment/Intent indexes (NEW in v0.1.0+)\nCREATE INDEX idx_mentions_sentiment ON mentions(sentiment);\nCREATE INDEX idx_mentions_context ON mentions(mention_context);\nCREATE INDEX idx_intent_type ON intent_classifications(intent_type);\nCREATE INDEX idx_buyer_stage ON intent_classifications(buyer_stage);\nCREATE INDEX idx_urgency_signal ON intent_classifications(urgency_signal);\n</code></pre>"},{"location":"reference/database-schema/#schema-versioning","title":"Schema Versioning","text":"<p>The database schema uses versioning for migrations:</p> <pre><code>SELECT version FROM schema_version;\n-- Returns: 1 (current version)\n</code></pre> <p>Future schema changes will increment this version and provide migration scripts.</p>"},{"location":"reference/database-schema/#example-queries","title":"Example Queries","text":""},{"location":"reference/database-schema/#sentiment-analysis","title":"Sentiment Analysis","text":"<pre><code>-- Brand mentions by sentiment\nSELECT sentiment, COUNT(*) as count\nFROM mentions\nWHERE normalized_name = 'yourbrand'\nGROUP BY sentiment;\n</code></pre>"},{"location":"reference/database-schema/#high-value-intent-filtering","title":"High-Value Intent Filtering","text":"<pre><code>-- High-intent brand mentions\nSELECT m.brand, ic.intent_type, ic.buyer_stage, ic.urgency_signal\nFROM mentions m\nJOIN intent_classifications ic ON m.intent_id = ic.intent_id AND m.run_id = ic.run_id\nWHERE ic.intent_type = 'transactional'\n  AND ic.buyer_stage = 'decision'\n  AND ic.urgency_signal = 'high'\n  AND m.sentiment = 'positive';\n</code></pre> <p>See SQLite Database for more queries.</p>"},{"location":"reference/python-api/","title":"Python API","text":"<p>Using LLM Answer Watcher as a Python library.</p>"},{"location":"reference/python-api/#programmatic-usage","title":"Programmatic Usage","text":"<pre><code>from llm_answer_watcher.config.loader import load_config_from_file\nfrom llm_answer_watcher.llm_runner.runner import run_all\n\n# Load configuration\nconfig = load_config_from_file(\"config.yaml\")\n\n# Run monitoring\nresult = run_all(config)\n\nprint(f\"Run ID: {result['run_id']}\")\nprint(f\"Cost: ${result['total_cost_usd']:.4f}\")\nprint(f\"Brands: {result['brands_detected']}\")\n</code></pre>"},{"location":"reference/python-api/#core-modules","title":"Core Modules","text":""},{"location":"reference/python-api/#config-loading","title":"Config Loading","text":"<pre><code>from llm_answer_watcher.config.loader import load_config_from_file\nfrom llm_answer_watcher.config.schema import RuntimeConfig\n\nconfig: RuntimeConfig = load_config_from_file(\"config.yaml\")\n</code></pre>"},{"location":"reference/python-api/#llm-clients","title":"LLM Clients","text":"<pre><code>from llm_answer_watcher.llm_runner.models import build_client\n\nclient = build_client(\n    provider=\"openai\",\n    model_name=\"gpt-4o-mini\",\n    api_key=api_key,\n    system_prompt=prompt\n)\n\nresponse = client.generate_answer(\"What are the best tools?\")\n</code></pre>"},{"location":"reference/python-api/#extraction","title":"Extraction","text":"<pre><code>from llm_answer_watcher.extractor.mention_detector import detect_mentions\n\nmentions = detect_mentions(\n    text=llm_response,\n    brands_mine=[\"YourBrand\"],\n    brands_competitors=[\"CompetitorA\"]\n)\n</code></pre> <p>See Architecture for design details.</p>"},{"location":"user-guide/configuration/brands/","title":"Brand Configuration","text":"<p>Brand configuration defines which brands to track in LLM responses. Proper brand configuration is critical for accurate mention detection and false-positive prevention.</p>"},{"location":"user-guide/configuration/brands/#brand-categories","title":"Brand Categories","text":"<p>LLM Answer Watcher tracks two categories of brands:</p>"},{"location":"user-guide/configuration/brands/#mine","title":"Mine","text":"<p>Your brand(s) that you want to monitor. At least one required.</p> <pre><code>brands:\n  mine:\n    - \"MyBrand\"\n    - \"MyBrand.io\"\n    - \"MyBrand CRM\"\n</code></pre>"},{"location":"user-guide/configuration/brands/#competitors","title":"Competitors","text":"<p>Competitor brands you want to track for comparison.</p> <pre><code>brands:\n  competitors:\n    - \"CompetitorA\"\n    - \"CompetitorB\"\n    - \"MarketLeader\"\n</code></pre>"},{"location":"user-guide/configuration/brands/#basic-brand-configuration","title":"Basic Brand Configuration","text":""},{"location":"user-guide/configuration/brands/#minimal-example","title":"Minimal Example","text":"<p>Simplest configuration with single brand:</p> <pre><code>brands:\n  mine:\n    - \"Warmly\"\n\n  competitors:\n    - \"Instantly\"\n    - \"Lemwarm\"\n</code></pre>"},{"location":"user-guide/configuration/brands/#comprehensive-example","title":"Comprehensive Example","text":"<p>Full configuration with aliases:</p> <pre><code>brands:\n  mine:\n    - \"Warmly\"          # Base name\n    - \"Warmly.io\"       # With TLD\n    - \"Warmly AI\"       # With product descriptor\n\n  competitors:\n    # Direct competitors\n    - \"Instantly\"\n    - \"Lemwarm\"\n    - \"Smartlead\"\n\n    # Indirect competitors\n    - \"HubSpot\"\n    - \"Salesforce\"\n\n    # Category leaders\n    - \"Apollo.io\"\n</code></pre> <p>How Many Competitors?</p> <p>Recommended: 5-15 competitors</p> <ul> <li>Too few: Miss important context</li> <li>Too many: Dilutes focus, increases noise</li> </ul> <p>Focus on competitors that:</p> <ul> <li>Directly compete for the same customers</li> <li>Appear frequently in buyer comparisons</li> <li>Represent different market segments</li> </ul>"},{"location":"user-guide/configuration/brands/#brand-alias-strategies","title":"Brand Alias Strategies","text":""},{"location":"user-guide/configuration/brands/#why-use-aliases","title":"Why Use Aliases?","text":"<p>LLMs may refer to your brand in different ways:</p> <ul> <li>With/without TLD: \"Warmly\" vs \"Warmly.io\"</li> <li>With/without product name: \"HubSpot\" vs \"HubSpot CRM\"</li> <li>Common variations: \"Salesforce\" vs \"SFDC\"</li> <li>Capitalization: \"GitHub\" vs \"Github\"</li> </ul>"},{"location":"user-guide/configuration/brands/#alias-best-practices","title":"Alias Best Practices","text":"<p>Include common variations:</p> <pre><code>brands:\n  mine:\n    - \"GitHub\"\n    - \"Github\"        # Common misspelling\n    - \"GitHub.com\"\n    - \"GitHub Actions\" # Product line\n</code></pre> <p>TLD variations:</p> <pre><code>brands:\n  mine:\n    - \"Stripe\"\n    - \"Stripe.com\"\n    - \"stripe.io\"     # If you own it\n</code></pre> <p>Product family variations:</p> <pre><code>brands:\n  mine:\n    - \"HubSpot\"\n    - \"HubSpot CRM\"\n    - \"HubSpot Marketing Hub\"\n    - \"HubSpot Sales Hub\"\n</code></pre> <p>Abbreviations and acronyms:</p> <pre><code>brands:\n  mine:\n    - \"Salesforce\"\n    - \"SFDC\"          # Common abbreviation\n    - \"Salesforce.com\"\n</code></pre> <p>Avoid Over-Aliasing</p> <p>Don't include:</p> <ul> <li>Generic terms: \"CRM\" (too broad)</li> <li>Common words: \"Hub\" (false positives)</li> <li>Competitor names: Track separately in competitors list</li> </ul>"},{"location":"user-guide/configuration/brands/#case-sensitivity","title":"Case Sensitivity","text":"<p>Brand matching is case-insensitive by default:</p> <pre><code>brands:\n  mine:\n    - \"GitHub\"  # Matches: GitHub, github, GITHUB, GiTHuB\n</code></pre> <p>You only need one capitalization variant:</p> <pre><code># \u274c Redundant\nbrands:\n  mine:\n    - \"GitHub\"\n    - \"github\"\n    - \"GITHUB\"\n\n# \u2705 Sufficient\nbrands:\n  mine:\n    - \"GitHub\"\n</code></pre>"},{"location":"user-guide/configuration/brands/#word-boundary-matching","title":"Word-Boundary Matching","text":"<p>LLM Answer Watcher uses word-boundary regex to prevent false positives.</p>"},{"location":"user-guide/configuration/brands/#how-it-works","title":"How It Works","text":"<p>Word boundaries (<code>\\b</code>) ensure brands match only as complete words:</p> <pre><code>pattern = r'\\b' + re.escape(brand_alias) + r'\\b'\n</code></pre> <p>Examples:</p> Text Brand Alias Matches? Reason \"Use HubSpot daily\" \"HubSpot\" \u2705 Yes Complete word \"GitHub and HubSpot\" \"HubSpot\" \u2705 Yes Complete word \"Hubspot is great\" \"HubSpot\" \u2705 Yes Case-insensitive \"Use hub for projects\" \"Hub\" \u2705 Yes Complete word \"GitHub has features\" \"Hub\" \u274c No Inside \"GitHub\" \"rehub your content\" \"Hub\" \u274c No Inside \"rehub\""},{"location":"user-guide/configuration/brands/#why-word-boundaries-matter","title":"Why Word Boundaries Matter","text":"<p>Without word boundaries (naive substring matching):</p> <pre><code>brands:\n  mine:\n    - \"Hub\"  # \u274c BAD: Matches \"GitHub\", \"HubSpot\", \"rehub\", etc.\n</code></pre> <p>With word boundaries (LLM Answer Watcher default):</p> <pre><code>brands:\n  mine:\n    - \"Hub\"  # \u2705 GOOD: Only matches \"Hub\" as complete word\n</code></pre> <p>Special Characters</p> <p>Word boundaries work with special characters:</p> <ul> <li><code>Apollo.io</code> matches \"Apollo.io\" but not \"Apolloio\"</li> <li><code>Slack-Bot</code> matches \"Slack-Bot\" but not \"SlackBot\"</li> </ul>"},{"location":"user-guide/configuration/brands/#testing-word-boundaries","title":"Testing Word Boundaries","text":"<p>Test your brand aliases:</p> <pre><code>import re\n\ndef test_brand_match(text: str, brand: str) -&gt; bool:\n    pattern = r'\\b' + re.escape(brand) + r'\\b'\n    return bool(re.search(pattern, text, re.IGNORECASE))\n\n# Test cases\nprint(test_brand_match(\"Use HubSpot daily\", \"HubSpot\"))  # True\nprint(test_brand_match(\"GitHub and GitLab\", \"Git\"))      # False\n</code></pre>"},{"location":"user-guide/configuration/brands/#brand-normalization","title":"Brand Normalization","text":"<p>Brands are normalized for deduplication and analysis.</p>"},{"location":"user-guide/configuration/brands/#normalization-process","title":"Normalization Process","text":"<ol> <li>Case folding: Convert to lowercase</li> <li>TLD removal: Strip <code>.com</code>, <code>.io</code>, etc.</li> <li>Whitespace normalization: Collapse multiple spaces</li> <li>Punctuation handling: Preserve hyphens, remove others</li> </ol> <p>Examples:</p> Original Normalized Rationale \"HubSpot\" \"hubspot\" Lowercase \"HubSpot.com\" \"hubspot\" TLD removed \"Apollo.io\" \"apollo\" TLD removed \"Slack  Bot\" \"slackbot\" Spaces collapsed \"GitHub-Actions\" \"github-actions\" Hyphens preserved"},{"location":"user-guide/configuration/brands/#why-normalization-matters","title":"Why Normalization Matters","text":"<p>Prevents duplicate counting:</p> <pre><code>brands:\n  mine:\n    - \"Warmly\"\n    - \"Warmly.io\"\n</code></pre> <p>LLM response: \"I recommend Warmly and Warmly.io for outreach.\"</p> <p>Without normalization: 2 mentions counted With normalization: 1 mention counted (both normalize to \"warmly\")</p>"},{"location":"user-guide/configuration/brands/#normalized-name-in-database","title":"Normalized Name in Database","text":"<p>The SQLite database stores both:</p> <ul> <li><code>brand</code>: Original matched text</li> <li><code>normalized_name</code>: Normalized version for deduplication</li> </ul> <pre><code>SELECT brand, normalized_name, COUNT(*) as mentions\nFROM mentions\nWHERE run_id = '2025-11-01T08-00-00Z'\nGROUP BY normalized_name;\n</code></pre>"},{"location":"user-guide/configuration/brands/#competitor-selection-strategies","title":"Competitor Selection Strategies","text":""},{"location":"user-guide/configuration/brands/#direct-competitors","title":"Direct Competitors","text":"<p>Brands solving the same problem for the same audience:</p> <pre><code>brands:\n  competitors:\n    # Email warmup tools (if you're Warmly)\n    - \"Instantly\"\n    - \"Lemwarm\"\n    - \"Smartlead\"\n    - \"Woodpecker\"\n</code></pre>"},{"location":"user-guide/configuration/brands/#indirect-competitors","title":"Indirect Competitors","text":"<p>Brands in adjacent categories:</p> <pre><code>brands:\n  competitors:\n    # If you're an email warmup tool\n    - \"HubSpot\"        # Full sales platform\n    - \"Apollo.io\"      # Sales intelligence\n    - \"Salesforce\"     # Enterprise CRM\n</code></pre>"},{"location":"user-guide/configuration/brands/#category-leaders","title":"Category Leaders","text":"<p>Market-defining brands to benchmark against:</p> <pre><code>brands:\n  competitors:\n    # Category leaders (if you're a startup CRM)\n    - \"Salesforce\"     # Enterprise standard\n    - \"HubSpot\"        # SMB leader\n    - \"Pipedrive\"      # Sales-focused\n</code></pre>"},{"location":"user-guide/configuration/brands/#segment-specific-competitors","title":"Segment-Specific Competitors","text":"<p>Brands targeting different segments:</p> <pre><code>brands:\n  competitors:\n    # Startup segment\n    - \"Attio\"\n    - \"Folk\"\n\n    # SMB segment\n    - \"Pipedrive\"\n    - \"Copper\"\n\n    # Enterprise segment\n    - \"Salesforce\"\n    - \"Microsoft Dynamics\"\n</code></pre>"},{"location":"user-guide/configuration/brands/#brand-configuration-patterns","title":"Brand Configuration Patterns","text":""},{"location":"user-guide/configuration/brands/#single-product-company","title":"Single Product Company","text":"<p>Simple brand with variations:</p> <pre><code>brands:\n  mine:\n    - \"MyProduct\"\n    - \"MyProduct.io\"\n    - \"MyProduct.com\"\n\n  competitors:\n    - \"CompetitorA\"\n    - \"CompetitorB\"\n    - \"CompetitorC\"\n</code></pre>"},{"location":"user-guide/configuration/brands/#multi-product-company","title":"Multi-Product Company","text":"<p>Track different product lines:</p> <pre><code>brands:\n  mine:\n    - \"MyCompany\"\n    - \"MyCompany CRM\"\n    - \"MyCompany Marketing\"\n    - \"MyCompany Sales Hub\"\n\n  competitors:\n    # CRM competitors\n    - \"Salesforce\"\n    - \"HubSpot\"\n\n    # Marketing automation competitors\n    - \"Marketo\"\n    - \"Pardot\"\n</code></pre>"},{"location":"user-guide/configuration/brands/#parent-company-subsidiaries","title":"Parent Company + Subsidiaries","text":"<p>Track corporate structure:</p> <pre><code>brands:\n  mine:\n    - \"ParentCo\"\n    - \"ProductA\"       # Subsidiary\n    - \"ProductB\"       # Subsidiary\n\n  competitors:\n    - \"CompetitorCorp\"\n    - \"CompetitorProduct\"\n</code></pre>"},{"location":"user-guide/configuration/brands/#rebranded-company","title":"Rebranded Company","text":"<p>Track both old and new names:</p> <pre><code>brands:\n  mine:\n    - \"NewBrand\"       # Current name\n    - \"OldBrand\"       # Legacy name (still in training data)\n    - \"NewBrand.io\"\n\n  competitors:\n    - \"Competitor\"\n</code></pre>"},{"location":"user-guide/configuration/brands/#regional-variations","title":"Regional Variations","text":"<p>Track region-specific brands:</p> <pre><code>brands:\n  mine:\n    - \"MyBrand\"        # Global\n    - \"MyBrand US\"\n    - \"MyBrand EU\"\n\n  competitors:\n    - \"GlobalCompetitor\"\n    - \"USCompetitor\"\n    - \"EUCompetitor\"\n</code></pre>"},{"location":"user-guide/configuration/brands/#advanced-brand-configuration","title":"Advanced Brand Configuration","text":""},{"location":"user-guide/configuration/brands/#fuzzy-matching","title":"Fuzzy Matching","text":"<p>Enable fuzzy matching for misspellings (optional):</p> <pre><code>extraction_settings:\n  fuzzy_matching:\n    enabled: true\n    threshold: 0.9     # Similarity threshold (0.0-1.0)\n\nbrands:\n  mine:\n    - \"Warmly\"         # Also matches: \"Warmley\", \"Warmlly\"\n</code></pre> <p>Fuzzy Matching Trade-offs</p> <p>Pros:</p> <ul> <li>Catches misspellings</li> <li>More comprehensive tracking</li> </ul> <p>Cons:</p> <ul> <li>Higher false-positive rate</li> <li>Slower extraction</li> <li>May match unrelated words</li> </ul> <p>Recommended threshold: 0.9 (very strict)</p>"},{"location":"user-guide/configuration/brands/#brand-exclusions","title":"Brand Exclusions","text":"<p>Exclude certain patterns (advanced):</p> <pre><code>brands:\n  mine:\n    - \"Apple\"\n\n  exclusions:\n    - \"apple pie\"      # Don't match \"apple\" in \"apple pie\"\n    - \"apple juice\"\n</code></pre> <p>Exclusions Not Yet Implemented</p> <p>This feature is planned for a future release. Currently, use word boundaries to minimize false positives.</p>"},{"location":"user-guide/configuration/brands/#brand-categories_1","title":"Brand Categories","text":"<p>Group brands by category (for analysis):</p> <pre><code>brands:\n  mine:\n    - \"MyBrand\"\n\n  competitors:\n    # Tag with category (custom metadata)\n    - name: \"CompetitorA\"\n      category: \"direct\"\n\n    - name: \"CompetitorB\"\n      category: \"direct\"\n\n    - name: \"MarketLeader\"\n      category: \"aspirational\"\n</code></pre> <p>Categories Not Yet Implemented</p> <p>This feature is planned for a future release. Currently, track categories externally.</p>"},{"location":"user-guide/configuration/brands/#brand-mention-analysis","title":"Brand Mention Analysis","text":""},{"location":"user-guide/configuration/brands/#viewing-mentions","title":"Viewing Mentions","text":"<p>Query SQLite database:</p> <pre><code>-- All mentions for a run\nSELECT brand, COUNT(*) as mentions\nFROM mentions\nWHERE run_id = '2025-11-01T08-00-00Z'\nGROUP BY normalized_name\nORDER BY mentions DESC;\n</code></pre> <pre><code>-- My brand mentions over time\nSELECT DATE(timestamp_utc) as date, COUNT(*) as mentions\nFROM mentions\nWHERE normalized_name = 'mybrand'\nGROUP BY DATE(timestamp_utc)\nORDER BY date DESC;\n</code></pre> <pre><code>-- Competitor comparison\nSELECT\n    brand,\n    COUNT(*) as total_mentions,\n    AVG(rank_position) as avg_rank\nFROM mentions\nWHERE run_id = '2025-11-01T08-00-00Z'\nGROUP BY normalized_name\nORDER BY avg_rank ASC;\n</code></pre>"},{"location":"user-guide/configuration/brands/#mention-metrics","title":"Mention Metrics","text":"<p>Key metrics to track:</p> <ul> <li>Mention rate: % of queries where brand appears</li> <li>Average rank: Mean position in ranked lists</li> <li>Top-3 rate: % of mentions in top 3</li> <li>Share of voice: Your mentions / total mentions</li> </ul> <p>Calculate in SQL:</p> <pre><code>-- Mention rate\nSELECT\n    (COUNT(DISTINCT CASE WHEN normalized_name = 'mybrand' THEN intent_id END) * 100.0 / COUNT(DISTINCT intent_id)) as mention_rate\nFROM mentions\nWHERE run_id = '2025-11-01T08-00-00Z';\n</code></pre> <pre><code>-- Top-3 rate\nSELECT\n    (SUM(CASE WHEN rank_position &lt;= 3 THEN 1 ELSE 0 END) * 100.0 / COUNT(*)) as top3_rate\nFROM mentions\nWHERE normalized_name = 'mybrand'\n  AND run_id = '2025-11-01T08-00-00Z';\n</code></pre>"},{"location":"user-guide/configuration/brands/#validation-and-testing","title":"Validation and Testing","text":""},{"location":"user-guide/configuration/brands/#validate-brand-configuration","title":"Validate Brand Configuration","text":"<p>Check for common issues:</p> <pre><code>llm-answer-watcher validate --config watcher.config.yaml\n</code></pre> <p>Validation checks:</p> <ul> <li>At least one brand in <code>mine</code> list</li> <li>No empty brand aliases</li> <li>No duplicate aliases (warning)</li> <li>Brand aliases &gt;= 3 characters (warning)</li> </ul>"},{"location":"user-guide/configuration/brands/#test-brand-matching","title":"Test Brand Matching","text":"<p>Test your brands against sample text:</p> <pre><code># Create test file\necho \"I recommend HubSpot, Salesforce, and Warmly for CRM.\" &gt; test.txt\n\n# Test matching (hypothetical command)\nllm-answer-watcher test-brands --config watcher.config.yaml --text test.txt\n</code></pre> <p>Expected output:</p> <pre><code>\u2705 Found 3 brand mentions:\n   - HubSpot (competitor, position 16)\n   - Salesforce (competitor, position 26)\n   - Warmly (mine, position 42)\n</code></pre>"},{"location":"user-guide/configuration/brands/#common-validation-errors","title":"Common Validation Errors","text":"<p>Error: <code>At least one brand required in 'mine'</code></p> <pre><code># \u274c Wrong\nbrands:\n  mine: []\n\n# \u2705 Correct\nbrands:\n  mine:\n    - \"MyBrand\"\n</code></pre> <p>Error: <code>Brand alias too short: \"io\"</code></p> <pre><code># \u274c Warning (high false-positive risk)\nbrands:\n  mine:\n    - \"io\"\n\n# \u2705 Better\nbrands:\n  mine:\n    - \"MyBrand.io\"\n</code></pre> <p>Warning: <code>Duplicate brand alias: \"HubSpot\"</code></p> <pre><code># \u274c Redundant\nbrands:\n  mine:\n    - \"HubSpot\"\n  competitors:\n    - \"HubSpot\"  # Same brand in both categories!\n\n# \u2705 Correct\nbrands:\n  mine:\n    - \"MyBrand\"\n  competitors:\n    - \"HubSpot\"\n</code></pre>"},{"location":"user-guide/configuration/brands/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/configuration/brands/#1-start-with-core-brand-names","title":"1. Start with Core Brand Names","text":"<p>Begin with unambiguous brand names:</p> <pre><code>brands:\n  mine:\n    - \"Warmly\"       # Clear, unambiguous\n\n  competitors:\n    - \"Instantly\"\n    - \"Lemwarm\"\n</code></pre>"},{"location":"user-guide/configuration/brands/#2-add-tld-variations-gradually","title":"2. Add TLD Variations Gradually","text":"<p>Monitor results, then add TLDs if needed:</p> <pre><code># Week 1: Start simple\nbrands:\n  mine:\n    - \"Warmly\"\n\n# Week 2: Add TLD after seeing LLM responses\nbrands:\n  mine:\n    - \"Warmly\"\n    - \"Warmly.io\"\n</code></pre>"},{"location":"user-guide/configuration/brands/#3-use-specific-names-not-generic-terms","title":"3. Use Specific Names, Not Generic Terms","text":"<pre><code># \u274c Bad (too generic)\nbrands:\n  mine:\n    - \"CRM\"\n    - \"Email\"\n    - \"Sales Tool\"\n\n# \u2705 Good (specific)\nbrands:\n  mine:\n    - \"Warmly CRM\"\n    - \"Warmly Email\"\n</code></pre>"},{"location":"user-guide/configuration/brands/#4-track-10-15-competitors-maximum","title":"4. Track 10-15 Competitors Maximum","text":"<p>Focus on key competitors:</p> <pre><code>brands:\n  competitors:\n    # Top 5 direct competitors\n    - \"DirectA\"\n    - \"DirectB\"\n    - \"DirectC\"\n    - \"DirectD\"\n    - \"DirectE\"\n\n    # Top 3 category leaders\n    - \"LeaderA\"\n    - \"LeaderB\"\n    - \"LeaderC\"\n</code></pre>"},{"location":"user-guide/configuration/brands/#5-review-mentions-regularly","title":"5. Review Mentions Regularly","text":"<p>Check for unexpected matches:</p> <pre><code>-- Find unexpected brand mentions\nSELECT brand, answer_text\nFROM mentions\nJOIN answers_raw USING (run_id, intent_id)\nWHERE normalized_name = 'mybrand'\n  AND run_id = '2025-11-01T08-00-00Z';\n</code></pre> <p>Look for false positives or missing variations.</p>"},{"location":"user-guide/configuration/brands/#6-version-brand-lists","title":"6. Version Brand Lists","text":"<p>Track brand list changes:</p> <pre><code>git add watcher.config.yaml\ngit commit -m \"feat: add HubSpot as competitor\"\n</code></pre>"},{"location":"user-guide/configuration/brands/#7-test-before-production","title":"7. Test Before Production","text":"<p>Validate brand configuration:</p> <pre><code>llm-answer-watcher validate --config watcher.config.yaml\nllm-answer-watcher run --config watcher.config.yaml --dry-run\n</code></pre>"},{"location":"user-guide/configuration/brands/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/configuration/brands/#false-positives","title":"False Positives","text":"<p>Problem: Brand matches where it shouldn't</p> <p>Example: \"Hub\" matches in \"GitHub\"</p> <p>Solution: Use more specific aliases:</p> <pre><code># \u274c Too generic\nbrands:\n  mine:\n    - \"Hub\"\n\n# \u2705 More specific\nbrands:\n  mine:\n    - \"MyHub\"\n    - \"MyHub.io\"\n</code></pre>"},{"location":"user-guide/configuration/brands/#false-negatives","title":"False Negatives","text":"<p>Problem: Brand doesn't match when it should</p> <p>Example: LLM says \"Warmly.ai\" but you only track \"Warmly.io\"</p> <p>Solution: Add missing variation:</p> <pre><code>brands:\n  mine:\n    - \"Warmly\"\n    - \"Warmly.io\"\n    - \"Warmly.ai\"    # Add missing TLD\n</code></pre>"},{"location":"user-guide/configuration/brands/#duplicate-counting","title":"Duplicate Counting","text":"<p>Problem: Same brand counted multiple times</p> <p>Example: \"Warmly\" and \"Warmly.io\" counted separately</p> <p>Solution: This is expected! Normalization prevents duplicates in analysis:</p> <pre><code>-- Use normalized_name for deduplication\nSELECT normalized_name, COUNT(*) as mentions\nFROM mentions\nGROUP BY normalized_name;\n\n-- Use brand to see exact matches\nSELECT brand, COUNT(*) as raw_mentions\nFROM mentions\nGROUP BY brand;\n</code></pre>"},{"location":"user-guide/configuration/brands/#brand-not-found","title":"Brand Not Found","text":"<p>Problem: Brand not detected in LLM response</p> <p>Possible causes:</p> <ol> <li>LLM didn't mention it: Check raw response</li> <li>Misspelling: Add variations or enable fuzzy matching</li> <li>Different phrasing: LLM used different name</li> </ol> <p>Debug:</p> <pre><code>-- Check raw response\nSELECT answer_text\nFROM answers_raw\nWHERE run_id = '2025-11-01T08-00-00Z'\n  AND intent_id = 'best-tools';\n</code></pre> <p>Look for how LLM referred to your brand.</p>"},{"location":"user-guide/configuration/brands/#next-steps","title":"Next Steps","text":"<ul> <li>Intent Configuration: Design prompts that surface your brand</li> <li>Rank Extraction: Understand how ranking works</li> <li>Brand Detection: Deep dive into detection algorithms</li> <li>Historical Tracking: Analyze brand trends over time</li> </ul>"},{"location":"user-guide/configuration/budget/","title":"Budget Configuration","text":"<p>Budget controls prevent runaway costs by setting spending limits before execution starts. LLM Answer Watcher validates estimated costs against your budget and aborts if limits would be exceeded.</p>"},{"location":"user-guide/configuration/budget/#why-budget-controls","title":"Why Budget Controls?","text":"<p>LLM API costs can add up quickly:</p> <ul> <li>Testing: Multiple intents \u00d7 multiple models = high query volume</li> <li>Mistakes: Accidental loops or configuration errors</li> <li>Provider changes: Pricing updates or model changes</li> <li>Experimentation: Trying new configurations without cost awareness</li> </ul> <p>Budget controls ensure you never spend more than intended.</p>"},{"location":"user-guide/configuration/budget/#basic-budget-configuration","title":"Basic Budget Configuration","text":""},{"location":"user-guide/configuration/budget/#enabling-budget-controls","title":"Enabling Budget Controls","text":"<p>Add a <code>budget</code> section to <code>run_settings</code>:</p> <pre><code>run_settings:\n  output_dir: \"./output\"\n  sqlite_db_path: \"./output/watcher.db\"\n\n  models:\n    - provider: \"openai\"\n      model_name: \"gpt-4o-mini\"\n      env_api_key: \"OPENAI_API_KEY\"\n\n  budget:\n    enabled: true\n    max_per_run_usd: 1.00        # Hard limit: abort if total &gt; $1.00\n    max_per_intent_usd: 0.10     # Hard limit: abort if any intent &gt; $0.10\n    warn_threshold_usd: 0.50     # Warning: log if total &gt; $0.50 (but continue)\n</code></pre>"},{"location":"user-guide/configuration/budget/#disabling-budget-controls","title":"Disabling Budget Controls","text":"<p>For unlimited spending:</p> <pre><code>run_settings:\n  budget:\n    enabled: false  # No cost limits\n</code></pre> <p>Disabled Budgets</p> <p>Only disable budgets when:</p> <ul> <li>You fully understand costs</li> <li>Running production monitoring with known costs</li> <li>Budget controls interfere with automation</li> </ul> <p>Recommendation: Keep budgets enabled even in production.</p>"},{"location":"user-guide/configuration/budget/#budget-parameters","title":"Budget Parameters","text":""},{"location":"user-guide/configuration/budget/#enabled-boolean","title":"<code>enabled</code> (boolean)","text":"<p>Enable or disable budget enforcement.</p> <pre><code>budget:\n  enabled: true  # Enforce budget limits\n</code></pre> <p>Default: <code>false</code> (budgets disabled)</p> <p>Recommended: <code>true</code> for all use cases</p>"},{"location":"user-guide/configuration/budget/#max_per_run_usd-float","title":"<code>max_per_run_usd</code> (float)","text":"<p>Maximum total cost per run (all intents \u00d7 all models).</p> <pre><code>budget:\n  max_per_run_usd: 1.00  # Abort if total estimated cost &gt; $1.00\n</code></pre> <p>Calculation: <pre><code>max_per_run = (num_intents \u00d7 num_models) \u00d7 avg_cost_per_query\n</code></pre></p> <p>Example:</p> <ul> <li>3 intents \u00d7 2 models = 6 queries</li> <li>Average cost: $0.005 per query</li> <li>Total estimated cost: $0.03</li> <li>Budget limit: $1.00</li> <li>\u2705 Result: Run proceeds</li> </ul>"},{"location":"user-guide/configuration/budget/#max_per_intent_usd-float","title":"<code>max_per_intent_usd</code> (float)","text":"<p>Maximum cost per single intent (across all models).</p> <pre><code>budget:\n  max_per_intent_usd: 0.10  # Abort if any single intent &gt; $0.10\n</code></pre> <p>Calculation: <pre><code>max_per_intent = num_models \u00d7 avg_cost_per_query\n</code></pre></p> <p>Example:</p> <ul> <li>3 models for one intent</li> <li>Average cost: $0.005 per query</li> <li>Intent cost: $0.015</li> <li>Budget limit: $0.10</li> <li>\u2705 Result: Intent proceeds</li> </ul> <p>Use case: Prevent expensive intents with long prompts or web search.</p>"},{"location":"user-guide/configuration/budget/#warn_threshold_usd-float","title":"<code>warn_threshold_usd</code> (float)","text":"<p>Warning threshold (logs warning but continues).</p> <pre><code>budget:\n  warn_threshold_usd: 0.50  # Log warning if total &gt; $0.50\n</code></pre> <p>Behavior:</p> <ul> <li>If <code>estimated_cost &lt;= warn_threshold</code>: Silent execution</li> <li>If <code>warn_threshold &lt; estimated_cost &lt;= max_per_run</code>: Log warning, continue</li> <li>If <code>estimated_cost &gt; max_per_run</code>: Abort execution</li> </ul> <p>Example output:</p> <pre><code>\u26a0\ufe0f  Cost warning: Estimated run cost $0.75 exceeds warning threshold of $0.50\n   Budget limit: $1.00 (OK to proceed)\n   Run will execute 12 queries across 3 intents and 4 models.\n</code></pre>"},{"location":"user-guide/configuration/budget/#budget-configuration-patterns","title":"Budget Configuration Patterns","text":""},{"location":"user-guide/configuration/budget/#development-testing","title":"Development / Testing","text":"<p>Strict limits for experimentation:</p> <pre><code>run_settings:\n  budget:\n    enabled: true\n    max_per_run_usd: 0.10         # Very low limit\n    max_per_intent_usd: 0.05      # Catch expensive intents early\n    warn_threshold_usd: 0.05      # Warn at same level as max\n</code></pre> <p>Use when:</p> <ul> <li>Testing configuration changes</li> <li>Developing new intents</li> <li>Running frequent test runs</li> <li>Learning the tool</li> </ul>"},{"location":"user-guide/configuration/budget/#production-monitoring","title":"Production Monitoring","text":"<p>Balanced limits for regular monitoring:</p> <pre><code>run_settings:\n  budget:\n    enabled: true\n    max_per_run_usd: 5.00         # Reasonable daily limit\n    max_per_intent_usd: 0.50      # Prevent runaway intent costs\n    warn_threshold_usd: 2.50      # Alert if &gt; $2.50\n</code></pre> <p>Use when:</p> <ul> <li>Daily/weekly monitoring</li> <li>Established configuration</li> <li>Known cost profile</li> <li>Production use</li> </ul>"},{"location":"user-guide/configuration/budget/#cicd-pipelines","title":"CI/CD Pipelines","text":"<p>Conservative limits for automated runs:</p> <pre><code>run_settings:\n  budget:\n    enabled: true\n    max_per_run_usd: 0.50         # Low limit for automated runs\n    max_per_intent_usd: 0.10\n    warn_threshold_usd: 0.25\n</code></pre> <p>Use when:</p> <ul> <li>Automated testing</li> <li>Pull request checks</li> <li>Continuous monitoring</li> <li>High-frequency runs</li> </ul>"},{"location":"user-guide/configuration/budget/#executive-reports","title":"Executive Reports","text":"<p>Higher limits for comprehensive analysis:</p> <pre><code>run_settings:\n  budget:\n    enabled: true\n    max_per_run_usd: 25.00        # Higher limit for quality models\n    max_per_intent_usd: 2.00\n    warn_threshold_usd: 10.00\n</code></pre> <p>Use when:</p> <ul> <li>Monthly executive reports</li> <li>Using premium models (GPT-4, Claude Opus)</li> <li>Comprehensive competitive analysis</li> <li>Deep-dive research</li> </ul>"},{"location":"user-guide/configuration/budget/#warning-only-mode","title":"Warning-Only Mode","text":"<p>Logs warnings but never aborts:</p> <pre><code>run_settings:\n  budget:\n    enabled: true\n    max_per_run_usd: 999999.99    # Effectively unlimited\n    max_per_intent_usd: 999999.99\n    warn_threshold_usd: 1.00      # But warn at $1\n</code></pre> <p>Use when:</p> <ul> <li>Production monitoring with known costs</li> <li>Don't want aborts to break automation</li> <li>Still want cost visibility</li> </ul> <p>Use with Caution</p> <p>This defeats the purpose of budget controls. Only use when you fully understand cost implications.</p>"},{"location":"user-guide/configuration/budget/#cost-estimation","title":"Cost Estimation","text":"<p>LLM Answer Watcher estimates costs before execution using:</p>"},{"location":"user-guide/configuration/budget/#estimation-formula","title":"Estimation Formula","text":"<pre><code>estimated_cost = (\n    (input_tokens \u00d7 input_price_per_token) +\n    (output_tokens \u00d7 output_price_per_token)\n) \u00d7 safety_buffer\n</code></pre> <p>Parameters:</p> <ul> <li><code>input_tokens</code>: Estimated from prompt length (~150 tokens)</li> <li><code>output_tokens</code>: Estimated average response (~500 tokens)</li> <li><code>input_price_per_token</code>: From llm-prices.com (cached 24h)</li> <li><code>output_price_per_token</code>: From llm-prices.com (cached 24h)</li> <li><code>safety_buffer</code>: 1.2 (20% buffer for variance)</li> </ul>"},{"location":"user-guide/configuration/budget/#estimation-accuracy","title":"Estimation Accuracy","text":"<p>Cost estimates are approximate:</p> <ul> <li>Actual costs: May vary \u00b120% from estimates</li> <li>Factors affecting accuracy:</li> <li>Prompt length (longer = higher input cost)</li> <li>Response length (varies by model and prompt)</li> <li>Web search usage (adds \\(10-\\)25 per 1k calls)</li> <li>Function calling (may increase token usage)</li> </ul> <p>Estimation Accuracy</p> <p>Estimates are conservative (tend to overestimate). Actual costs are typically 10-20% lower than estimated.</p>"},{"location":"user-guide/configuration/budget/#checking-estimated-costs","title":"Checking Estimated Costs","text":"<p>Before running:</p> <pre><code>llm-answer-watcher run --config watcher.config.yaml --dry-run\n</code></pre> <p>Output:</p> <pre><code>\ud83d\udcb0 Cost Estimation:\n   \u251c\u2500\u2500 OpenAI gpt-4o-mini: $0.0004 per query \u00d7 3 intents = $0.0012\n   \u251c\u2500\u2500 Anthropic claude-3-5-haiku: $0.0022 per query \u00d7 3 intents = $0.0066\n   \u251c\u2500\u2500 Safety buffer (20%): +$0.0016\n   \u2514\u2500\u2500 Total estimated cost: $0.0094\n\n\u2705 Budget check passed:\n   \u251c\u2500\u2500 Estimated cost: $0.0094\n   \u251c\u2500\u2500 Budget limit: $1.00\n   \u2514\u2500\u2500 Remaining budget: $0.9906\n</code></pre> <p>After running:</p> <p>Check actual costs in <code>run_meta.json</code>:</p> <pre><code>{\n  \"run_id\": \"2025-11-01T08-00-00Z\",\n  \"total_cost_usd\": 0.0087,\n  \"estimated_cost_usd\": 0.0094,\n  \"cost_accuracy\": 92.6\n}\n</code></pre>"},{"location":"user-guide/configuration/budget/#dynamic-pricing","title":"Dynamic Pricing","text":"<p>LLM Answer Watcher automatically loads current pricing from llm-prices.com.</p>"},{"location":"user-guide/configuration/budget/#how-dynamic-pricing-works","title":"How Dynamic Pricing Works","text":"<ol> <li>On first run: Fetch pricing from llm-prices.com</li> <li>Cache for 24 hours: Store in <code>~/.cache/llm-answer-watcher/pricing.json</code></li> <li>Auto-refresh: Re-fetch after 24 hours</li> <li>Fallback: Use hardcoded prices if API unavailable</li> </ol>"},{"location":"user-guide/configuration/budget/#viewing-current-pricing","title":"Viewing Current Pricing","text":"<pre><code># Show all models\nllm-answer-watcher prices show\n\n# Show specific provider\nllm-answer-watcher prices show --provider openai\n\n# Show specific model\nllm-answer-watcher prices show --model gpt-4o-mini\n\n# Export as JSON\nllm-answer-watcher prices list --format json\n</code></pre> <p>Example output:</p> <pre><code>\ud83d\udcb0 Current LLM Pricing (as of 2025-11-01):\n\nOpenAI:\n  gpt-4o-mini:\n    Input:  $0.15 per 1M tokens\n    Output: $0.60 per 1M tokens\n\n  gpt-4o:\n    Input:  $2.50 per 1M tokens\n    Output: $10.00 per 1M tokens\n\nAnthropic:\n  claude-3-5-haiku-20241022:\n    Input:  $0.80 per 1M tokens\n    Output: $4.00 per 1M tokens\n</code></pre>"},{"location":"user-guide/configuration/budget/#forcing-pricing-refresh","title":"Forcing Pricing Refresh","text":"<pre><code># Force refresh (ignore cache)\nllm-answer-watcher prices refresh --force\n\n# Verify pricing updated\nllm-answer-watcher prices show\n</code></pre>"},{"location":"user-guide/configuration/budget/#pricing-cache-location","title":"Pricing Cache Location","text":"<ul> <li>Linux/Mac: <code>~/.cache/llm-answer-watcher/pricing.json</code></li> <li>Windows: <code>%LOCALAPPDATA%/llm-answer-watcher/pricing.json</code></li> </ul> <p>Clear cache:</p> <pre><code>rm ~/.cache/llm-answer-watcher/pricing.json\n</code></pre>"},{"location":"user-guide/configuration/budget/#web-search-costs","title":"Web Search Costs","text":"<p>Web search adds additional costs beyond token usage.</p>"},{"location":"user-guide/configuration/budget/#openai-web-search-pricing","title":"OpenAI Web Search Pricing","text":"Model Tier Cost per 1,000 Calls Content Tokens Standard (all models) $10 @ model rate gpt-4o-mini, gpt-4.1-mini $10 Fixed 8k tokens Preview reasoning (o1, o3) $10 @ model rate Preview non-reasoning $25 FREE"},{"location":"user-guide/configuration/budget/#web-search-cost-calculation","title":"Web Search Cost Calculation","text":"<pre><code># Standard model\nweb_search_cost = (\n    (num_searches \u00d7 $0.01) +  # $10 per 1k calls\n    (search_tokens \u00d7 input_price_per_token)\n)\n\n# Mini models (fixed 8k tokens)\nweb_search_cost = (\n    (num_searches \u00d7 $0.01) +\n    (8000 \u00d7 input_price_per_token)\n)\n</code></pre>"},{"location":"user-guide/configuration/budget/#estimating-web-search-costs","title":"Estimating Web Search Costs","text":"<pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    tools:\n      - type: \"web_search\"\n</code></pre> <p>Estimated cost per query (with web search):</p> <ul> <li>Base query: $0.0004 (tokens)</li> <li>Web search call: $0.01 (per call)</li> <li>Web search content: $0.0012 (8k tokens @ $0.15/1M)</li> <li>Total: ~$0.0116 per query</li> </ul> <p>See Web Search Configuration for details.</p>"},{"location":"user-guide/configuration/budget/#perplexity-request-fees","title":"Perplexity Request Fees","text":"<p>Perplexity charges request fees in addition to token costs:</p> <ul> <li>Basic searches: ~$0.005 per request</li> <li>Complex searches: ~\\(0.01-\\)0.03 per request</li> </ul> <p>Perplexity Costs Not Fully Estimated</p> <p>Request fees are not yet included in cost estimates. Budget accordingly when using Perplexity:</p> <pre><code>budget:\n  max_per_run_usd: 2.00  # Higher buffer for Perplexity\n</code></pre>"},{"location":"user-guide/configuration/budget/#budget-enforcement-behavior","title":"Budget Enforcement Behavior","text":""},{"location":"user-guide/configuration/budget/#pre-execution-validation","title":"Pre-Execution Validation","text":"<p>Budget validation happens before any LLM calls:</p> <ol> <li>Load configuration</li> <li>Estimate total cost</li> <li>Check against budget limits</li> <li>If budget exceeded: Abort immediately</li> <li>If budget OK: Proceed with execution</li> </ol> <p>No LLM calls are made if budget would be exceeded.</p>"},{"location":"user-guide/configuration/budget/#abort-on-budget-exceeded","title":"Abort on Budget Exceeded","text":"<p>When budget is exceeded:</p> <pre><code>llm-answer-watcher run --config watcher.config.yaml\n</code></pre> <p>Output:</p> <pre><code>\u274c Budget exceeded: Estimated run cost $1.25 exceeds max_per_run_usd budget of $1.00.\n\n   Run would execute 12 queries:\n   \u251c\u2500\u2500 3 intents \u00d7 4 models = 12 queries\n   \u251c\u2500\u2500 Estimated cost: $1.25\n   \u251c\u2500\u2500 Budget limit: $1.00\n   \u2514\u2500\u2500 Overage: $0.25\n\n   Options:\n   1. Reduce number of models or intents\n   2. Increase budget limit in watcher.config.yaml\n   3. Use --force to override budget (not recommended)\n</code></pre> <p>Exit code: <code>1</code> (configuration error)</p>"},{"location":"user-guide/configuration/budget/#force-override","title":"Force Override","text":"<p>Override budget limits (use with caution):</p> <pre><code>llm-answer-watcher run --config watcher.config.yaml --force\n</code></pre> <p>Output:</p> <pre><code>\u26a0\ufe0f  Budget check OVERRIDDEN with --force flag\n\n   Estimated cost: $1.25\n   Budget limit: $1.00\n   Overage: $0.25\n\n   Proceeding anyway...\n</code></pre> <p>Force Override</p> <p>Only use <code>--force</code> when:</p> <ul> <li>You understand exact costs</li> <li>Budget limit is incorrect</li> <li>Emergency production run</li> </ul> <p>Never use <code>--force</code> in automated scripts.</p>"},{"location":"user-guide/configuration/budget/#warning-threshold-behavior","title":"Warning Threshold Behavior","text":"<p>When cost exceeds warning threshold (but not max):</p> <pre><code>llm-answer-watcher run --config watcher.config.yaml\n</code></pre> <p>Output:</p> <pre><code>\u26a0\ufe0f  Cost warning: Estimated run cost $0.75 exceeds warning threshold of $0.50\n\n   \u251c\u2500\u2500 Estimated cost: $0.75\n   \u251c\u2500\u2500 Warning threshold: $0.50\n   \u251c\u2500\u2500 Budget limit: $1.00\n   \u2514\u2500\u2500 Status: OK to proceed\n\n   Run will execute 12 queries. Continue? [Y/n]\n</code></pre> <p>Behavior:</p> <ul> <li>In human mode: Prompt for confirmation</li> <li>With <code>--yes</code> flag: Continue automatically</li> <li>In agent mode: Continue automatically (warning logged)</li> </ul>"},{"location":"user-guide/configuration/budget/#cost-tracking","title":"Cost Tracking","text":""},{"location":"user-guide/configuration/budget/#per-run-cost-summary","title":"Per-Run Cost Summary","text":"<p>After each run, check <code>run_meta.json</code>:</p> <pre><code>{\n  \"run_id\": \"2025-11-01T08-00-00Z\",\n  \"timestamp_utc\": \"2025-11-01T08:00:00Z\",\n  \"total_cost_usd\": 0.0142,\n  \"estimated_cost_usd\": 0.0168,\n  \"cost_accuracy_percent\": 84.5,\n  \"queries_completed\": 6,\n  \"queries_failed\": 0,\n  \"cost_by_provider\": {\n    \"openai\": 0.0048,\n    \"anthropic\": 0.0094\n  },\n  \"cost_by_model\": {\n    \"gpt-4o-mini\": 0.0048,\n    \"claude-3-5-haiku-20241022\": 0.0094\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration/budget/#historical-cost-analysis","title":"Historical Cost Analysis","text":"<p>Query SQLite database:</p> <pre><code>-- Total spending\nSELECT SUM(total_cost_usd) as total_spent\nFROM runs;\n\n-- Spending by week\nSELECT\n    strftime('%Y-W%W', timestamp_utc) as week,\n    SUM(total_cost_usd) as weekly_cost,\n    COUNT(*) as runs\nFROM runs\nGROUP BY week\nORDER BY week DESC;\n\n-- Spending by model\nSELECT\n    model_name,\n    COUNT(*) as queries,\n    SUM(estimated_cost_usd) as total_cost,\n    AVG(estimated_cost_usd) as avg_cost_per_query\nFROM answers_raw\nGROUP BY model_name\nORDER BY total_cost DESC;\n\n-- Spending by intent\nSELECT\n    intent_id,\n    COUNT(*) as queries,\n    SUM(estimated_cost_usd) as total_cost,\n    AVG(estimated_cost_usd) as avg_cost_per_query\nFROM answers_raw\nGROUP BY intent_id\nORDER BY total_cost DESC;\n</code></pre>"},{"location":"user-guide/configuration/budget/#monthly-budget-tracking","title":"Monthly Budget Tracking","text":"<p>Track spending vs. monthly budget:</p> <pre><code>-- Current month spending\nSELECT SUM(total_cost_usd) as month_to_date\nFROM runs\nWHERE strftime('%Y-%m', timestamp_utc) = strftime('%Y-%m', 'now');\n\n-- Monthly trend\nSELECT\n    strftime('%Y-%m', timestamp_utc) as month,\n    SUM(total_cost_usd) as monthly_cost,\n    COUNT(*) as runs,\n    AVG(total_cost_usd) as avg_cost_per_run\nFROM runs\nGROUP BY month\nORDER BY month DESC;\n</code></pre>"},{"location":"user-guide/configuration/budget/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/configuration/budget/#1-always-enable-budgets","title":"1. Always Enable Budgets","text":"<p>Even in production:</p> <pre><code>run_settings:\n  budget:\n    enabled: true\n    max_per_run_usd: 10.00  # Reasonable safety limit\n</code></pre>"},{"location":"user-guide/configuration/budget/#2-set-conservative-limits","title":"2. Set Conservative Limits","text":"<p>Start low, increase as needed:</p> <pre><code># Week 1: Very conservative\nbudget:\n  max_per_run_usd: 0.10\n\n# Week 2: Based on actual usage\nbudget:\n  max_per_run_usd: 0.50\n\n# Production: 2x observed average\nbudget:\n  max_per_run_usd: 1.00\n</code></pre>"},{"location":"user-guide/configuration/budget/#3-use-warning-thresholds","title":"3. Use Warning Thresholds","text":"<p>Get alerts before hitting limits:</p> <pre><code>budget:\n  max_per_run_usd: 1.00\n  warn_threshold_usd: 0.50  # Alert at 50% of limit\n</code></pre>"},{"location":"user-guide/configuration/budget/#4-separate-budgets-by-environment","title":"4. Separate Budgets by Environment","text":"<p>Different limits for different environments:</p> <pre><code># dev.config.yaml\nrun_settings:\n  budget:\n    max_per_run_usd: 0.10\n\n# prod.config.yaml\nrun_settings:\n  budget:\n    max_per_run_usd: 5.00\n</code></pre>"},{"location":"user-guide/configuration/budget/#5-monitor-actual-vs-estimated-costs","title":"5. Monitor Actual vs. Estimated Costs","text":"<p>Track estimation accuracy:</p> <pre><code>SELECT\n    AVG(total_cost_usd / estimated_cost_usd) as avg_accuracy,\n    MIN(total_cost_usd / estimated_cost_usd) as min_accuracy,\n    MAX(total_cost_usd / estimated_cost_usd) as max_accuracy\nFROM runs\nWHERE estimated_cost_usd &gt; 0;\n</code></pre> <p>Adjust safety buffer if needed.</p>"},{"location":"user-guide/configuration/budget/#6-account-for-web-search-costs","title":"6. Account for Web Search Costs","text":"<p>Budget higher when using web search:</p> <pre><code># Without web search\nbudget:\n  max_per_run_usd: 0.50\n\n# With web search (10x higher)\nbudget:\n  max_per_run_usd: 5.00\n</code></pre>"},{"location":"user-guide/configuration/budget/#7-use-dry-runs","title":"7. Use Dry Runs","text":"<p>Check costs before running:</p> <pre><code>llm-answer-watcher run --config watcher.config.yaml --dry-run\n</code></pre>"},{"location":"user-guide/configuration/budget/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/configuration/budget/#budget-always-exceeded","title":"Budget Always Exceeded","text":"<p>Problem: Every run exceeds budget</p> <p>Possible causes:</p> <ol> <li>Too many intents or models</li> <li>Budget limit too low</li> <li>Expensive models (GPT-4, Claude Opus)</li> <li>Web search enabled</li> </ol> <p>Solutions:</p> <pre><code># Reduce intents\nintents:\n  - id: \"primary-intent\"\n    prompt: \"Most important question\"\n\n# Reduce models\nmodels:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"  # Cheapest option\n\n# Increase budget\nbudget:\n  max_per_run_usd: 2.00  # Higher limit\n\n# Disable web search\nmodels:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    # Remove tools section\n</code></pre>"},{"location":"user-guide/configuration/budget/#estimated-costs-inaccurate","title":"Estimated Costs Inaccurate","text":"<p>Problem: Actual costs differ significantly from estimates</p> <p>Possible causes:</p> <ol> <li>Longer/shorter responses than expected</li> <li>Web search usage not estimated correctly</li> <li>Pricing data outdated</li> <li>Function calling adds tokens</li> </ol> <p>Solutions:</p> <pre><code># Refresh pricing\nllm-answer-watcher prices refresh --force\n\n# Check estimation accuracy\n# (in run_meta.json after run)\ncat output/2025-11-01T08-00-00Z/run_meta.json | jq '.cost_accuracy_percent'\n\n# Adjust safety buffer if needed (future feature)\n</code></pre>"},{"location":"user-guide/configuration/budget/#budget-blocks-valid-runs","title":"Budget Blocks Valid Runs","text":"<p>Problem: Budget blocks run that should be allowed</p> <p>Cause: Budget limit too conservative</p> <p>Solution: Increase limit based on historical data:</p> <pre><code>-- Check average run cost\nSELECT AVG(total_cost_usd) as avg_cost, MAX(total_cost_usd) as max_cost\nFROM runs;\n</code></pre> <p>Set budget to 2x average or 1.5x max:</p> <pre><code>budget:\n  max_per_run_usd: 0.50  # 2x average of $0.25\n</code></pre>"},{"location":"user-guide/configuration/budget/#next-steps","title":"Next Steps","text":"<ul> <li>Cost Management: Deep dive into cost tracking</li> <li>Web Search Configuration: Understand web search costs</li> <li>Model Configuration: Choose cost-effective models</li> <li>Automation: Budget controls in CI/CD</li> </ul>"},{"location":"user-guide/configuration/intents/","title":"Intent Configuration","text":"<p>Intents are the questions you ask LLMs to test brand visibility. Well-designed intents produce actionable insights about how LLMs recommend your brand versus competitors.</p>"},{"location":"user-guide/configuration/intents/#what-is-an-intent","title":"What is an Intent?","text":"<p>An intent represents a buyer-journey question that prospects might ask an LLM when researching solutions.</p> <p>Examples:</p> <ul> <li>\"What are the best CRM tools for startups?\"</li> <li>\"Compare HubSpot vs Salesforce for small teams\"</li> <li>\"How do I improve email deliverability?\"</li> </ul>"},{"location":"user-guide/configuration/intents/#basic-intent-configuration","title":"Basic Intent Configuration","text":""},{"location":"user-guide/configuration/intents/#minimal-intent","title":"Minimal Intent","text":"<p>Simplest intent with required fields:</p> <pre><code>intents:\n  - id: \"best-tools\"\n    prompt: \"What are the best tools for my category?\"\n</code></pre> <p>Required fields:</p> <ul> <li><code>id</code>: Unique identifier (alphanumeric, hyphens, underscores)</li> <li><code>prompt</code>: Natural language question</li> </ul>"},{"location":"user-guide/configuration/intents/#multiple-intents","title":"Multiple Intents","text":"<p>Test different buyer scenarios:</p> <pre><code>intents:\n  - id: \"best-tools-general\"\n    prompt: \"What are the best email warmup tools?\"\n\n  - id: \"best-tools-startups\"\n    prompt: \"What are the best email warmup tools for startups?\"\n\n  - id: \"comparison-with-competitor\"\n    prompt: \"Compare Instantly vs Warmly for email warmup\"\n</code></pre> <p>How Many Intents?</p> <p>Recommended: 3-10 intents</p> <ul> <li>Too few: Limited coverage of buyer journey</li> <li>Too many: High costs, slow execution</li> </ul> <p>Focus on intents that represent actual buyer questions.</p>"},{"location":"user-guide/configuration/intents/#intent-design-principles","title":"Intent Design Principles","text":""},{"location":"user-guide/configuration/intents/#1-natural-language","title":"1. Natural Language","text":"<p>Write prompts as real users would ask:</p> <pre><code># \u2705 Good: Natural question\nintents:\n  - id: \"best-crm-startups\"\n    prompt: \"What's the best CRM for early-stage startups?\"\n\n# \u274c Bad: Unnatural phrasing\nintents:\n  - id: \"crm-query\"\n    prompt: \"List CRM software products ranked by quality for startup segment\"\n</code></pre>"},{"location":"user-guide/configuration/intents/#2-buyer-focused","title":"2. Buyer-Focused","text":"<p>Imply purchase intent:</p> <pre><code># \u2705 Good: Clear purchase intent\nintents:\n  - id: \"best-email-tools\"\n    prompt: \"What are the best email warmup tools to buy?\"\n\n# \u274c Bad: Informational query\nintents:\n  - id: \"email-info\"\n    prompt: \"What is email warming?\"\n</code></pre>"},{"location":"user-guide/configuration/intents/#3-ranking-friendly","title":"3. Ranking-Friendly","text":"<p>Ask for ranked or ordered lists:</p> <pre><code># \u2705 Good: Implies ranking\nintents:\n  - id: \"top-tools\"\n    prompt: \"What are the top 5 email warmup tools?\"\n\n# \u274c Bad: No ranking signal\nintents:\n  - id: \"tools-info\"\n    prompt: \"Tell me about email warmup tools\"\n</code></pre>"},{"location":"user-guide/configuration/intents/#4-specific-use-cases","title":"4. Specific Use Cases","text":"<p>Target specific scenarios:</p> <pre><code># \u2705 Good: Specific use case\nintents:\n  - id: \"best-for-cold-email\"\n    prompt: \"What are the best email warmup tools for cold outreach campaigns?\"\n\n# \u274c Bad: Too generic\nintents:\n  - id: \"email-tools\"\n    prompt: \"What are email tools?\"\n</code></pre>"},{"location":"user-guide/configuration/intents/#intent-patterns","title":"Intent Patterns","text":""},{"location":"user-guide/configuration/intents/#category-leadership","title":"Category Leadership","text":"<p>Test if your brand is considered a category leader:</p> <pre><code>intents:\n  - id: \"best-in-category\"\n    prompt: \"What are the best [category] tools?\"\n\n  - id: \"top-choices\"\n    prompt: \"What are the top [category] platforms?\"\n\n  - id: \"leading-solutions\"\n    prompt: \"What are the leading [category] solutions?\"\n</code></pre>"},{"location":"user-guide/configuration/intents/#segment-specific","title":"Segment-Specific","text":"<p>Target different customer segments:</p> <pre><code>intents:\n  # Startup segment\n  - id: \"best-for-startups\"\n    prompt: \"What are the best CRM tools for early-stage startups?\"\n\n  # SMB segment\n  - id: \"best-for-smb\"\n    prompt: \"What are the best CRM tools for small businesses?\"\n\n  # Enterprise segment\n  - id: \"best-for-enterprise\"\n    prompt: \"What are the best CRM tools for large enterprises?\"\n</code></pre>"},{"location":"user-guide/configuration/intents/#use-case-specific","title":"Use-Case Specific","text":"<p>Target specific jobs-to-be-done:</p> <pre><code>intents:\n  - id: \"improve-deliverability\"\n    prompt: \"What tools can help me improve email deliverability?\"\n\n  - id: \"warm-cold-emails\"\n    prompt: \"How can I warm up my email domain for cold outreach?\"\n\n  - id: \"avoid-spam\"\n    prompt: \"What tools help me avoid the spam folder?\"\n</code></pre>"},{"location":"user-guide/configuration/intents/#competitive-comparison","title":"Competitive Comparison","text":"<p>Test head-to-head comparisons:</p> <pre><code>intents:\n  - id: \"vs-main-competitor\"\n    prompt: \"Compare [YourBrand] vs [MainCompetitor] for [use case]\"\n\n  - id: \"alternatives-to-competitor\"\n    prompt: \"What are the best alternatives to [Competitor]?\"\n\n  - id: \"hubspot-replacement\"\n    prompt: \"What's the best replacement for HubSpot for small teams?\"\n</code></pre>"},{"location":"user-guide/configuration/intents/#problem-solution","title":"Problem-Solution","text":"<p>Frame around customer pain points:</p> <pre><code>intents:\n  - id: \"solve-deliverability\"\n    prompt: \"My emails are going to spam. What tools can help?\"\n\n  - id: \"improve-open-rates\"\n    prompt: \"How can I improve my email open rates?\"\n\n  - id: \"scale-outreach\"\n    prompt: \"What tools help me scale cold email outreach?\"\n</code></pre>"},{"location":"user-guide/configuration/intents/#buying-journey-stages","title":"Buying Journey Stages","text":"<p>Target different stages:</p> <pre><code>intents:\n  # Awareness: \"What is...?\"\n  - id: \"awareness\"\n    prompt: \"What is email warmup and why do I need it?\"\n\n  # Consideration: \"What are the options?\"\n  - id: \"consideration\"\n    prompt: \"What are the best email warmup tools?\"\n\n  # Decision: \"Which should I choose?\"\n  - id: \"decision\"\n    prompt: \"Should I use Warmly or Instantly for email warmup?\"\n</code></pre>"},{"location":"user-guide/configuration/intents/#advanced-intent-configuration","title":"Advanced Intent Configuration","text":""},{"location":"user-guide/configuration/intents/#intent-with-operations","title":"Intent with Operations","text":"<p>Run custom operations after each query:</p> <pre><code>intents:\n  - id: \"best-email-tools\"\n    prompt: \"What are the best email warmup tools?\"\n\n    operations:\n      - id: \"content-gaps\"\n        description: \"Identify content opportunities\"\n        prompt: |\n          Analyze this LLM response and identify content gaps that could improve our ranking.\n\n          My brand: {brand:mine}\n          Current rank: {rank:mine}\n          Response: {intent:response}\n\n          Provide 3 specific content recommendations.\n        model: \"gpt-4o-mini\"\n\n      - id: \"competitor-analysis\"\n        description: \"Extract competitor strengths\"\n        prompt: |\n          What strengths are mentioned for each competitor?\n\n          Competitors: {competitors:mentioned}\n          Response: {intent:response}\n        model: \"gpt-4o-mini\"\n</code></pre> <p>See Operations Configuration for details.</p>"},{"location":"user-guide/configuration/intents/#intent-with-dependencies","title":"Intent with Dependencies","text":"<p>Chain operations with dependencies:</p> <pre><code>intents:\n  - id: \"best-crm-tools\"\n    prompt: \"What are the best CRM tools for startups?\"\n\n    operations:\n      - id: \"extract-features\"\n        description: \"Extract features mentioned\"\n        prompt: \"Extract features mentioned for each tool: {intent:response}\"\n        model: \"gpt-4o-mini\"\n\n      - id: \"gap-analysis\"\n        description: \"Identify feature gaps\"\n        prompt: |\n          Based on these features: {operation:extract-features}\n\n          What features are missing from {brand:mine} compared to competitors?\n        depends_on: [\"extract-features\"]\n        model: \"gpt-4o-mini\"\n</code></pre>"},{"location":"user-guide/configuration/intents/#intent-with-custom-metadata","title":"Intent with Custom Metadata","text":"<p>Add metadata for analysis (future feature):</p> <pre><code>intents:\n  - id: \"best-tools\"\n    prompt: \"What are the best email warmup tools?\"\n    metadata:\n      stage: \"consideration\"\n      priority: \"high\"\n      segment: \"smb\"\n</code></pre> <p>Metadata Not Yet Implemented</p> <p>Custom metadata is planned for a future release.</p>"},{"location":"user-guide/configuration/intents/#intent-id-naming-conventions","title":"Intent ID Naming Conventions","text":"<p>Intent IDs must be:</p> <ul> <li>Unique across the configuration</li> <li>Alphanumeric with hyphens and underscores</li> <li>Descriptive and readable</li> </ul> <p>Good intent IDs:</p> <pre><code>intents:\n  - id: \"best-email-warmup-tools\"\n  - id: \"hubspot-alternatives-smb\"\n  - id: \"improve-deliverability-2025\"\n  - id: \"warmly-vs-instantly\"\n</code></pre> <p>Bad intent IDs:</p> <pre><code># \u274c Not descriptive\nintents:\n  - id: \"intent1\"\n  - id: \"test\"\n  - id: \"query\"\n\n# \u274c Invalid characters\nintents:\n  - id: \"best tools\"        # Space not allowed\n  - id: \"best-tools!\"       # Special char not allowed\n  - id: \"best/tools\"        # Slash not allowed\n</code></pre> <p>Intent ID Best Practices</p> <ul> <li>Use descriptive names that explain the intent</li> <li>Include segment/use-case in ID if relevant</li> <li>Use hyphens for readability: <code>best-crm-for-startups</code></li> <li>Keep under 50 characters</li> <li>Avoid special characters except <code>-</code> and <code>_</code></li> </ul>"},{"location":"user-guide/configuration/intents/#prompt-engineering-for-intents","title":"Prompt Engineering for Intents","text":""},{"location":"user-guide/configuration/intents/#effective-prompt-patterns","title":"Effective Prompt Patterns","text":"<p>Pattern 1: Top N Format</p> <pre><code>intents:\n  - id: \"top-5-crm\"\n    prompt: \"What are the top 5 CRM tools for startups in 2025?\"\n</code></pre> <p>Benefits:</p> <ul> <li>Clear ranking expectation</li> <li>Limited scope (5 items)</li> <li>Time-bound (2025)</li> </ul> <p>Pattern 2: Use-Case Specific</p> <pre><code>intents:\n  - id: \"best-for-cold-email\"\n    prompt: \"What are the best email warmup tools specifically for cold email campaigns?\"\n</code></pre> <p>Benefits:</p> <ul> <li>Targets specific use case</li> <li>Filters out generic responses</li> <li>Relevant to your positioning</li> </ul> <p>Pattern 3: Comparison</p> <pre><code>intents:\n  - id: \"compare-top-tools\"\n    prompt: \"Compare the top email warmup tools for improving deliverability\"\n</code></pre> <p>Benefits:</p> <ul> <li>Encourages detailed analysis</li> <li>Shows relative positioning</li> <li>Highlights differentiators</li> </ul> <p>Pattern 4: Problem-Oriented</p> <pre><code>intents:\n  - id: \"solve-spam-problem\"\n    prompt: \"My sales emails are going to spam. What tools can help me fix this?\"\n</code></pre> <p>Benefits:</p> <ul> <li>Natural buyer question</li> <li>Solution-focused</li> <li>Real pain point</li> </ul> <p>Pattern 5: Segment-Specific</p> <pre><code>intents:\n  - id: \"best-for-startups\"\n    prompt: \"What's the best CRM for a 10-person startup with limited budget?\"\n</code></pre> <p>Benefits:</p> <ul> <li>Targets specific segment</li> <li>Includes constraints (budget)</li> <li>Realistic buyer scenario</li> </ul>"},{"location":"user-guide/configuration/intents/#prompt-length","title":"Prompt Length","text":"<p>Recommended: 10-30 words</p> <pre><code># \u2705 Good: Clear and concise\nintents:\n  - id: \"best-tools\"\n    prompt: \"What are the best email warmup tools for cold outreach in 2025?\"\n\n# \u274c Too short: Lacks context\nintents:\n  - id: \"tools\"\n    prompt: \"Email tools?\"\n\n# \u274c Too long: Overly specific\nintents:\n  - id: \"detailed-query\"\n    prompt: \"I am a sales development representative at a B2B SaaS startup with 5 SDRs sending approximately 500 cold emails per day and we're experiencing deliverability issues with 40% of our emails going to spam, what are the absolute best email warmup tools that can help us improve our domain reputation and inbox placement rate while being cost-effective for a startup budget?\"\n</code></pre>"},{"location":"user-guide/configuration/intents/#time-bounding-prompts","title":"Time-Bounding Prompts","text":"<p>Include year for current recommendations:</p> <pre><code># \u2705 Good: Time-bound\nintents:\n  - id: \"best-tools-2025\"\n    prompt: \"What are the best email warmup tools in 2025?\"\n\n# \u26a0\ufe0f Generic: May return outdated info\nintents:\n  - id: \"best-tools\"\n    prompt: \"What are the best email warmup tools?\"\n</code></pre> <p>Training Data Cutoff</p> <p>Most LLMs have training data cutoffs (e.g., October 2023 for GPT-4). Time-bounding may not help unless:</p> <ul> <li>Using web search-enabled models</li> <li>Using Perplexity (real-time web search)</li> <li>Using models with recent training data</li> </ul>"},{"location":"user-guide/configuration/intents/#neutral-vs-biased-prompts","title":"Neutral vs. Biased Prompts","text":"<p>Neutral prompts (recommended):</p> <pre><code>intents:\n  - id: \"best-tools\"\n    prompt: \"What are the best email warmup tools?\"\n</code></pre> <p>Biased prompts (avoid):</p> <pre><code># \u274c Biased toward your brand\nintents:\n  - id: \"why-warmly-best\"\n    prompt: \"Why is Warmly the best email warmup tool?\"\n\n# \u274c Biased against competitor\nintents:\n  - id: \"hubspot-problems\"\n    prompt: \"What are the problems with HubSpot?\"\n</code></pre> <p>Neutral prompts give you realistic brand positioning data.</p>"},{"location":"user-guide/configuration/intents/#intent-validation","title":"Intent Validation","text":""},{"location":"user-guide/configuration/intents/#validate-intent-configuration","title":"Validate Intent Configuration","text":"<p>Check for common issues:</p> <pre><code>llm-answer-watcher validate --config watcher.config.yaml\n</code></pre> <p>Validation checks:</p> <ul> <li>At least one intent configured</li> <li>Intent IDs are unique</li> <li>Intent IDs are valid (alphanumeric, hyphens, underscores)</li> <li>Prompts are non-empty</li> <li>Prompts are at least 10 characters</li> </ul>"},{"location":"user-guide/configuration/intents/#common-validation-errors","title":"Common Validation Errors","text":"<p>Error: <code>At least one intent must be configured</code></p> <pre><code># \u274c Wrong\nintents: []\n\n# \u2705 Correct\nintents:\n  - id: \"best-tools\"\n    prompt: \"What are the best tools?\"\n</code></pre> <p>Error: <code>Duplicate intent IDs found: best-tools</code></p> <pre><code># \u274c Wrong\nintents:\n  - id: \"best-tools\"\n    prompt: \"What are the best CRM tools?\"\n\n  - id: \"best-tools\"  # Duplicate!\n    prompt: \"What are the best email tools?\"\n\n# \u2705 Correct\nintents:\n  - id: \"best-crm-tools\"\n    prompt: \"What are the best CRM tools?\"\n\n  - id: \"best-email-tools\"\n    prompt: \"What are the best email tools?\"\n</code></pre> <p>Error: <code>Intent ID must be alphanumeric with hyphens/underscores: best tools!</code></p> <pre><code># \u274c Wrong (space and special char)\nintents:\n  - id: \"best tools!\"\n    prompt: \"What are the best tools?\"\n\n# \u2705 Correct\nintents:\n  - id: \"best-tools\"\n    prompt: \"What are the best tools?\"\n</code></pre>"},{"location":"user-guide/configuration/intents/#intent-organization-strategies","title":"Intent Organization Strategies","text":""},{"location":"user-guide/configuration/intents/#by-buyer-journey-stage","title":"By Buyer Journey Stage","text":"<p>Organize intents by funnel stage:</p> <pre><code>intents:\n  # Awareness stage\n  - id: \"awareness-what-is-email-warmup\"\n    prompt: \"What is email warmup and why is it important?\"\n\n  - id: \"awareness-deliverability-problems\"\n    prompt: \"Why are my emails going to spam?\"\n\n  # Consideration stage\n  - id: \"consideration-best-tools\"\n    prompt: \"What are the best email warmup tools?\"\n\n  - id: \"consideration-tool-comparison\"\n    prompt: \"Compare the top email warmup platforms\"\n\n  # Decision stage\n  - id: \"decision-warmly-vs-instantly\"\n    prompt: \"Should I use Warmly or Instantly?\"\n\n  - id: \"decision-pricing\"\n    prompt: \"What's the most cost-effective email warmup tool?\"\n</code></pre>"},{"location":"user-guide/configuration/intents/#by-customer-segment","title":"By Customer Segment","text":"<p>Organize intents by target segment:</p> <pre><code>intents:\n  # Startup segment\n  - id: \"startup-best-crm\"\n    prompt: \"What's the best CRM for early-stage startups?\"\n\n  - id: \"startup-affordable-tools\"\n    prompt: \"What are affordable CRM options for startups?\"\n\n  # SMB segment\n  - id: \"smb-best-crm\"\n    prompt: \"What's the best CRM for small businesses?\"\n\n  - id: \"smb-easy-setup\"\n    prompt: \"What's the easiest CRM to set up for a 20-person team?\"\n\n  # Enterprise segment\n  - id: \"enterprise-best-crm\"\n    prompt: \"What's the best enterprise CRM platform?\"\n\n  - id: \"enterprise-scalable\"\n    prompt: \"What CRM platforms scale to 1000+ users?\"\n</code></pre>"},{"location":"user-guide/configuration/intents/#by-use-case","title":"By Use Case","text":"<p>Organize intents by jobs-to-be-done:</p> <pre><code>intents:\n  # Use case: Cold email\n  - id: \"cold-email-best-tools\"\n    prompt: \"What are the best tools for cold email outreach?\"\n\n  - id: \"cold-email-deliverability\"\n    prompt: \"How can I improve cold email deliverability?\"\n\n  # Use case: Account-based sales\n  - id: \"abs-best-tools\"\n    prompt: \"What are the best tools for account-based sales?\"\n\n  - id: \"abs-personalization\"\n    prompt: \"What tools help personalize outreach at scale?\"\n\n  # Use case: Lead nurturing\n  - id: \"nurture-best-tools\"\n    prompt: \"What are the best tools for lead nurturing?\"\n</code></pre>"},{"location":"user-guide/configuration/intents/#by-competitor","title":"By Competitor","text":"<p>Track competitive positioning:</p> <pre><code>intents:\n  # vs. Main Competitor\n  - id: \"vs-instantly\"\n    prompt: \"Compare Warmly vs Instantly for email warmup\"\n\n  - id: \"alternatives-to-instantly\"\n    prompt: \"What are the best alternatives to Instantly?\"\n\n  # vs. Market Leader\n  - id: \"vs-hubspot\"\n    prompt: \"Compare Warmly vs HubSpot for sales outreach\"\n\n  - id: \"alternatives-to-hubspot\"\n    prompt: \"What are the best alternatives to HubSpot for startups?\"\n</code></pre>"},{"location":"user-guide/configuration/intents/#testing-intent-prompts","title":"Testing Intent Prompts","text":""},{"location":"user-guide/configuration/intents/#manual-testing","title":"Manual Testing","text":"<p>Test prompts with ChatGPT/Claude before adding:</p> <ol> <li>Ask the prompt directly</li> <li>Check if response includes ranked lists</li> <li>Verify brand mentions</li> <li>Adjust prompt as needed</li> </ol>"},{"location":"user-guide/configuration/intents/#ab-testing-intents","title":"A/B Testing Intents","text":"<p>Compare prompt variations:</p> <pre><code>intents:\n  # Variation A: Generic\n  - id: \"best-tools-generic\"\n    prompt: \"What are the best email warmup tools?\"\n\n  # Variation B: Specific\n  - id: \"best-tools-specific\"\n    prompt: \"What are the best email warmup tools for cold outreach in 2025?\"\n</code></pre> <p>Compare results to see which prompt surfaces your brand better.</p>"},{"location":"user-guide/configuration/intents/#iteration-process","title":"Iteration Process","text":"<ol> <li>Start broad: Test generic prompts</li> <li>Analyze results: Check brand mention rates</li> <li>Refine prompts: Add specificity where needed</li> <li>Test again: Compare refined vs. original</li> <li>Keep winners: Use prompts with best brand visibility</li> </ol>"},{"location":"user-guide/configuration/intents/#intent-metrics","title":"Intent Metrics","text":"<p>Track intent performance:</p> <pre><code>-- Mention rate by intent\nSELECT\n    intent_id,\n    COUNT(DISTINCT run_id) as runs,\n    SUM(CASE WHEN normalized_name IN ('mybrand', 'mybrand.io') THEN 1 ELSE 0 END) as my_brand_mentions,\n    (SUM(CASE WHEN normalized_name IN ('mybrand', 'mybrand.io') THEN 1 ELSE 0 END) * 100.0 / COUNT(DISTINCT run_id)) as mention_rate\nFROM mentions\nGROUP BY intent_id\nORDER BY mention_rate DESC;\n</code></pre> <pre><code>-- Average rank by intent\nSELECT\n    intent_id,\n    AVG(rank_position) as avg_rank,\n    MIN(rank_position) as best_rank,\n    COUNT(*) as total_mentions\nFROM mentions\nWHERE normalized_name IN ('mybrand', 'mybrand.io')\nGROUP BY intent_id\nORDER BY avg_rank ASC;\n</code></pre> <pre><code>-- Top-performing intents\nSELECT\n    intent_id,\n    COUNT(*) as queries,\n    SUM(CASE WHEN normalized_name IN ('mybrand', 'mybrand.io') AND rank_position &lt;= 3 THEN 1 ELSE 0 END) as top3_mentions,\n    (SUM(CASE WHEN normalized_name IN ('mybrand', 'mybrand.io') AND rank_position &lt;= 3 THEN 1 ELSE 0 END) * 100.0 / COUNT(*)) as top3_rate\nFROM mentions\nGROUP BY intent_id\nORDER BY top3_rate DESC;\n</code></pre>"},{"location":"user-guide/configuration/intents/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/configuration/intents/#1-start-with-3-5-core-intents","title":"1. Start with 3-5 Core Intents","text":"<p>Begin with essential buyer questions:</p> <pre><code>intents:\n  - id: \"best-tools\"\n    prompt: \"What are the best [category] tools?\"\n\n  - id: \"best-for-startups\"\n    prompt: \"What's the best [category] tool for startups?\"\n\n  - id: \"vs-main-competitor\"\n    prompt: \"Compare [YourBrand] vs [MainCompetitor]\"\n</code></pre>"},{"location":"user-guide/configuration/intents/#2-test-prompts-manually-first","title":"2. Test Prompts Manually First","text":"<p>Before adding to config, test with ChatGPT/Claude:</p> <ul> <li>Does it produce ranked lists?</li> <li>Does it mention your brand?</li> <li>Is the response format consistent?</li> </ul>"},{"location":"user-guide/configuration/intents/#3-use-natural-language","title":"3. Use Natural Language","text":"<p>Write prompts as real users would ask:</p> <pre><code># \u2705 Good\nintents:\n  - id: \"improve-deliverability\"\n    prompt: \"How can I improve my email deliverability?\"\n\n# \u274c Bad\nintents:\n  - id: \"deliverability\"\n    prompt: \"EMAIL_DELIVERABILITY_TOOLS_QUERY\"\n</code></pre>"},{"location":"user-guide/configuration/intents/#4-include-ranking-signals","title":"4. Include Ranking Signals","text":"<p>Ask for \"best\", \"top\", or \"recommended\":</p> <pre><code>intents:\n  - id: \"best-tools\"\n    prompt: \"What are the best email warmup tools?\"  # \"best\" = ranking signal\n\n  - id: \"top-tools\"\n    prompt: \"What are the top 5 CRM platforms?\"  # \"top 5\" = ranking signal\n</code></pre>"},{"location":"user-guide/configuration/intents/#5-version-control-intents","title":"5. Version Control Intents","text":"<p>Track intent changes with git:</p> <pre><code>git add watcher.config.yaml\ngit commit -m \"feat: add cold-email intent for startup segment\"\n</code></pre>"},{"location":"user-guide/configuration/intents/#6-monitor-intent-performance","title":"6. Monitor Intent Performance","text":"<p>Review which intents surface your brand:</p> <pre><code>SELECT intent_id, COUNT(*) as my_brand_mentions\nFROM mentions\nWHERE normalized_name = 'mybrand'\nGROUP BY intent_id\nORDER BY my_brand_mentions DESC;\n</code></pre> <p>Focus on high-performing intents, retire low-performers.</p>"},{"location":"user-guide/configuration/intents/#7-update-prompts-based-on-results","title":"7. Update Prompts Based on Results","text":"<p>Iterate on prompts:</p> <pre><code># Original (low brand mentions)\n- id: \"best-tools\"\n  prompt: \"What are email tools?\"\n\n# Improved (higher brand mentions)\n- id: \"best-tools\"\n  prompt: \"What are the best email warmup tools for cold outreach?\"\n</code></pre>"},{"location":"user-guide/configuration/intents/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/configuration/intents/#brand-not-mentioned","title":"Brand Not Mentioned","text":"<p>Problem: Your brand doesn't appear in LLM responses</p> <p>Possible causes:</p> <ol> <li>Generic prompt: Too broad, LLM focuses on market leaders</li> <li>Wrong segment: Prompt targets different customer segment</li> <li>Outdated training data: LLM trained before your brand existed</li> </ol> <p>Solutions:</p> <ul> <li>Make prompt more specific to your use case</li> <li>Target your niche/segment explicitly</li> <li>Use web search-enabled models for fresh data</li> </ul>"},{"location":"user-guide/configuration/intents/#inconsistent-responses","title":"Inconsistent Responses","text":"<p>Problem: Different responses for same intent across runs</p> <p>Cause: LLM non-determinism (temperature &gt; 0)</p> <p>Solution: Use lower temperature for consistency:</p> <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    temperature: 0.0  # Deterministic\n</code></pre>"},{"location":"user-guide/configuration/intents/#no-ranked-lists","title":"No Ranked Lists","text":"<p>Problem: LLM doesn't provide ranked lists</p> <p>Cause: Prompt doesn't request ranking</p> <p>Solution: Add ranking signal:</p> <pre><code># \u274c Before\n- id: \"tools\"\n  prompt: \"Tell me about email warmup tools\"\n\n# \u2705 After\n- id: \"top-tools\"\n  prompt: \"What are the top 5 email warmup tools ranked by quality?\"\n</code></pre>"},{"location":"user-guide/configuration/intents/#next-steps","title":"Next Steps","text":"<ul> <li>Brand Configuration: Optimize brand detection</li> <li>Operations Configuration: Automate post-query analysis</li> <li>Rank Extraction: Understand ranking detection</li> <li>HTML Reports: Visualize intent results</li> </ul>"},{"location":"user-guide/configuration/models/","title":"Model Configuration","text":"<p>Model configuration controls which LLMs to query and how they're accessed. LLM Answer Watcher supports multiple providers with unified configuration.</p>"},{"location":"user-guide/configuration/models/#supported-providers","title":"Supported Providers","text":"Provider Models Available Pricing Best For OpenAI gpt-4o-mini, gpt-4o, gpt-4-turbo \\(0.15-\\)10/1M tokens Fast, cost-effective, production Anthropic claude-3-5-haiku, claude-3-5-sonnet, claude-3-opus \\(0.80-\\)75/1M tokens High-quality reasoning Mistral mistral-large, mistral-medium, mistral-small \\(2-\\)8/1M tokens European compliance X.AI Grok grok-beta, grok-2-1212, grok-3 \\(2-\\)25/1M tokens Real-time X integration Google gemini-2.0-flash, gemini-1.5-pro \\(0.075-\\)7/1M tokens Multimodal, fast Perplexity sonar, sonar-pro, sonar-reasoning \\(1-\\)15/1M tokens Web-grounded answers"},{"location":"user-guide/configuration/models/#basic-model-configuration","title":"Basic Model Configuration","text":""},{"location":"user-guide/configuration/models/#single-model-setup","title":"Single Model Setup","text":"<p>Minimal configuration with one model:</p> <pre><code>run_settings:\n  models:\n    - provider: \"openai\"\n      model_name: \"gpt-4o-mini\"\n      env_api_key: \"OPENAI_API_KEY\"\n</code></pre> <p>Required fields:</p> <ul> <li><code>provider</code>: Provider name (see supported providers above)</li> <li><code>model_name</code>: Specific model identifier</li> <li><code>env_api_key</code>: Environment variable name containing API key</li> </ul>"},{"location":"user-guide/configuration/models/#multi-model-setup","title":"Multi-Model Setup","text":"<p>Query multiple models for comparison:</p> <pre><code>run_settings:\n  models:\n    # Fast and cheap\n    - provider: \"openai\"\n      model_name: \"gpt-4o-mini\"\n      env_api_key: \"OPENAI_API_KEY\"\n\n    # High quality\n    - provider: \"anthropic\"\n      model_name: \"claude-3-5-sonnet-20241022\"\n      env_api_key: \"ANTHROPIC_API_KEY\"\n\n    # Web-grounded\n    - provider: \"perplexity\"\n      model_name: \"sonar-pro\"\n      env_api_key: \"PERPLEXITY_API_KEY\"\n</code></pre> <p>Multi-Model Benefits</p> <p>Querying multiple models helps you:</p> <ul> <li>Compare providers: See which LLMs favor your brand</li> <li>Reduce variance: Average rankings across models</li> <li>Hedge risk: Don't depend on one provider's algorithm</li> <li>Track trends: Monitor provider-specific changes over time</li> </ul>"},{"location":"user-guide/configuration/models/#provider-specific-configuration","title":"Provider-Specific Configuration","text":""},{"location":"user-guide/configuration/models/#openai","title":"OpenAI","text":"<p>Supported models:</p> <ul> <li><code>gpt-4o-mini</code>: Fast, cheap, production-ready (\\(0.15/\\)0.60 per 1M input/output tokens)</li> <li><code>gpt-4o</code>: High quality, balanced cost (\\(2.50/\\)10 per 1M tokens)</li> <li><code>gpt-4-turbo</code>: Fast GPT-4, good for complex tasks (\\(10/\\)30 per 1M tokens)</li> <li><code>gpt-3.5-turbo</code>: Legacy, very cheap (\\(0.50/\\)1.50 per 1M tokens)</li> </ul> <p>Basic configuration:</p> <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n</code></pre> <p>With custom system prompt:</p> <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    system_prompt: \"openai/gpt-4-default\"\n</code></pre> <p>With web search enabled:</p> <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    tools:\n      - type: \"web_search\"\n    tool_choice: \"auto\"\n</code></pre> <p>Web Search Costs</p> <p>OpenAI web search adds \\(10-\\)25 per 1,000 calls. See Web Search Configuration.</p> <p>API key setup:</p> <pre><code>export OPENAI_API_KEY=sk-your-openai-key-here\n</code></pre> <p>Get your API key from: platform.openai.com/api-keys</p>"},{"location":"user-guide/configuration/models/#anthropic-claude","title":"Anthropic (Claude)","text":"<p>Supported models:</p> <ul> <li><code>claude-3-5-haiku-20241022</code>: Fast, cheap, smart (\\(0.80/\\)4 per 1M tokens)</li> <li><code>claude-3-5-sonnet-20241022</code>: Balanced quality/cost (\\(3/\\)15 per 1M tokens)</li> <li><code>claude-3-opus-20240229</code>: Highest quality (\\(15/\\)75 per 1M tokens)</li> </ul> <p>Basic configuration:</p> <pre><code>models:\n  - provider: \"anthropic\"\n    model_name: \"claude-3-5-haiku-20241022\"\n    env_api_key: \"ANTHROPIC_API_KEY\"\n</code></pre> <p>With custom system prompt:</p> <pre><code>models:\n  - provider: \"anthropic\"\n    model_name: \"claude-3-5-sonnet-20241022\"\n    env_api_key: \"ANTHROPIC_API_KEY\"\n    system_prompt: \"anthropic/default\"\n</code></pre> <p>API key setup:</p> <pre><code>export ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here\n</code></pre> <p>Get your API key from: console.anthropic.com/settings/keys</p> <p>Claude Strengths</p> <p>Claude models excel at:</p> <ul> <li>Nuanced reasoning: Better at understanding context</li> <li>Longer responses: More comprehensive answers</li> <li>Safety: Strong content moderation</li> <li>Instruction following: Precise adherence to prompts</li> </ul>"},{"location":"user-guide/configuration/models/#mistral","title":"Mistral","text":"<p>Supported models:</p> <ul> <li><code>mistral-large-latest</code>: Flagship model (\\(2/\\)6 per 1M tokens)</li> <li><code>mistral-medium-latest</code>: Balanced (\\(2.50/\\)7.50 per 1M tokens)</li> <li><code>mistral-small-latest</code>: Fast and cheap (\\(0.20/\\)0.60 per 1M tokens)</li> </ul> <p>Basic configuration:</p> <pre><code>models:\n  - provider: \"mistral\"\n    model_name: \"mistral-large-latest\"\n    env_api_key: \"MISTRAL_API_KEY\"\n</code></pre> <p>API key setup:</p> <pre><code>export MISTRAL_API_KEY=your-mistral-api-key-here\n</code></pre> <p>Get your API key from: console.mistral.ai/api-keys</p> <p>Mistral Strengths</p> <p>Mistral models are ideal for:</p> <ul> <li>European compliance: GDPR-friendly European provider</li> <li>Multilingual: Strong performance in French, German, Spanish</li> <li>Cost efficiency: Competitive pricing</li> <li>Open weights: Some models have open weights available</li> </ul>"},{"location":"user-guide/configuration/models/#xai-grok","title":"X.AI (Grok)","text":"<p>Supported models:</p> <ul> <li><code>grok-beta</code>: Beta access model (\\(2/\\)10 per 1M tokens)</li> <li><code>grok-2-1212</code>: Latest stable version (\\(2/\\)10 per 1M tokens)</li> <li><code>grok-2-latest</code>: Always latest version (\\(2/\\)10 per 1M tokens)</li> <li><code>grok-3</code>: Next-generation model (\\(5/\\)25 per 1M tokens)</li> <li><code>grok-3-mini</code>: Fast, lightweight (\\(2/\\)8 per 1M tokens)</li> </ul> <p>Basic configuration:</p> <pre><code>models:\n  - provider: \"grok\"\n    model_name: \"grok-2-1212\"\n    env_api_key: \"XAI_API_KEY\"\n</code></pre> <p>API key setup:</p> <pre><code>export XAI_API_KEY=xai-your-grok-key-here\n</code></pre> <p>Get your API key from: console.x.ai</p> <p>Grok Strengths</p> <p>Grok models offer:</p> <ul> <li>X platform integration: Real-time data from X (Twitter)</li> <li>OpenAI compatibility: Drop-in replacement for OpenAI API</li> <li>Current events: Up-to-date information</li> <li>Humor: Unique personality in responses</li> </ul>"},{"location":"user-guide/configuration/models/#google-gemini","title":"Google (Gemini)","text":"<p>Supported models:</p> Model Cost (Input/Output) Grounding Best For <code>gemini-2.5-flash</code> \\(0.04/\\)0.12 per 1M \u2705 Yes Recommended - production <code>gemini-2.5-flash-lite</code> \\(0.02/\\)0.06 per 1M \u274c No High-volume, non-grounded <code>gemini-2.5-pro</code> \\(0.60/\\)1.80 per 1M \u2705 Yes Highest quality <code>gemini-2.0-flash-exp</code> \\(0.075/\\)0.30 per 1M \u26a0\ufe0f Experimental Testing <code>gemini-1.5-pro</code> \\(1.25/\\)5 per 1M \u274c No Legacy (not recommended) <p>Basic configuration (without grounding):</p> <pre><code>models:\n  - provider: \"google\"\n    model_name: \"gemini-2.5-flash-lite\"\n    env_api_key: \"GEMINI_API_KEY\"\n</code></pre> <p>With Google Search grounding (recommended for brand monitoring):</p> <pre><code>models:\n  - provider: \"google\"\n    model_name: \"gemini-2.5-flash\"\n    env_api_key: \"GEMINI_API_KEY\"\n    system_prompt: \"google/gemini-grounding\"\n    tools:\n      - google_search: {}  # Enable Google Search\n</code></pre> <p>API key setup:</p> <pre><code>export GEMINI_API_KEY=AIza-your-google-api-key-here\n</code></pre> <p>Get your API key from: aistudio.google.com/app/apikey</p> <p>Gemini Strengths</p> <p>Gemini models excel at:</p> <ul> <li>Google Search grounding: Real-time web data with no per-request fees</li> <li>Speed: Very fast inference</li> <li>Cost: Most cost-effective for web-grounded queries</li> <li>Multimodal: Built for text, image, video, audio</li> <li>Long context: Up to 2M token context window</li> </ul> <p>Configuration Format Difference</p> <p>Google uses <code>google_search: {}</code> (dictionary format) while OpenAI uses <code>type: \"web_search\"</code> (typed format). This reflects different provider API specifications. See Google provider docs for details.</p>"},{"location":"user-guide/configuration/models/#perplexity","title":"Perplexity","text":"<p>Supported models:</p> <ul> <li><code>sonar</code>: Fast, web-grounded (\\(1/\\)1 per 1M tokens + request fees)</li> <li><code>sonar-pro</code>: High-quality grounded (\\(3/\\)15 per 1M tokens + request fees)</li> <li><code>sonar-reasoning</code>: Enhanced reasoning (\\(1/\\)5 per 1M tokens + request fees)</li> <li><code>sonar-reasoning-pro</code>: Best reasoning (\\(3/\\)15 per 1M tokens + request fees)</li> <li><code>sonar-deep-research</code>: In-depth research (\\(3/\\)15 per 1M tokens + request fees)</li> </ul> <p>Basic configuration:</p> <pre><code>models:\n  - provider: \"perplexity\"\n    model_name: \"sonar-pro\"\n    env_api_key: \"PERPLEXITY_API_KEY\"\n</code></pre> <p>API key setup:</p> <pre><code>export PERPLEXITY_API_KEY=pplx-your-perplexity-key-here\n</code></pre> <p>Get your API key from: perplexity.ai/settings/api</p> <p>Perplexity Request Fees</p> <p>Perplexity charges additional request fees based on search context:</p> <ul> <li>Basic searches: ~$0.005 per request</li> <li>Complex searches: ~\\(0.01-\\)0.03 per request</li> </ul> <p>These fees are not yet included in cost estimates. Budget accordingly.</p> <p>Perplexity Strengths</p> <p>Perplexity models offer:</p> <ul> <li>Web grounding: All answers cite web sources</li> <li>Fresh data: Real-time web search</li> <li>Citations: Transparent source attribution</li> <li>Research mode: Deep-dive analysis</li> </ul>"},{"location":"user-guide/configuration/models/#advanced-model-configuration","title":"Advanced Model Configuration","text":""},{"location":"user-guide/configuration/models/#custom-system-prompts","title":"Custom System Prompts","text":"<p>System prompts customize model behavior. LLM Answer Watcher includes default prompts for each provider.</p> <p>Using default provider prompt:</p> <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    # Uses openai/default.json automatically\n</code></pre> <p>Using custom prompt:</p> <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    system_prompt: \"openai/extraction-default\"\n</code></pre> <p>Prompt file structure:</p> <p>System prompts are stored in <code>llm_answer_watcher/system_prompts/{provider}/{prompt_name}.json</code>:</p> <pre><code>{\n  \"role\": \"system\",\n  \"content\": \"You are a helpful assistant that provides accurate, comprehensive answers to user questions about software tools and services. When asked for recommendations, provide a balanced view of multiple options with their strengths and weaknesses.\"\n}\n</code></pre> <p>Creating custom prompts:</p> <ol> <li>Create a new prompt file in the provider directory</li> <li>Reference it in your configuration</li> <li>Test with validation:</li> </ol> <pre><code>llm-answer-watcher validate --config watcher.config.yaml\n</code></pre> <p>System Prompt Best Practices</p> <ul> <li>Be specific: Clear instructions produce better results</li> <li>Stay neutral: Don't bias toward your brand</li> <li>Request structure: Ask for ranked lists, numbered items</li> <li>Test variations: Try different prompts, measure impact</li> </ul>"},{"location":"user-guide/configuration/models/#temperature-and-sampling","title":"Temperature and Sampling","text":"<p>Control response randomness (some providers only):</p> <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    temperature: 0.7  # 0.0 = deterministic, 1.0 = creative\n    top_p: 0.9        # Nucleus sampling\n</code></pre> <p>Temperature Guide</p> <ul> <li>0.0-0.3: Deterministic, consistent answers (recommended for monitoring)</li> <li>0.4-0.7: Balanced creativity and consistency</li> <li>0.8-1.0: Creative, varied responses (not recommended for tracking)</li> </ul>"},{"location":"user-guide/configuration/models/#max-tokens","title":"Max Tokens","text":"<p>Limit response length:</p> <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    max_tokens: 1000  # Limit to ~750 words\n</code></pre> <p>Max Tokens and Cost</p> <p>Setting <code>max_tokens</code> limits output cost but may truncate responses. For monitoring, allow enough tokens for complete answers (500-2000 recommended).</p>"},{"location":"user-guide/configuration/models/#tools-and-function-calling","title":"Tools and Function Calling","text":"<p>Enable tools like web search:</p> <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    tools:\n      - type: \"web_search\"\n    tool_choice: \"auto\"  # or \"required\", \"none\"\n</code></pre> <p>Tool choice options:</p> <ul> <li><code>auto</code>: Model decides when to use tools (recommended)</li> <li><code>required</code>: Model must use tools for every query</li> <li><code>none</code>: Disable tools for this query</li> </ul> <p>See Web Search Configuration for details.</p>"},{"location":"user-guide/configuration/models/#model-selection-strategies","title":"Model Selection Strategies","text":""},{"location":"user-guide/configuration/models/#cost-optimized","title":"Cost-Optimized","text":"<p>Minimize costs with cheap models:</p> <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"  # $0.15/$0.60 per 1M tokens\n    env_api_key: \"OPENAI_API_KEY\"\n\n  - provider: \"google\"\n    model_name: \"gemini-2.0-flash-exp\"  # $0.075/$0.30 per 1M tokens\n    env_api_key: \"GOOGLE_API_KEY\"\n</code></pre> <p>Estimated cost per run (3 intents): ~\\(0.003-\\)0.005</p> <p>Use when:</p> <ul> <li>Running frequent monitoring (hourly/daily)</li> <li>Testing configuration changes</li> <li>Limited budget</li> <li>High query volume</li> </ul>"},{"location":"user-guide/configuration/models/#quality-optimized","title":"Quality-Optimized","text":"<p>Best accuracy with premium models:</p> <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o\"  # $2.50/$10 per 1M tokens\n    env_api_key: \"OPENAI_API_KEY\"\n\n  - provider: \"anthropic\"\n    model_name: \"claude-3-5-sonnet-20241022\"  # $3/$15 per 1M tokens\n    env_api_key: \"ANTHROPIC_API_KEY\"\n</code></pre> <p>Estimated cost per run (3 intents): ~\\(0.05-\\)0.10</p> <p>Use when:</p> <ul> <li>Weekly/monthly executive reports</li> <li>Competitive intelligence deep-dives</li> <li>High-stakes positioning decisions</li> <li>Complex queries requiring reasoning</li> </ul>"},{"location":"user-guide/configuration/models/#balanced","title":"Balanced","text":"<p>Mix of cost and quality:</p> <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"  # Fast, cheap baseline\n    env_api_key: \"OPENAI_API_KEY\"\n\n  - provider: \"anthropic\"\n    model_name: \"claude-3-5-haiku-20241022\"  # Quality check\n    env_api_key: \"ANTHROPIC_API_KEY\"\n\n  - provider: \"perplexity\"\n    model_name: \"sonar-pro\"  # Web-grounded\n    env_api_key: \"PERPLEXITY_API_KEY\"\n</code></pre> <p>Estimated cost per run (3 intents): ~\\(0.02-\\)0.04</p> <p>Use when:</p> <ul> <li>Regular monitoring (daily/weekly)</li> <li>Comparing provider perspectives</li> <li>Balanced budget</li> <li>Production use cases</li> </ul>"},{"location":"user-guide/configuration/models/#fresh-data","title":"Fresh Data","text":"<p>Web-grounded models for current information:</p> <pre><code>models:\n  - provider: \"perplexity\"\n    model_name: \"sonar-pro\"\n    env_api_key: \"PERPLEXITY_API_KEY\"\n\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    tools:\n      - type: \"web_search\"\n    tool_choice: \"auto\"\n</code></pre> <p>Use when:</p> <ul> <li>Monitoring recent product launches</li> <li>Tracking current events impact</li> <li>Detecting real-time ranking changes</li> <li>Competitive news monitoring</li> </ul>"},{"location":"user-guide/configuration/models/#regional-compliance","title":"Regional Compliance","text":"<p>Models for specific regulatory requirements:</p> <pre><code>models:\n  # European providers for GDPR\n  - provider: \"mistral\"\n    model_name: \"mistral-large-latest\"\n    env_api_key: \"MISTRAL_API_KEY\"\n\n  # Baseline comparison\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n</code></pre> <p>Use when:</p> <ul> <li>GDPR compliance required</li> <li>Data residency requirements</li> <li>Regional preference testing</li> </ul>"},{"location":"user-guide/configuration/models/#model-pricing-comparison","title":"Model Pricing Comparison","text":"<p>Current pricing as of November 2024:</p> Model Input (per 1M tokens) Output (per 1M tokens) Cost per Query* gpt-4o-mini $0.15 $0.60 $0.0004 gpt-4o $2.50 $10.00 $0.0056 claude-3-5-haiku $0.80 $4.00 $0.0022 claude-3-5-sonnet $3.00 $15.00 $0.0090 mistral-large $2.00 $6.00 $0.0040 grok-2-1212 $2.00 $10.00 $0.0054 gemini-2.0-flash $0.075 $0.30 $0.0002 sonar-pro $3.00 $15.00 $0.0090** <p>* Assumes ~150 input tokens + ~500 output tokens per query ** Plus request fees (~\\(0.005-\\)0.03 per query)</p> <p>Dynamic Pricing</p> <p>LLM Answer Watcher automatically loads current pricing from llm-prices.com with 24-hour caching. Prices may change.</p> <p>Check current pricing: <pre><code>llm-answer-watcher prices show\n</code></pre></p>"},{"location":"user-guide/configuration/models/#extraction-model-configuration","title":"Extraction Model Configuration","text":"<p>Use a dedicated model for extraction (faster, cheaper than querying main models):</p> <pre><code>extraction_settings:\n  extraction_model:\n    provider: \"openai\"\n    model_name: \"gpt-4o-mini\"  # Fast, cheap model\n    env_api_key: \"OPENAI_API_KEY\"\n    system_prompt: \"openai/extraction-default\"\n\n  method: \"function_calling\"\n  fallback_to_regex: true\n  min_confidence: 0.7\n</code></pre> <p>Benefits:</p> <ul> <li>Cost savings: Use cheap model for extraction</li> <li>Speed: Fast models for quick parsing</li> <li>Separation: Main models for quality, extraction model for structure</li> <li>Accuracy: Function calling more accurate than regex</li> </ul> <p>Recommended extraction models:</p> <ul> <li><code>gpt-4o-mini</code>: Best balance of speed, cost, accuracy</li> <li><code>gpt-4.1-nano</code>: Ultra-fast, ultra-cheap (OpenAI only)</li> <li><code>gemini-2.0-flash-exp</code>: Very fast, very cheap</li> <li><code>claude-3-5-haiku-20241022</code>: High accuracy, reasonable cost</li> </ul> <p>See Function Calling for details.</p>"},{"location":"user-guide/configuration/models/#multi-model-comparison-strategies","title":"Multi-Model Comparison Strategies","text":""},{"location":"user-guide/configuration/models/#ab-testing","title":"A/B Testing","text":"<p>Compare two providers:</p> <pre><code>models:\n  # Variant A: OpenAI\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n\n  # Variant B: Anthropic\n  - provider: \"anthropic\"\n    model_name: \"claude-3-5-haiku-20241022\"\n    env_api_key: \"ANTHROPIC_API_KEY\"\n</code></pre> <p>Analyze results:</p> <pre><code>-- Compare brand mentions by provider\nSELECT\n    model_provider,\n    COUNT(*) as total_mentions,\n    AVG(rank_position) as avg_rank\nFROM mentions\nWHERE normalized_name = 'yourbrand'\nGROUP BY model_provider;\n</code></pre>"},{"location":"user-guide/configuration/models/#provider-diversity","title":"Provider Diversity","text":"<p>Query multiple providers for comprehensive coverage:</p> <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n\n  - provider: \"anthropic\"\n    model_name: \"claude-3-5-haiku-20241022\"\n    env_api_key: \"ANTHROPIC_API_KEY\"\n\n  - provider: \"perplexity\"\n    model_name: \"sonar-pro\"\n    env_api_key: \"PERPLEXITY_API_KEY\"\n\n  - provider: \"google\"\n    model_name: \"gemini-2.0-flash-exp\"\n    env_api_key: \"GOOGLE_API_KEY\"\n</code></pre> <p>Benefits:</p> <ul> <li>Reduce algorithm dependence</li> <li>Hedge against provider changes</li> <li>Capture diverse perspectives</li> <li>Build comprehensive dataset</li> </ul>"},{"location":"user-guide/configuration/models/#model-size-comparison","title":"Model Size Comparison","text":"<p>Compare model sizes within a provider:</p> <pre><code>models:\n  # Small model\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n\n  # Large model\n  - provider: \"openai\"\n    model_name: \"gpt-4o\"\n    env_api_key: \"OPENAI_API_KEY\"\n</code></pre> <p>Analyze cost vs. quality trade-offs:</p> <pre><code>-- Compare cost and mention rates by model\nSELECT\n    model_name,\n    COUNT(*) as queries,\n    SUM(estimated_cost_usd) as total_cost,\n    AVG(estimated_cost_usd) as avg_cost_per_query,\n    SUM(CASE WHEN brand IN (SELECT * FROM mine_brands) THEN 1 ELSE 0 END) as my_brand_mentions\nFROM answers_raw\nGROUP BY model_name;\n</code></pre>"},{"location":"user-guide/configuration/models/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/configuration/models/#api-key-issues","title":"API Key Issues","text":"<p>Problem: <code>API key not found: OPENAI_API_KEY</code></p> <p>Solution: Set the environment variable:</p> <pre><code>export OPENAI_API_KEY=sk-your-key-here\n</code></pre> <p>Verify:</p> <pre><code>echo $OPENAI_API_KEY\nllm-answer-watcher validate --config watcher.config.yaml\n</code></pre> <p>Problem: <code>Invalid API key for provider openai</code></p> <p>Solution: Check API key format and validity:</p> <pre><code># Test with curl (OpenAI)\ncurl https://api.openai.com/v1/models \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\"\n\n# Test with curl (Anthropic)\ncurl https://api.anthropic.com/v1/messages \\\n  -H \"x-api-key: $ANTHROPIC_API_KEY\" \\\n  -H \"anthropic-version: 2023-06-01\"\n</code></pre> <p>Get a new API key from your provider's console.</p>"},{"location":"user-guide/configuration/models/#model-not-found","title":"Model Not Found","text":"<p>Problem: <code>Model not found: gpt-4-mini</code></p> <p>Solution: Use correct model name:</p> <pre><code># \u274c Wrong (doesn't exist)\nmodel_name: \"gpt-4-mini\"\n\n# \u2705 Correct\nmodel_name: \"gpt-4o-mini\"\n</code></pre> <p>Check provider documentation for valid models.</p>"},{"location":"user-guide/configuration/models/#rate-limiting","title":"Rate Limiting","text":"<p>Problem: <code>Rate limit exceeded for provider openai</code></p> <p>Solution: LLM Answer Watcher automatically retries with exponential backoff. If persistent:</p> <ol> <li>Upgrade to higher rate limits (pay-as-you-go tier)</li> <li>Reduce concurrent queries</li> <li>Add delays between queries:</li> </ol> <pre><code>run_settings:\n  rate_limit_delay_seconds: 1  # Delay between queries\n</code></pre>"},{"location":"user-guide/configuration/models/#cost-overruns","title":"Cost Overruns","text":"<p>Problem: Unexpected high costs</p> <p>Solution: Enable budget controls:</p> <pre><code>run_settings:\n  budget:\n    enabled: true\n    max_per_run_usd: 1.00\n    warn_threshold_usd: 0.50\n</code></pre> <p>Check estimated costs before running:</p> <pre><code>llm-answer-watcher run --config watcher.config.yaml --dry-run\n</code></pre> <p>See Budget Configuration for details.</p>"},{"location":"user-guide/configuration/models/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/configuration/models/#1-start-with-one-model","title":"1. Start with One Model","text":"<p>Begin with a single cheap model:</p> <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n</code></pre> <p>Validate your configuration, then expand to multiple models.</p>"},{"location":"user-guide/configuration/models/#2-use-cost-optimized-models-for-frequent-runs","title":"2. Use Cost-Optimized Models for Frequent Runs","text":"<p>Daily/hourly monitoring:</p> <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"  # ~$0.0004 per query\n    env_api_key: \"OPENAI_API_KEY\"\n</code></pre> <p>Weekly reports:</p> <pre><code>models:\n  - provider: \"anthropic\"\n    model_name: \"claude-3-5-sonnet-20241022\"  # ~$0.009 per query\n    env_api_key: \"ANTHROPIC_API_KEY\"\n</code></pre>"},{"location":"user-guide/configuration/models/#3-enable-web-search-for-fresh-data","title":"3. Enable Web Search for Fresh Data","text":"<p>When tracking current events:</p> <pre><code>models:\n  - provider: \"perplexity\"\n    model_name: \"sonar-pro\"\n    env_api_key: \"PERPLEXITY_API_KEY\"\n</code></pre> <p>Or:</p> <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    tools:\n      - type: \"web_search\"\n    tool_choice: \"auto\"\n</code></pre>"},{"location":"user-guide/configuration/models/#4-separate-extraction-models","title":"4. Separate Extraction Models","text":"<p>Use dedicated model for extraction:</p> <pre><code># Main models for quality answers\nrun_settings:\n  models:\n    - provider: \"anthropic\"\n      model_name: \"claude-3-5-sonnet-20241022\"\n      env_api_key: \"ANTHROPIC_API_KEY\"\n\n# Cheap model for extraction\nextraction_settings:\n  extraction_model:\n    provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n  method: \"function_calling\"\n</code></pre>"},{"location":"user-guide/configuration/models/#5-version-control-model-configs","title":"5. Version Control Model Configs","text":"<p>Track model changes in git:</p> <pre><code>git add watcher.config.yaml\ngit commit -m \"feat: add Claude 3.5 Sonnet for quality comparison\"\n</code></pre> <p>This creates an audit trail of which models you were using when.</p>"},{"location":"user-guide/configuration/models/#6-monitor-provider-changes","title":"6. Monitor Provider Changes","text":"<p>Providers update models frequently. Subscribe to:</p> <ul> <li>OpenAI Blog</li> <li>Anthropic Blog</li> <li>Mistral Announcements</li> <li>Google AI Blog</li> </ul> <p>Update your config when new models release.</p>"},{"location":"user-guide/configuration/models/#7-test-before-production","title":"7. Test Before Production","text":"<p>Validate new model configurations:</p> <pre><code># Dry run to check costs\nllm-answer-watcher run --config watcher.config.yaml --dry-run\n\n# Validate configuration\nllm-answer-watcher validate --config watcher.config.yaml\n\n# Test with single intent\nllm-answer-watcher run --config watcher.config.yaml --intent best-tools\n</code></pre>"},{"location":"user-guide/configuration/models/#next-steps","title":"Next Steps","text":"<ul> <li>Brand Configuration: Optimize brand detection</li> <li>Intent Configuration: Design effective prompts</li> <li>Budget Configuration: Control costs</li> <li>Web Search Configuration: Enable real-time data</li> <li>Cost Management: Track spending</li> </ul>"},{"location":"user-guide/configuration/operations/","title":"Post-Intent Operations","text":"<p>Post-intent operations allow you to execute custom actions after each intent query completes. This advanced feature enables dynamic workflows like discovering competitors mentioned by LLMs.</p>"},{"location":"user-guide/configuration/operations/#overview","title":"Overview","text":"<p>Operations are defined per-intent and execute after the LLM response is received:</p> <pre><code>intents:\n  - id: \"best-tools\"\n    prompt: \"What are the best tools?\"\n    operations:\n      - type: \"extract_competitors\"\n        save_to: \"discovered_competitors\"\n</code></pre>"},{"location":"user-guide/configuration/operations/#supported-operation-types","title":"Supported Operation Types","text":""},{"location":"user-guide/configuration/operations/#extract_competitors","title":"<code>extract_competitors</code>","text":"<p>Automatically extracts brand names mentioned in LLM responses that aren't in your configured brand lists.</p> <p>Use Case: Discover new competitors you weren't tracking.</p> <p>Configuration:</p> <pre><code>intents:\n  - id: \"market-research\"\n    prompt: \"What are all the tools in this category?\"\n    operations:\n      - type: \"extract_competitors\"\n        save_to: \"discovered_brands\"\n        params:\n          min_confidence: 0.7\n          exclude_generic_terms: true\n</code></pre> <p>Parameters:</p> <ul> <li><code>save_to</code> (required): Variable name to store results</li> <li><code>min_confidence</code>: Minimum confidence threshold (0.0-1.0)</li> <li><code>exclude_generic_terms</code>: Filter out generic words</li> </ul> <p>Output:</p> <p>Results saved to <code>intent_*_operation_extract_competitors.json</code>:</p> <pre><code>{\n  \"operation_type\": \"extract_competitors\",\n  \"discovered_brands\": [\n    {\"name\": \"NewCompetitor\", \"confidence\": 0.95},\n    {\"name\": \"EmergingTool\", \"confidence\": 0.82}\n  ]\n}\n</code></pre>"},{"location":"user-guide/configuration/operations/#operation-chaining","title":"Operation Chaining","text":"<p>Execute multiple operations in sequence:</p> <pre><code>intents:\n  - id: \"comprehensive-analysis\"\n    prompt: \"Analyze the market landscape\"\n    operations:\n      # Step 1: Extract competitors\n      - type: \"extract_competitors\"\n        save_to: \"new_competitors\"\n\n      # Step 2: Could add more operations in future\n      # - type: \"sentiment_analysis\"\n      #   depends_on: \"new_competitors\"\n</code></pre> <p>Operations execute in order and can depend on previous results.</p>"},{"location":"user-guide/configuration/operations/#real-world-examples","title":"Real-World Examples","text":""},{"location":"user-guide/configuration/operations/#market-discovery","title":"Market Discovery","text":"<p>Find competitors you didn't know about:</p> <pre><code>intents:\n  - id: \"discover-market\"\n    prompt: \"List all email marketing tools you know\"\n    operations:\n      - type: \"extract_competitors\"\n        save_to: \"market_scan\"\n        params:\n          min_confidence: 0.8\n</code></pre>"},{"location":"user-guide/configuration/operations/#quarterly-expansion","title":"Quarterly Expansion","text":"<p>Update your competitor list quarterly:</p> <pre><code>intents:\n  - id: \"q1-market-scan\"\n    prompt: \"What are the top 20 tools in our category as of Q1 2025?\"\n    operations:\n      - type: \"extract_competitors\"\n        save_to: \"q1_competitors\"\n</code></pre> <p>Then review <code>q1_competitors.json</code> and add new brands to your config.</p>"},{"location":"user-guide/configuration/operations/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/configuration/operations/#1-use-high-confidence-thresholds","title":"1. Use High Confidence Thresholds","text":"<p>Avoid false positives:</p> <pre><code>params:\n  min_confidence: 0.8  # Only very confident extractions\n</code></pre>"},{"location":"user-guide/configuration/operations/#2-review-before-adding-to-config","title":"2. Review Before Adding to Config","text":"<p>Operations discover candidates - manually review before adding to your brand list.</p>"},{"location":"user-guide/configuration/operations/#3-separate-discovery-intents","title":"3. Separate Discovery Intents","text":"<p>Create dedicated intents for competitor discovery:</p> <pre><code>intents:\n  # Regular monitoring\n  - id: \"best-tools\"\n    prompt: \"What are the best tools?\"\n\n  # Discovery (run monthly)\n  - id: \"market-discovery\"\n    prompt: \"Comprehensive list of all tools in category\"\n    operations:\n      - type: \"extract_competitors\"\n        save_to: \"monthly_scan\"\n</code></pre>"},{"location":"user-guide/configuration/operations/#accessing-operation-results","title":"Accessing Operation Results","text":"<p>Results are stored in the output directory:</p> <pre><code>output/2025-11-05T14-30-00Z/\n\u251c\u2500\u2500 intent_market-discovery_operation_extract_competitors.json\n\u2514\u2500\u2500 ...\n</code></pre> <p>Also queryable from SQLite:</p> <pre><code>SELECT operation_type, operation_results\nFROM intent_operations\nWHERE intent_id = 'market-discovery';\n</code></pre>"},{"location":"user-guide/configuration/operations/#future-operation-types","title":"Future Operation Types","text":"<p>Planned for future releases:</p> <ul> <li><code>sentiment_analysis</code>: Analyze tone of brand mentions</li> <li><code>feature_extraction</code>: Extract mentioned features/capabilities</li> <li><code>pricing_detection</code>: Detect pricing information</li> <li><code>use_case_mapping</code>: Map brands to specific use cases</li> </ul>"},{"location":"user-guide/configuration/operations/#limitations","title":"Limitations","text":"<ul> <li>Operations run synchronously (no parallel execution yet)</li> <li>Limited to extraction tasks (no API calls or external actions)</li> <li>Results require manual review before acting on them</li> </ul>"},{"location":"user-guide/configuration/operations/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about intent configuration</li> <li>See complete examples</li> </ul>"},{"location":"user-guide/configuration/overview/","title":"Configuration Overview","text":"<p>LLM Answer Watcher uses a YAML configuration file to control all aspects of monitoring: which LLMs to query, which brands to track, what questions to ask, and how to manage costs.</p>"},{"location":"user-guide/configuration/overview/#configuration-structure","title":"Configuration Structure","text":"<p>A complete configuration file has these main sections:</p> <pre><code>run_settings:        # Output paths, models, and run behavior\nextraction_settings: # Optional: advanced extraction configuration\nbrands:              # Your brand and competitors to track\nintents:             # Questions to ask LLMs\nglobal_operations:   # Optional: operations run for every intent\n</code></pre>"},{"location":"user-guide/configuration/overview/#quick-start-example","title":"Quick Start Example","text":"<p>Here's a minimal configuration to get started:</p> <pre><code>run_settings:\n  output_dir: \"./output\"\n  sqlite_db_path: \"./output/watcher.db\"\n\n  models:\n    - provider: \"openai\"\n      model_name: \"gpt-4o-mini\"\n      env_api_key: \"OPENAI_API_KEY\"\n\nbrands:\n  mine:\n    - \"MyBrand\"\n    - \"MyBrand.io\"\n\n  competitors:\n    - \"CompetitorA\"\n    - \"CompetitorB\"\n\nintents:\n  - id: \"best-tools\"\n    prompt: \"What are the best tools for [your category]?\"\n</code></pre> <p>Environment Variables</p> <p>Set your API keys as environment variables before running: <pre><code>export OPENAI_API_KEY=sk-your-key-here\nexport ANTHROPIC_API_KEY=sk-ant-your-key-here\n</code></pre></p>"},{"location":"user-guide/configuration/overview/#configuration-sections-explained","title":"Configuration Sections Explained","text":""},{"location":"user-guide/configuration/overview/#run-settings","title":"Run Settings","text":"<p>Controls where output is stored, which models to query, and runtime behavior.</p> <p>Key fields:</p> <ul> <li><code>output_dir</code>: Directory for run results (JSON files, HTML reports)</li> <li><code>sqlite_db_path</code>: SQLite database path for historical tracking</li> <li><code>models</code>: List of LLM models to query (see Model Configuration)</li> <li><code>use_llm_rank_extraction</code>: Use LLM to extract rankings (slower, more accurate)</li> <li><code>budget</code>: Optional cost controls (see Budget Configuration)</li> </ul> <p>Example:</p> <pre><code>run_settings:\n  output_dir: \"./output\"\n  sqlite_db_path: \"./output/watcher.db\"\n\n  models:\n    - provider: \"openai\"\n      model_name: \"gpt-4o-mini\"\n      env_api_key: \"OPENAI_API_KEY\"\n\n    - provider: \"anthropic\"\n      model_name: \"claude-3-5-haiku-20241022\"\n      env_api_key: \"ANTHROPIC_API_KEY\"\n\n  use_llm_rank_extraction: false\n\n  budget:\n    enabled: true\n    max_per_run_usd: 1.00\n    warn_threshold_usd: 0.50\n</code></pre>"},{"location":"user-guide/configuration/overview/#extraction-settings-optional","title":"Extraction Settings (Optional)","text":"<p>Advanced configuration for brand mention and rank extraction using function calling.</p> <p>Key fields:</p> <ul> <li><code>extraction_model</code>: Dedicated model for extraction (faster, cheaper than main models)</li> <li><code>method</code>: Extraction method (<code>function_calling</code>, <code>regex</code>, or <code>hybrid</code>)</li> <li><code>fallback_to_regex</code>: Fall back to regex if function calling fails</li> <li><code>min_confidence</code>: Minimum confidence threshold (0.0-1.0)</li> </ul> <p>Example:</p> <pre><code>extraction_settings:\n  extraction_model:\n    provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    system_prompt: \"openai/extraction-default\"\n\n  method: \"function_calling\"\n  fallback_to_regex: true\n  min_confidence: 0.7\n</code></pre> <p>When to Use Extraction Settings</p> <p>Use extraction settings when:</p> <ul> <li>Regex extraction misses complex brand mentions</li> <li>You need higher accuracy for ranking positions</li> <li>You want to extract additional structured data</li> </ul> <p>Skip it when:</p> <ul> <li>You're optimizing for cost (regex is free)</li> <li>Your brand names are simple and unambiguous</li> <li>You're running frequent monitoring jobs</li> </ul>"},{"location":"user-guide/configuration/overview/#brands","title":"Brands","text":"<p>Defines which brands to track in LLM responses.</p> <p>Two categories:</p> <ol> <li>mine: Your brand aliases (at least one required)</li> <li>competitors: Competitor brands to monitor</li> </ol> <p>Example:</p> <pre><code>brands:\n  mine:\n    - \"Warmly\"\n    - \"Warmly.io\"\n    - \"Warmly AI\"\n\n  competitors:\n    - \"Instantly\"\n    - \"Lemwarm\"\n    - \"HubSpot\"\n    - \"Apollo.io\"\n    - \"Woodpecker\"\n</code></pre> <p>Brand Alias Best Practices</p> <ul> <li>Include all variations (with/without TLD, with/without product name)</li> <li>Use word-boundary matching to avoid false positives</li> <li>Add common misspellings if relevant</li> <li>Keep list focused (10-20 competitors maximum)</li> </ul> <p>See Brand Configuration for detailed strategies.</p>"},{"location":"user-guide/configuration/overview/#intents","title":"Intents","text":"<p>Questions you want to ask LLMs to test brand visibility.</p> <p>Key fields:</p> <ul> <li><code>id</code>: Unique identifier (alphanumeric, hyphens, underscores)</li> <li><code>prompt</code>: Natural language question to ask</li> <li><code>operations</code>: Optional post-query operations (see Operations)</li> </ul> <p>Example:</p> <pre><code>intents:\n  - id: \"best-email-warmup-tools\"\n    prompt: \"What are the best email warmup tools?\"\n\n  - id: \"email-warmup-comparison\"\n    prompt: \"Compare the top email warmup tools for improving deliverability\"\n\n  - id: \"hubspot-alternatives\"\n    prompt: \"What are the best alternatives to HubSpot for small sales teams?\"\n</code></pre> <p>Intent Prompt Design</p> <p>Good prompts are:</p> <ul> <li>Natural: How a real user would ask</li> <li>Specific: Target a clear use case</li> <li>Buyer-focused: Imply purchase intent</li> <li>Ranking-friendly: Ask for \"best\" or \"top\" tools</li> </ul> <p>Bad prompts:</p> <ul> <li>Too generic: \"Tell me about CRM tools\"</li> <li>No ranking signal: \"What is HubSpot?\"</li> <li>Biased: \"Why is MyBrand better than CompetitorA?\"</li> </ul>"},{"location":"user-guide/configuration/overview/#global-operations-optional","title":"Global Operations (Optional)","text":"<p>Operations that run for every intent across all models.</p> <p>Use cases:</p> <ul> <li>Quality scoring for all LLM responses</li> <li>Sentiment analysis</li> <li>Content gap detection</li> <li>Consistent post-processing</li> </ul> <p>Example:</p> <pre><code>global_operations:\n  - id: \"quality-score\"\n    description: \"Rate LLM response quality\"\n    prompt: |\n      Rate this LLM response on a scale of 1-10 for accuracy and completeness:\n\n      Question: {intent:prompt}\n      Response: {intent:response}\n\n      Provide a single number score (1-10) and brief justification.\n    model: \"gpt-4o-mini\"\n    enabled: true\n</code></pre> <p>Global vs Intent-Specific Operations</p> <p>Use global operations for:</p> <ul> <li>Consistent quality checks</li> <li>Standard metrics across all intents</li> <li>Cost-effective batch analysis</li> </ul> <p>Use intent-specific operations for:</p> <ul> <li>Detailed competitive analysis</li> <li>Context-specific insights</li> <li>Intent-dependent workflows</li> </ul>"},{"location":"user-guide/configuration/overview/#configuration-validation","title":"Configuration Validation","text":"<p>Validate your configuration before running:</p> <pre><code>llm-answer-watcher validate --config watcher.config.yaml\n</code></pre> <p>Common validation errors:</p> <ol> <li>Missing API keys: Environment variable not set</li> <li>Duplicate intent IDs: Intent IDs must be unique</li> <li>Invalid provider: Unsupported provider name</li> <li>Empty brand list: At least one brand in <code>mine</code> required</li> <li>Invalid intent ID: Must be alphanumeric with hyphens/underscores</li> </ol> <p>Validation Output</p> <pre><code>\u2705 Configuration valid\n\u251c\u2500\u2500 3 intents configured\n\u251c\u2500\u2500 2 models configured (OpenAI, Anthropic)\n\u251c\u2500\u2500 2 brands monitored\n\u251c\u2500\u2500 5 competitors tracked\n\u2514\u2500\u2500 Estimated cost: $0.0142 per run\n</code></pre>"},{"location":"user-guide/configuration/overview/#configuration-best-practices","title":"Configuration Best Practices","text":""},{"location":"user-guide/configuration/overview/#1-start-small","title":"1. Start Small","text":"<p>Begin with one model and a few intents:</p> <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"  # Cheapest option\n    env_api_key: \"OPENAI_API_KEY\"\n\nintents:\n  - id: \"primary-intent\"\n    prompt: \"Your most important question\"\n</code></pre>"},{"location":"user-guide/configuration/overview/#2-use-budget-controls","title":"2. Use Budget Controls","text":"<p>Prevent unexpected costs:</p> <pre><code>run_settings:\n  budget:\n    enabled: true\n    max_per_run_usd: 1.00\n    warn_threshold_usd: 0.50\n</code></pre>"},{"location":"user-guide/configuration/overview/#3-keep-brand-lists-focused","title":"3. Keep Brand Lists Focused","text":"<p>Track 10-20 competitors maximum:</p> <pre><code>brands:\n  mine:\n    - \"YourBrand\"      # Exact name\n    - \"YourBrand.io\"   # With TLD\n\n  competitors:\n    # Top 5 direct competitors\n    - \"CompetitorA\"\n    - \"CompetitorB\"\n    # Top 3 category leaders\n    - \"MarketLeader\"\n</code></pre>"},{"location":"user-guide/configuration/overview/#4-design-intent-prompts-carefully","title":"4. Design Intent Prompts Carefully","text":"<p>Ask natural questions with ranking signals:</p> <pre><code>intents:\n  # Good: Natural, specific, implies ranking\n  - id: \"best-crm-for-startups\"\n    prompt: \"What are the best CRM tools for early-stage startups?\"\n\n  # Bad: Generic, no ranking signal\n  - id: \"crm-info\"\n    prompt: \"Tell me about CRM software\"\n</code></pre>"},{"location":"user-guide/configuration/overview/#5-use-system-prompts","title":"5. Use System Prompts","text":"<p>Customize model behavior with system prompts:</p> <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    system_prompt: \"openai/gpt-4-default\"  # Uses default prompt\n</code></pre> <p>System prompts are stored in <code>llm_answer_watcher/system_prompts/{provider}/{prompt_name}.json</code>.</p>"},{"location":"user-guide/configuration/overview/#6-enable-web-search-for-fresh-data","title":"6. Enable Web Search for Fresh Data","text":"<p>Use web search for queries needing current information:</p> <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    tools:\n      - type: \"web_search\"\n    tool_choice: \"auto\"\n</code></pre> <p>Web Search Costs</p> <p>Web search adds \\(10-\\)25 per 1,000 calls depending on the model. See Web Search Configuration.</p>"},{"location":"user-guide/configuration/overview/#7-version-your-config","title":"7. Version Your Config","text":"<p>Track configuration changes with git:</p> <pre><code>git add watcher.config.yaml\ngit commit -m \"feat: add new competitor tracking\"\n</code></pre> <p>This creates an audit trail of what you were monitoring when.</p>"},{"location":"user-guide/configuration/overview/#configuration-examples","title":"Configuration Examples","text":""},{"location":"user-guide/configuration/overview/#production-monitoring","title":"Production Monitoring","text":"<p>Multi-model, budget-controlled, comprehensive tracking:</p> <pre><code>run_settings:\n  output_dir: \"./output\"\n  sqlite_db_path: \"./output/watcher.db\"\n\n  models:\n    - provider: \"openai\"\n      model_name: \"gpt-4o-mini\"\n      env_api_key: \"OPENAI_API_KEY\"\n\n    - provider: \"anthropic\"\n      model_name: \"claude-3-5-haiku-20241022\"\n      env_api_key: \"ANTHROPIC_API_KEY\"\n\n    - provider: \"perplexity\"\n      model_name: \"sonar-pro\"\n      env_api_key: \"PERPLEXITY_API_KEY\"\n\n  budget:\n    enabled: true\n    max_per_run_usd: 5.00\n    warn_threshold_usd: 2.50\n\nextraction_settings:\n  extraction_model:\n    provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n  method: \"function_calling\"\n  fallback_to_regex: true\n  min_confidence: 0.7\n\nbrands:\n  mine:\n    - \"MyBrand\"\n    - \"MyBrand.io\"\n\n  competitors:\n    - \"TopCompetitor\"\n    - \"MainRival\"\n    - \"IndustryLeader\"\n\nintents:\n  - id: \"best-tools-general\"\n    prompt: \"What are the best [category] tools?\"\n\n  - id: \"best-tools-startups\"\n    prompt: \"What are the best [category] tools for startups?\"\n\n  - id: \"best-tools-enterprise\"\n    prompt: \"What are the best [category] tools for enterprises?\"\n</code></pre>"},{"location":"user-guide/configuration/overview/#development-testing","title":"Development Testing","text":"<p>Minimal config for fast iteration:</p> <pre><code>run_settings:\n  output_dir: \"./output\"\n  sqlite_db_path: \"./output/watcher.db\"\n\n  models:\n    - provider: \"openai\"\n      model_name: \"gpt-4o-mini\"\n      env_api_key: \"OPENAI_API_KEY\"\n\n  budget:\n    enabled: true\n    max_per_run_usd: 0.10\n\nbrands:\n  mine:\n    - \"TestBrand\"\n\n  competitors:\n    - \"CompetitorA\"\n\nintents:\n  - id: \"test-intent\"\n    prompt: \"What are the best tools for testing?\"\n</code></pre>"},{"location":"user-guide/configuration/overview/#cicd-regression-testing","title":"CI/CD Regression Testing","text":"<p>Automated monitoring with strict controls:</p> <pre><code>run_settings:\n  output_dir: \"./ci-output\"\n  sqlite_db_path: \"./ci-output/watcher.db\"\n\n  models:\n    - provider: \"openai\"\n      model_name: \"gpt-4o-mini\"\n      env_api_key: \"OPENAI_API_KEY\"\n\n  budget:\n    enabled: true\n    max_per_run_usd: 0.50\n\nbrands:\n  mine:\n    - \"MyBrand\"\n\n  competitors:\n    - \"TopCompetitor\"\n\nintents:\n  - id: \"regression-test\"\n    prompt: \"What are the best [category] tools?\"\n</code></pre>"},{"location":"user-guide/configuration/overview/#configuration-file-location","title":"Configuration File Location","text":"<p>LLM Answer Watcher looks for configuration in these locations (in order):</p> <ol> <li>Path specified with <code>--config</code> flag</li> <li><code>watcher.config.yaml</code> in current directory</li> <li><code>~/.config/llm-answer-watcher/config.yaml</code></li> </ol> <p>Best practice: Keep config files in your project directory and specify explicitly:</p> <pre><code>llm-answer-watcher run --config watcher.config.yaml\n</code></pre>"},{"location":"user-guide/configuration/overview/#environment-variables","title":"Environment Variables","text":""},{"location":"user-guide/configuration/overview/#api-keys","title":"API Keys","text":"<p>All API keys are loaded from environment variables for security:</p> <pre><code># OpenAI\nexport OPENAI_API_KEY=sk-your-key-here\n\n# Anthropic\nexport ANTHROPIC_API_KEY=sk-ant-your-key-here\n\n# Mistral\nexport MISTRAL_API_KEY=your-mistral-key-here\n\n# X.AI Grok\nexport XAI_API_KEY=xai-your-key-here\n\n# Google Gemini\nexport GOOGLE_API_KEY=AIza-your-key-here\n\n# Perplexity\nexport PERPLEXITY_API_KEY=pplx-your-key-here\n</code></pre>"},{"location":"user-guide/configuration/overview/#configuration-overrides","title":"Configuration Overrides","text":"<p>Override config values with environment variables:</p> <pre><code># Override output directory\nexport LLM_WATCHER_OUTPUT_DIR=\"./custom-output\"\n\n# Override database path\nexport LLM_WATCHER_DB_PATH=\"./custom.db\"\n\n# Disable budget checks\nexport LLM_WATCHER_BUDGET_ENABLED=false\n</code></pre>"},{"location":"user-guide/configuration/overview/#security-considerations","title":"Security Considerations","text":""},{"location":"user-guide/configuration/overview/#never-commit-api-keys","title":"Never Commit API Keys","text":"<p>Add <code>.env</code> files to <code>.gitignore</code>:</p> <pre><code># .gitignore\n.env\n.env.local\n*.env\nwatcher.config.local.yaml\n</code></pre>"},{"location":"user-guide/configuration/overview/#use-environment-specific-configs","title":"Use Environment-Specific Configs","text":"<p>Create separate configs for each environment:</p> <pre><code>configs/\n\u251c\u2500\u2500 watcher.config.dev.yaml      # Development\n\u251c\u2500\u2500 watcher.config.staging.yaml  # Staging\n\u251c\u2500\u2500 watcher.config.prod.yaml     # Production\n</code></pre> <p>Load the appropriate config:</p> <pre><code>llm-answer-watcher run --config configs/watcher.config.prod.yaml\n</code></pre>"},{"location":"user-guide/configuration/overview/#rotate-api-keys-regularly","title":"Rotate API Keys Regularly","text":"<p>Update API keys in your environment:</p> <pre><code># Update key\nexport OPENAI_API_KEY=sk-new-key-here\n\n# Verify it works\nllm-answer-watcher validate --config watcher.config.yaml\n</code></pre>"},{"location":"user-guide/configuration/overview/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/configuration/overview/#configuration-validation-fails","title":"Configuration Validation Fails","text":"<p>Problem: <code>Configuration error: Invalid YAML syntax</code></p> <p>Solution: Check YAML syntax with a validator:</p> <pre><code>python -c \"import yaml; yaml.safe_load(open('watcher.config.yaml'))\"\n</code></pre> <p>Common YAML errors:</p> <ul> <li>Inconsistent indentation (use 2 spaces)</li> <li>Missing colons after keys</li> <li>Unquoted strings with special characters</li> <li>Mixing tabs and spaces</li> </ul> <p>Problem: <code>API key not found: OPENAI_API_KEY</code></p> <p>Solution: Set the environment variable:</p> <pre><code>export OPENAI_API_KEY=sk-your-key-here\n</code></pre> <p>Verify it's set:</p> <pre><code>echo $OPENAI_API_KEY\n</code></pre> <p>Problem: <code>Duplicate intent IDs found: best-tools</code></p> <p>Solution: Make each intent ID unique:</p> <pre><code>intents:\n  - id: \"best-tools-general\"      # Changed from \"best-tools\"\n    prompt: \"What are the best tools?\"\n\n  - id: \"best-tools-startups\"     # Changed from \"best-tools\"\n    prompt: \"What are the best tools for startups?\"\n</code></pre>"},{"location":"user-guide/configuration/overview/#output-directory-issues","title":"Output Directory Issues","text":"<p>Problem: <code>Cannot write to output directory: Permission denied</code></p> <p>Solution: Check directory permissions:</p> <pre><code>mkdir -p ./output\nchmod 755 ./output\n</code></pre> <p>Or change to a directory you own:</p> <pre><code>run_settings:\n  output_dir: \"~/llm-watcher-output\"\n</code></pre> <p>Problem: <code>SQLite database is locked</code></p> <p>Solution: Ensure no other processes are using the database:</p> <pre><code># Check for locks\nlsof ./output/watcher.db\n\n# Kill blocking processes if safe\nkill -9 &lt;PID&gt;\n</code></pre> <p>Or use a separate database:</p> <pre><code>run_settings:\n  sqlite_db_path: \"./output/watcher-$(date +%s).db\"\n</code></pre>"},{"location":"user-guide/configuration/overview/#model-configuration-issues","title":"Model Configuration Issues","text":"<p>Problem: <code>Unsupported provider: openai-gpt4</code></p> <p>Solution: Use correct provider names:</p> <pre><code># \u274c Wrong\nprovider: \"openai-gpt4\"\n\n# \u2705 Correct\nprovider: \"openai\"\nmodel_name: \"gpt-4o-mini\"\n</code></pre> <p>Supported providers: <code>openai</code>, <code>anthropic</code>, <code>mistral</code>, <code>grok</code>, <code>google</code>, <code>perplexity</code></p> <p>Problem: <code>Model not found: gpt-4o-mini-turbo</code></p> <p>Solution: Use valid model names:</p> <pre><code># \u274c Wrong (doesn't exist)\nmodel_name: \"gpt-4o-mini-turbo\"\n\n# \u2705 Correct\nmodel_name: \"gpt-4o-mini\"\n</code></pre> <p>Check Model Configuration for valid model names.</p>"},{"location":"user-guide/configuration/overview/#next-steps","title":"Next Steps","text":"<p>Now that you understand the configuration structure, dive into specific sections:</p> <ul> <li>Model Configuration: Choose the right models for your use case</li> <li>Brand Configuration: Optimize brand detection strategies</li> <li>Intent Configuration: Design effective prompts</li> <li>Budget Configuration: Control costs and prevent overruns</li> <li>Web Search Configuration: Enable real-time information retrieval</li> <li>Operations Configuration: Automate post-query analysis</li> </ul> <p>Or jump to usage guides:</p> <ul> <li>CLI Commands: Run your first monitoring job</li> <li>Output Modes: Understand output formats</li> <li>Automation: Set up scheduled monitoring</li> </ul>"},{"location":"user-guide/configuration/web-search/","title":"Web Search Configuration","text":"<p>Web search enables LLMs to access real-time information from the web, providing current data beyond their training cutoff dates. This is crucial for monitoring brand visibility in fresh, up-to-date LLM responses.</p>"},{"location":"user-guide/configuration/web-search/#why-use-web-search","title":"Why Use Web Search?","text":""},{"location":"user-guide/configuration/web-search/#benefits","title":"Benefits","text":"<p>Fresh Data: Access information after LLM training cutoff</p> <ul> <li>Track recent product launches</li> <li>Monitor current competitive landscape</li> <li>Detect real-time ranking changes</li> <li>Capture latest industry trends</li> </ul> <p>Accurate Information: Grounded in current web sources</p> <ul> <li>Real-time pricing and features</li> <li>Current company positioning</li> <li>Latest product updates</li> <li>Active competitor status</li> </ul> <p>Citations: Transparent source attribution (Perplexity)</p> <ul> <li>See exactly which sources LLMs used</li> <li>Verify information accuracy</li> <li>Understand ranking drivers</li> <li>Track source patterns</li> </ul>"},{"location":"user-guide/configuration/web-search/#trade-offs","title":"Trade-offs","text":"<p>Higher Costs: Web search adds significant costs</p> <ul> <li>OpenAI: +\\(10-\\)25 per 1,000 calls</li> <li>Perplexity: +\\(0.005-\\)0.03 per request</li> <li>10-30x cost increase vs. base queries</li> </ul> <p>Slower Responses: Web search takes longer</p> <ul> <li>Base query: ~1-2 seconds</li> <li>With web search: ~3-10 seconds</li> <li>May impact automation pipelines</li> </ul> <p>Variability: Results can change frequently</p> <ul> <li>Web content changes constantly</li> <li>Less reproducible than static responses</li> <li>Harder to track trends</li> </ul>"},{"location":"user-guide/configuration/web-search/#supported-providers","title":"Supported Providers","text":""},{"location":"user-guide/configuration/web-search/#openai-web-search","title":"OpenAI Web Search","text":"<p>OpenAI offers web search through the Responses API with the <code>web_search</code> tool.</p> <p>Configuration:</p> <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    tools:\n      - type: \"web_search\"\n    tool_choice: \"auto\"\n</code></pre> <p>How it works:</p> <ol> <li>LLM receives user prompt</li> <li>Decides whether to use web search (if <code>tool_choice: auto</code>)</li> <li>Searches the web if needed</li> <li>Incorporates search results into response</li> <li>Returns answer with web context</li> </ol> <p>Pricing (per 1,000 calls):</p> Model Tier Cost Content Tokens Standard (all models) $10 @ model rate gpt-4o-mini, gpt-4.1-mini $10 Fixed 8k tokens Preview reasoning (o1, o3) $10 @ model rate Preview non-reasoning $25 FREE"},{"location":"user-guide/configuration/web-search/#perplexity-native-web-search","title":"Perplexity (Native Web Search)","text":"<p>Perplexity models have built-in web search - no configuration needed.</p> <p>Configuration:</p> <pre><code>models:\n  - provider: \"perplexity\"\n    model_name: \"sonar-pro\"\n    env_api_key: \"PERPLEXITY_API_KEY\"\n</code></pre> <p>How it works:</p> <ol> <li>Every query automatically searches the web</li> <li>LLM synthesizes answer from sources</li> <li>Returns response with citations</li> <li>Provides source URLs for verification</li> </ol> <p>Models:</p> <ul> <li><code>sonar</code>: Fast, web-grounded (\\(1/\\)1 per 1M tokens + request fees)</li> <li><code>sonar-pro</code>: High-quality grounded (\\(3/\\)15 per 1M tokens + request fees)</li> <li><code>sonar-reasoning</code>: Enhanced reasoning (\\(1/\\)5 per 1M tokens + request fees)</li> <li><code>sonar-deep-research</code>: In-depth analysis (\\(3/\\)15 per 1M tokens + request fees)</li> </ul> <p>Pricing: Token costs + request fees (~\\(0.005-\\)0.03 per request)</p> <p>Perplexity Request Fees</p> <p>Request fees are not yet included in cost estimates. Budget accordingly.</p>"},{"location":"user-guide/configuration/web-search/#google-search-grounding","title":"Google Search Grounding","text":"<p>Google Gemini models support Google Search grounding, which enables the LLM to search the web and ground responses in current information.</p> <p>Configuration:</p> <pre><code>models:\n  - provider: \"google\"\n    model_name: \"gemini-2.5-flash\"\n    env_api_key: \"GEMINI_API_KEY\"\n    system_prompt: \"google/gemini-grounding\"  # Recommended\n    tools:\n      - google_search: {}  # Enable Google Search\n</code></pre> <p>How it works:</p> <ol> <li>LLM receives user prompt</li> <li>Gemini automatically decides if search is needed</li> <li>Performs Google Search if beneficial</li> <li>Grounds response in search results</li> <li>Returns answer with grounding metadata</li> </ol> <p>Models:</p> <ul> <li><code>gemini-2.0-flash-lite</code>: Not supported (no grounding)</li> <li><code>gemini-2.0-flash-exp</code>: Supported (experimental)</li> <li><code>gemini-2.5-flash</code>: Supported (best for grounding)</li> <li><code>gemini-2.5-flash-lite</code>: Not supported</li> <li><code>gemini-2.5-pro</code>: Supported (highest quality)</li> </ul> <p>Pricing: Base model token costs (no additional fees for grounding)</p> <p>Configuration Format Difference</p> <p>Google uses <code>google_search: {}</code> (dictionary format) while OpenAI uses <code>type: \"web_search\"</code> (typed format). This reflects different provider API specifications. See detailed configuration below.</p>"},{"location":"user-guide/configuration/web-search/#openai-web-search-configuration","title":"OpenAI Web Search Configuration","text":""},{"location":"user-guide/configuration/web-search/#basic-configuration","title":"Basic Configuration","text":"<p>Enable web search with automatic activation:</p> <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    tools:\n      - type: \"web_search\"\n    tool_choice: \"auto\"  # Let model decide\n</code></pre>"},{"location":"user-guide/configuration/web-search/#tool-choice-options","title":"Tool Choice Options","text":"<p>Control when web search is used:</p> <p><code>auto</code> (Recommended): Model decides when to search</p> <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    tools:\n      - type: \"web_search\"\n    tool_choice: \"auto\"\n</code></pre> <p>Use when: You want LLM to determine if fresh data is needed.</p> <p><code>required</code>: Force web search for every query</p> <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    tools:\n      - type: \"web_search\"\n    tool_choice: \"required\"\n</code></pre> <p>Use when: You always want current information.</p> <p>Warning: Significantly increases costs (every query uses web search).</p> <p><code>none</code>: Disable web search for specific queries</p> <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    # No tools specified - web search disabled\n</code></pre> <p>Use when: Training data is sufficient, cost optimization priority.</p>"},{"location":"user-guide/configuration/web-search/#comparing-with-and-without-web-search","title":"Comparing With and Without Web Search","text":"<p>Test impact of web search:</p> <pre><code>models:\n  # With web search\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    tools:\n      - type: \"web_search\"\n    tool_choice: \"auto\"\n\n  # Without web search (control)\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    # No tools\n</code></pre> <p>Compare results to see web search impact on brand visibility.</p>"},{"location":"user-guide/configuration/web-search/#web-search-metadata","title":"Web Search Metadata","text":"<p>LLM Answer Watcher tracks web search usage:</p> <pre><code>{\n  \"intent_id\": \"best-email-tools\",\n  \"model_provider\": \"openai\",\n  \"model_name\": \"gpt-4o-mini\",\n  \"answer_text\": \"The best email warmup tools are...\",\n  \"web_search_used\": true,\n  \"web_search_count\": 3,\n  \"web_search_results\": [\n    {\n      \"url\": \"https://example.com/best-email-tools\",\n      \"title\": \"Top Email Warmup Tools 2025\",\n      \"snippet\": \"...\"\n    }\n  ],\n  \"usage_meta\": {\n    \"prompt_tokens\": 150,\n    \"completion_tokens\": 520,\n    \"web_search_tokens\": 8000\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration/web-search/#perplexity-configuration","title":"Perplexity Configuration","text":""},{"location":"user-guide/configuration/web-search/#basic-configuration_1","title":"Basic Configuration","text":"<p>Use Perplexity for automatic web grounding:</p> <pre><code>models:\n  - provider: \"perplexity\"\n    model_name: \"sonar-pro\"\n    env_api_key: \"PERPLEXITY_API_KEY\"\n</code></pre> <p>No additional configuration needed - web search is automatic.</p>"},{"location":"user-guide/configuration/web-search/#perplexity-model-selection","title":"Perplexity Model Selection","text":"<p>Choose model based on use case:</p> <p><code>sonar</code>: Fast, cost-effective</p> <pre><code>models:\n  - provider: \"perplexity\"\n    model_name: \"sonar\"\n    env_api_key: \"PERPLEXITY_API_KEY\"\n</code></pre> <ul> <li>Cost: \\(1/\\)1 per 1M tokens + ~$0.005 per request</li> <li>Speed: ~2-4 seconds per query</li> <li>Use when: Daily monitoring, high-volume queries</li> </ul> <p><code>sonar-pro</code>: High-quality grounded answers</p> <pre><code>models:\n  - provider: \"perplexity\"\n    model_name: \"sonar-pro\"\n    env_api_key: \"PERPLEXITY_API_KEY\"\n</code></pre> <ul> <li>Cost: \\(3/\\)15 per 1M tokens + ~$0.01 per request</li> <li>Speed: ~3-6 seconds per query</li> <li>Use when: Weekly reports, competitive analysis</li> </ul> <p><code>sonar-reasoning</code>: Enhanced reasoning with web search</p> <pre><code>models:\n  - provider: \"perplexity\"\n    model_name: \"sonar-reasoning\"\n    env_api_key: \"PERPLEXITY_API_KEY\"\n</code></pre> <ul> <li>Cost: \\(1/\\)5 per 1M tokens + ~$0.015 per request</li> <li>Speed: ~4-8 seconds per query</li> <li>Use when: Complex queries, deep analysis</li> </ul> <p><code>sonar-deep-research</code>: Comprehensive research mode</p> <pre><code>models:\n  - provider: \"perplexity\"\n    model_name: \"sonar-deep-research\"\n    env_api_key: \"PERPLEXITY_API_KEY\"\n</code></pre> <ul> <li>Cost: \\(3/\\)15 per 1M tokens + ~\\(0.02-\\)0.03 per request</li> <li>Speed: ~8-15 seconds per query</li> <li>Use when: Monthly executive reports, thorough research</li> </ul>"},{"location":"user-guide/configuration/web-search/#perplexity-citations","title":"Perplexity Citations","text":"<p>Perplexity provides source citations:</p> <pre><code>{\n  \"intent_id\": \"best-email-tools\",\n  \"model_provider\": \"perplexity\",\n  \"model_name\": \"sonar-pro\",\n  \"answer_text\": \"The best email warmup tools are...\",\n  \"citations\": [\n    {\n      \"index\": 1,\n      \"url\": \"https://www.g2.com/categories/email-warmup\",\n      \"title\": \"Best Email Warmup Software 2025\",\n      \"used_in_response\": true\n    },\n    {\n      \"index\": 2,\n      \"url\": \"https://blog.competitor.com/warmup-guide\",\n      \"title\": \"Email Warmup Best Practices\",\n      \"used_in_response\": true\n    }\n  ]\n}\n</code></pre> <p>Citation Analysis</p> <p>Track which sources influence LLM recommendations:</p> <ul> <li>Identify key industry publications</li> <li>Monitor competitor content</li> <li>Find content opportunities</li> <li>Track source diversity</li> </ul>"},{"location":"user-guide/configuration/web-search/#google-search-grounding-configuration","title":"Google Search Grounding Configuration","text":""},{"location":"user-guide/configuration/web-search/#basic-configuration_2","title":"Basic Configuration","text":"<p>Enable Google Search grounding for Gemini models:</p> <pre><code>models:\n  - provider: \"google\"\n    model_name: \"gemini-2.5-flash\"\n    env_api_key: \"GEMINI_API_KEY\"\n    system_prompt: \"google/gemini-grounding\"\n    tools:\n      - google_search: {}\n</code></pre> <p>Key configuration points:</p> <ul> <li><code>model_name</code>: Must be a grounding-capable model (see supported models below)</li> <li><code>system_prompt</code>: Use <code>\"google/gemini-grounding\"</code> for optimized grounding behavior</li> <li><code>tools</code>: Use <code>google_search: {}</code> format (Google API specification)</li> </ul>"},{"location":"user-guide/configuration/web-search/#configuration-format","title":"Configuration Format","text":"<p>Google uses a different tools format than OpenAI:</p> <p>Google format (dictionary with tool name as key): <pre><code>tools:\n  - google_search: {}\n</code></pre></p> <p>OpenAI format (dictionary with <code>type</code> field): <pre><code>tools:\n  - type: \"web_search\"\ntool_choice: \"auto\"\n</code></pre></p> <p>Why the difference?</p> <ul> <li>Each provider has different API specifications</li> <li>OpenAI uses typed tool specification with <code>tool_choice</code> control</li> <li>Google uses named tool objects with automatic decision-making</li> <li>The config does direct passthrough to each provider's API</li> </ul> <p>No Tool Choice</p> <p>Google Gemini automatically decides when to use Google Search based on the query. There's no <code>tool_choice</code> parameter - the model intelligently determines when grounding would improve the response.</p>"},{"location":"user-guide/configuration/web-search/#supported-models","title":"Supported Models","text":"<p>Not all Gemini models support Google Search grounding:</p> Model Grounding Support Best For <code>gemini-2.0-flash-lite</code> \u274c No Fast, non-grounded queries <code>gemini-2.0-flash-exp</code> \u26a0\ufe0f Experimental Testing new features <code>gemini-2.5-flash</code> \u2705 Yes Recommended - balanced speed/quality <code>gemini-2.5-flash-lite</code> \u274c No Fast, non-grounded queries <code>gemini-2.5-pro</code> \u2705 Yes Highest quality grounding <p>Recommendation: Use <code>gemini-2.5-flash</code> for production. It provides excellent grounding quality at reasonable cost.</p>"},{"location":"user-guide/configuration/web-search/#system-prompt-optimization","title":"System Prompt Optimization","text":"<p>Use the specialized <code>google/gemini-grounding</code> system prompt:</p> <pre><code>models:\n  - provider: \"google\"\n    model_name: \"gemini-2.5-flash\"\n    env_api_key: \"GEMINI_API_KEY\"\n    system_prompt: \"google/gemini-grounding\"  # Optimized for grounding\n    tools:\n      - google_search: {}\n</code></pre> <p>What it does:</p> <ul> <li>Instructs Gemini to use Google Search when beneficial</li> <li>Emphasizes grounding responses in search results</li> <li>Requests comprehensive source coverage</li> <li>Improves answer quality for brand monitoring</li> </ul> <p>Default system prompt (<code>google/default.json</code>) also works but is less optimized for web search use cases.</p>"},{"location":"user-guide/configuration/web-search/#configuration-examples","title":"Configuration Examples","text":"<p>With grounding (recommended for brand monitoring):</p> <pre><code>models:\n  - provider: \"google\"\n    model_name: \"gemini-2.5-flash\"\n    env_api_key: \"GEMINI_API_KEY\"\n    system_prompt: \"google/gemini-grounding\"\n    tools:\n      - google_search: {}\n\nintents:\n  - id: \"email-warmup-tools\"\n    prompt: \"What are the best email warmup tools in 2025?\"\n</code></pre> <p>Without grounding (faster, uses only training data):</p> <pre><code>models:\n  - provider: \"google\"\n    model_name: \"gemini-2.5-flash-lite\"\n    env_api_key: \"GEMINI_API_KEY\"\n    # No tools or system_prompt specified\n\nintents:\n  - id: \"email-warmup-tools\"\n    prompt: \"What are the best email warmup tools?\"\n</code></pre>"},{"location":"user-guide/configuration/web-search/#grounding-metadata","title":"Grounding Metadata","text":"<p>When Google Search is used, the response includes grounding metadata:</p> <pre><code>{\n  \"intent_id\": \"email-warmup-tools\",\n  \"model_provider\": \"google\",\n  \"model_name\": \"gemini-2.5-flash\",\n  \"answer_text\": \"Based on current research, the best email warmup tools are...\",\n  \"web_search_results\": {\n    \"web_search_queries\": [\n      \"best email warmup tools 2025\",\n      \"email warmup service comparison\"\n    ],\n    \"grounding_chunks\": [\n      {\n        \"web_source\": \"https://www.g2.com/categories/email-warmup\",\n        \"retrieved_context\": \"Top-rated email warmup tools include...\"\n      }\n    ],\n    \"grounding_supports\": [\n      {\n        \"segment\": {\n          \"start_index\": 150,\n          \"end_index\": 200,\n          \"text\": \"Warmly is a leading email warmup solution\"\n        },\n        \"grounding_chunk_indices\": [0, 2],\n        \"confidence_scores\": [0.95, 0.88]\n      }\n    ]\n  },\n  \"web_search_count\": 2\n}\n</code></pre> <p>Key fields:</p> <ul> <li><code>web_search_queries</code>: Google Search queries Gemini performed</li> <li><code>grounding_chunks</code>: Source URLs and retrieved context</li> <li><code>grounding_supports</code>: Which text segments were grounded in which sources</li> <li><code>confidence_scores</code>: How confident Gemini is in the grounding (0.0-1.0)</li> </ul>"},{"location":"user-guide/configuration/web-search/#pricing","title":"Pricing","text":"<p>Good news: Google Search grounding has no additional per-request fees.</p> <p>You only pay standard token costs:</p> Model Input Cost Output Cost <code>gemini-2.5-flash</code> $0.04 / 1M tokens $0.12 / 1M tokens <code>gemini-2.5-pro</code> $0.60 / 1M tokens $1.80 / 1M tokens <p>Example cost (email warmup query with grounding):</p> <pre><code>Query: 100 tokens input\nResponse: 300 tokens output (with grounding context)\n\ngemini-2.5-flash cost:\n= (100 \u00d7 $0.04/1M) + (300 \u00d7 $0.12/1M)\n= $0.000004 + $0.000036\n= $0.00004 per query\n</code></pre> <p>vs. OpenAI with web search: <pre><code>OpenAI gpt-4o-mini with web_search:\n= $0.0116 per query (~290x more expensive)\n</code></pre></p> <p>Cost Advantage</p> <p>Google Search grounding is significantly cheaper than OpenAI web search for high-volume monitoring. Grounding tokens are included in base pricing.</p>"},{"location":"user-guide/configuration/web-search/#when-to-use-google-search-grounding","title":"When to Use Google Search Grounding","text":"<p>Use Google Search Grounding when:</p> <ul> <li>\u2705 You need current, real-time information</li> <li>\u2705 You want Google's search quality and coverage</li> <li>\u2705 You're running high-volume monitoring (cost-effective)</li> <li>\u2705 You want automatic search decision-making</li> <li>\u2705 You need grounding metadata with source attribution</li> </ul> <p>Use OpenAI web search when:</p> <ul> <li>\u2705 You need explicit <code>tool_choice</code> control (force or disable search)</li> <li>\u2705 You prefer OpenAI's LLM reasoning quality</li> <li>\u2705 You're already invested in OpenAI ecosystem</li> </ul> <p>Use Perplexity when:</p> <ul> <li>\u2705 You need explicit source citations with URLs</li> <li>\u2705 You want always-on web search (no configuration)</li> <li>\u2705 You prefer Perplexity's citation format</li> </ul>"},{"location":"user-guide/configuration/web-search/#complete-example-configuration","title":"Complete Example Configuration","text":"<p>Multi-provider comparison with side-by-side grounding:</p> <pre><code>run_settings:\n  output_dir: \"./output\"\n  sqlite_db_path: \"./output/watcher.db\"\n\n  models:\n    # Google with grounding (cost-effective, automatic)\n    - provider: \"google\"\n      model_name: \"gemini-2.5-flash\"\n      env_api_key: \"GEMINI_API_KEY\"\n      system_prompt: \"google/gemini-grounding\"\n      tools:\n        - google_search: {}\n\n    # OpenAI with controlled web search\n    - provider: \"openai\"\n      model_name: \"gpt-4o-mini\"\n      env_api_key: \"OPENAI_API_KEY\"\n      tools:\n        - type: \"web_search\"\n      tool_choice: \"auto\"\n\n    # Perplexity with always-on citations\n    - provider: \"perplexity\"\n      model_name: \"sonar-pro\"\n      env_api_key: \"PERPLEXITY_API_KEY\"\n\nbrands:\n  mine:\n    - \"Warmly\"\n    - \"Lemlist\"\n  competitors:\n    - \"HubSpot\"\n    - \"Instantly\"\n\nintents:\n  - id: \"best-email-tools-2025\"\n    prompt: \"What are the best email warmup tools in 2025?\"\n</code></pre> <p>This configuration enables:</p> <ul> <li>Google: Automatic grounding with lowest cost</li> <li>OpenAI: LLM-controlled web search with reasoning</li> <li>Perplexity: Always-on search with explicit citations</li> </ul> <p>Compare results across all three to understand: - How each provider uses web search - Cost vs. quality trade-offs - Grounding vs. citation differences</p>"},{"location":"user-guide/configuration/web-search/#cost-management-for-web-search","title":"Cost Management for Web Search","text":""},{"location":"user-guide/configuration/web-search/#web-search-cost-breakdown","title":"Web Search Cost Breakdown","text":"<p>OpenAI gpt-4o-mini with web search:</p> <pre><code>Base query: $0.0004 (tokens only)\n+ Web search call: $0.01 (per 1k calls)\n+ Web search content: $0.0012 (8k tokens @ $0.15/1M)\n= Total: ~$0.0116 per query\n</code></pre> <p>Perplexity sonar-pro:</p> <pre><code>Base tokens: $0.0050 (500 output tokens @ $3/$15 per 1M)\n+ Request fee: $0.01 (varies by complexity)\n= Total: ~$0.015 per query\n</code></pre>"},{"location":"user-guide/configuration/web-search/#budget-configuration-for-web-search","title":"Budget Configuration for Web Search","text":"<p>Adjust budgets to account for higher costs:</p> <pre><code>run_settings:\n  # Without web search\n  budget:\n    max_per_run_usd: 0.50\n\n  # With web search (10-30x higher)\n  budget:\n    max_per_run_usd: 5.00\n</code></pre> <p>Example calculation:</p> <ul> <li>3 intents \u00d7 2 models with web search = 6 queries</li> <li>~$0.015 per query</li> <li>Total: $0.09 per run</li> <li>Recommended budget: $0.50 (5x safety margin)</li> </ul>"},{"location":"user-guide/configuration/web-search/#optimizing-web-search-costs","title":"Optimizing Web Search Costs","text":"<p>1. Use <code>auto</code> tool choice:</p> <pre><code>tools:\n  - type: \"web_search\"\ntool_choice: \"auto\"  # Only search when needed\n</code></pre> <p>Model only uses web search when beneficial, reducing unnecessary searches.</p> <p>2. Mix web and non-web models:</p> <pre><code>models:\n  # Web-grounded for fresh data\n  - provider: \"perplexity\"\n    model_name: \"sonar-pro\"\n    env_api_key: \"PERPLEXITY_API_KEY\"\n\n  # Base model for comparison\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    # No web search\n</code></pre> <p>Compare web vs. non-web responses to validate web search value.</p> <p>3. Use web search selectively:</p> <pre><code>intents:\n  # Fresh data needed\n  - id: \"current-best-tools\"\n    prompt: \"What are the best email tools in 2025?\"\n    # Use web search models for this intent\n\n  # Historical query\n  - id: \"email-warmup-concept\"\n    prompt: \"What is email warmup?\"\n    # No web search needed\n</code></pre> <p>Separate configs for different intent types.</p> <p>4. Track web search usage:</p> <pre><code>-- Web search usage rate\nSELECT\n    model_name,\n    COUNT(*) as total_queries,\n    SUM(web_search_used) as web_searches,\n    (SUM(web_search_used) * 100.0 / COUNT(*)) as usage_rate,\n    AVG(estimated_cost_usd) as avg_cost\nFROM answers_raw\nWHERE model_provider = 'openai'\nGROUP BY model_name;\n</code></pre> <p>Optimize based on actual usage patterns.</p>"},{"location":"user-guide/configuration/web-search/#use-cases-for-web-search","title":"Use Cases for Web Search","text":""},{"location":"user-guide/configuration/web-search/#1-recent-product-launches","title":"1. Recent Product Launches","text":"<p>Track brand visibility after launches:</p> <pre><code>intents:\n  - id: \"best-tools-2025\"\n    prompt: \"What are the best email warmup tools launched in 2025?\"\n\nmodels:\n  - provider: \"perplexity\"\n    model_name: \"sonar-pro\"\n    env_api_key: \"PERPLEXITY_API_KEY\"\n</code></pre> <p>Web search ensures LLM knows about recent launches.</p>"},{"location":"user-guide/configuration/web-search/#2-current-competitive-landscape","title":"2. Current Competitive Landscape","text":"<p>Monitor live market positioning:</p> <pre><code>intents:\n  - id: \"current-market-leaders\"\n    prompt: \"Who are the current market leaders in email warmup?\"\n\nmodels:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    tools:\n      - type: \"web_search\"\n    tool_choice: \"required\"  # Always search\n</code></pre>"},{"location":"user-guide/configuration/web-search/#3-pricing-and-features","title":"3. Pricing and Features","text":"<p>Track current pricing mentions:</p> <pre><code>intents:\n  - id: \"pricing-comparison\"\n    prompt: \"Compare pricing for email warmup tools in 2025\"\n\nmodels:\n  - provider: \"perplexity\"\n    model_name: \"sonar-pro\"\n    env_api_key: \"PERPLEXITY_API_KEY\"\n</code></pre>"},{"location":"user-guide/configuration/web-search/#4-news-and-events","title":"4. News and Events","text":"<p>Monitor impact of news on brand visibility:</p> <pre><code>intents:\n  - id: \"post-acquisition\"\n    prompt: \"What are the best email tools after HubSpot's recent acquisition?\"\n\nmodels:\n  - provider: \"perplexity\"\n    model_name: \"sonar-reasoning\"\n    env_api_key: \"PERPLEXITY_API_KEY\"\n</code></pre>"},{"location":"user-guide/configuration/web-search/#5-trend-analysis","title":"5. Trend Analysis","text":"<p>Track emerging trends:</p> <pre><code>intents:\n  - id: \"ai-email-tools\"\n    prompt: \"What are the best AI-powered email warmup tools?\"\n\nmodels:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    tools:\n      - type: \"web_search\"\n    tool_choice: \"auto\"\n</code></pre>"},{"location":"user-guide/configuration/web-search/#analyzing-web-search-results","title":"Analyzing Web Search Results","text":""},{"location":"user-guide/configuration/web-search/#web-search-metadata_1","title":"Web Search Metadata","text":"<p>Check if web search was used:</p> <pre><code>import json\n\nwith open(\"output/2025-11-01T08-00-00Z/intent_best-tools_raw_openai_gpt-4o-mini.json\") as f:\n    data = json.load(f)\n    print(f\"Web search used: {data.get('web_search_used')}\")\n    print(f\"Searches performed: {data.get('web_search_count')}\")\n</code></pre>"},{"location":"user-guide/configuration/web-search/#citation-analysis-perplexity","title":"Citation Analysis (Perplexity)","text":"<p>Extract and analyze citations:</p> <pre><code>import json\n\nwith open(\"output/2025-11-01T08-00-00Z/intent_best-tools_raw_perplexity_sonar-pro.json\") as f:\n    data = json.load(f)\n    for citation in data.get('citations', []):\n        print(f\"{citation['index']}: {citation['title']}\")\n        print(f\"   {citation['url']}\\n\")\n</code></pre>"},{"location":"user-guide/configuration/web-search/#source-patterns","title":"Source Patterns","text":"<p>Track which sources LLMs cite:</p> <pre><code>-- Citation frequency (future feature)\nSELECT\n    citation_domain,\n    COUNT(*) as citation_count,\n    COUNT(DISTINCT intent_id) as intents_cited_in\nFROM citations\nGROUP BY citation_domain\nORDER BY citation_count DESC\nLIMIT 10;\n</code></pre> <p>Citation Tracking</p> <p>Full citation tracking is planned for a future release. Currently, citations are stored in JSON artifacts.</p>"},{"location":"user-guide/configuration/web-search/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/configuration/web-search/#1-test-with-and-without-web-search","title":"1. Test With and Without Web Search","text":"<p>Compare to measure impact:</p> <pre><code>models:\n  # Baseline (no web search)\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n\n  # Test (with web search)\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    tools:\n      - type: \"web_search\"\n    tool_choice: \"auto\"\n</code></pre>"},{"location":"user-guide/configuration/web-search/#2-use-auto-tool-choice","title":"2. Use Auto Tool Choice","text":"<p>Let model decide when to search:</p> <pre><code>tools:\n  - type: \"web_search\"\ntool_choice: \"auto\"  # More cost-effective\n</code></pre>"},{"location":"user-guide/configuration/web-search/#3-budget-appropriately","title":"3. Budget Appropriately","text":"<p>Account for 10-30x cost increase:</p> <pre><code>budget:\n  max_per_run_usd: 5.00  # vs. $0.50 without web search\n</code></pre>"},{"location":"user-guide/configuration/web-search/#4-use-for-time-sensitive-queries","title":"4. Use for Time-Sensitive Queries","text":"<p>Enable web search when freshness matters:</p> <ul> <li>Recent product launches</li> <li>Current pricing</li> <li>Latest competitive moves</li> <li>Industry news impact</li> </ul>"},{"location":"user-guide/configuration/web-search/#5-track-citation-sources","title":"5. Track Citation Sources","text":"<p>Monitor which sources influence rankings:</p> <ul> <li>Identify key industry publications</li> <li>Find content gaps</li> <li>Track competitor content</li> <li>Understand ranking factors</li> </ul>"},{"location":"user-guide/configuration/web-search/#6-combine-providers","title":"6. Combine Providers","text":"<p>Use multiple web search approaches:</p> <pre><code>models:\n  # OpenAI: Selective web search\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    tools:\n      - type: \"web_search\"\n    tool_choice: \"auto\"\n\n  # Perplexity: Always web-grounded\n  - provider: \"perplexity\"\n    model_name: \"sonar-pro\"\n    env_api_key: \"PERPLEXITY_API_KEY\"\n</code></pre>"},{"location":"user-guide/configuration/web-search/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/configuration/web-search/#web-search-not-working","title":"Web Search Not Working","text":"<p>Problem: Web search tool not being used</p> <p>Check:</p> <ol> <li> <p>Tool configuration is correct:    <pre><code>tools:\n  - type: \"web_search\"  # Correct\n# Not: tool_type or search_tool\n</code></pre></p> </li> <li> <p>Tool choice is set:    <pre><code>tool_choice: \"auto\"  # or \"required\"\n</code></pre></p> </li> <li> <p>Model supports web search:</p> </li> <li>OpenAI: All chat models</li> <li>Perplexity: All models (automatic)</li> </ol>"},{"location":"user-guide/configuration/web-search/#high-costs","title":"High Costs","text":"<p>Problem: Web search costs higher than expected</p> <p>Solutions:</p> <ol> <li> <p>Check tool choice:    <pre><code>tool_choice: \"auto\"  # Not \"required\"\n</code></pre></p> </li> <li> <p>Monitor usage:    <pre><code>SELECT\n    COUNT(*) as total,\n    SUM(web_search_used) as searches,\n    AVG(estimated_cost_usd) as avg_cost\nFROM answers_raw;\n</code></pre></p> </li> <li> <p>Use cheaper models:    <pre><code>models:\n  - provider: \"openai\"\n    model_name: \"gpt-4o-mini\"  # Cheapest with web search\n</code></pre></p> </li> </ol>"},{"location":"user-guide/configuration/web-search/#inconsistent-results","title":"Inconsistent Results","text":"<p>Problem: Results vary between runs</p> <p>Cause: Web content changes frequently</p> <p>Expected behavior: Web-grounded responses will vary as web content updates.</p> <p>Mitigation:</p> <ul> <li>Run multiple queries, average results</li> <li>Track trends over time vs. point-in-time snapshots</li> <li>Use non-web models for baseline comparison</li> </ul>"},{"location":"user-guide/configuration/web-search/#next-steps","title":"Next Steps","text":"<ul> <li>Model Configuration: Choose models with web search</li> <li>Budget Configuration: Budget for web search costs</li> <li>Cost Management: Track web search spending</li> <li>HTML Reports: View web search metadata</li> </ul>"},{"location":"user-guide/features/brand-detection/","title":"Brand Mention Detection","text":"<p>Brand mention detection is the core feature of LLM Answer Watcher. It uses word-boundary regex matching to accurately identify brand mentions while preventing false positives.</p>"},{"location":"user-guide/features/brand-detection/#how-it-works","title":"How It Works","text":""},{"location":"user-guide/features/brand-detection/#word-boundary-matching","title":"Word-Boundary Matching","text":"<p>The system uses word-boundary regex (<code>\\b</code>) to ensure accurate matching:</p> <pre><code># Pattern: \\bHubSpot\\b\n# Matches: \"I use HubSpot daily\"\n# Doesn't match: \"I use HubSpotter\" or \"hub\" in \"GitHub\"\n</code></pre> <p>This prevents common false positives:</p> <ul> <li>\u2705 \"HubSpot\" matches \"HubSpot\" exactly</li> <li>\u274c \"Hub\" does NOT match \"HubSpot\"</li> <li>\u274c \"Spot\" does NOT match \"HubSpot\"</li> <li>\u274c \"hub\" does NOT match \"GitHub\"</li> </ul>"},{"location":"user-guide/features/brand-detection/#case-insensitive-matching","title":"Case-Insensitive Matching","text":"<p>All matching is case-insensitive:</p> <pre><code># All these match \"HubSpot\"\n\"HubSpot\", \"hubspot\", \"HUBSPOT\", \"HuBsPoT\"\n</code></pre>"},{"location":"user-guide/features/brand-detection/#brand-aliases","title":"Brand Aliases","text":"<p>Configure multiple aliases for each brand:</p> <pre><code>brands:\n  mine:\n    - \"Warmly\"\n    - \"Warmly.io\"\n    - \"Warmly AI\"\n\n  competitors:\n    - \"HubSpot\"\n    - \"HubSpot CRM\"\n    - \"Instantly\"\n    - \"Instantly.ai\"\n</code></pre>"},{"location":"user-guide/features/brand-detection/#configuration","title":"Configuration","text":""},{"location":"user-guide/features/brand-detection/#basic-brand-configuration","title":"Basic Brand Configuration","text":"<p>Minimal configuration with your brand and competitors:</p> <pre><code>brands:\n  mine:\n    - \"YourBrand\"\n\n  competitors:\n    - \"CompetitorA\"\n    - \"CompetitorB\"\n</code></pre>"},{"location":"user-guide/features/brand-detection/#advanced-brand-configuration","title":"Advanced Brand Configuration","text":"<p>Include all variations and common misspellings:</p> <pre><code>brands:\n  mine:\n    - \"Acme Corp\"\n    - \"Acme\"\n    - \"AcmeCorp\"\n    - \"Acme.io\"\n    - \"Acme Software\"\n\n  competitors:\n    # Direct competitors\n    - \"Competitor One\"\n    - \"CompetitorOne\"\n    - \"Competitor1\"\n\n    # Market leaders\n    - \"Industry Leader\"\n    - \"Big Player Inc\"\n\n    # Adjacent competitors\n    - \"Alternative Tool\"\n</code></pre>"},{"location":"user-guide/features/brand-detection/#brand-normalization","title":"Brand Normalization","text":"<p>Brands are normalized for storage and analysis:</p> <pre><code>\"HubSpot CRM\" \u2192 \"hubspot-crm\"\n\"Instantly.ai\" \u2192 \"instantly-ai\"\n\"Apollo.io\" \u2192 \"apollo-io\"\n</code></pre> <p>This ensures consistent matching across different formats.</p>"},{"location":"user-guide/features/brand-detection/#detection-methods","title":"Detection Methods","text":""},{"location":"user-guide/features/brand-detection/#method-1-regex-default","title":"Method 1: Regex (Default)","text":"<p>Fast, free, pattern-based detection.</p> <p>Advantages:</p> <ul> <li>Zero cost (no API calls)</li> <li>Instant results</li> <li>100% consistent</li> <li>Works offline</li> </ul> <p>Limitations:</p> <ul> <li>May miss contextual mentions</li> <li>Requires exact alias match</li> <li>No semantic understanding</li> </ul> <p>Configuration:</p> <pre><code>run_settings:\n  use_llm_rank_extraction: false\n</code></pre>"},{"location":"user-guide/features/brand-detection/#method-2-function-calling","title":"Method 2: Function Calling","text":"<p>LLM-assisted detection using function calling for higher accuracy.</p> <p>Advantages:</p> <ul> <li>Understands context</li> <li>Catches variations</li> <li>Semantic understanding</li> <li>Confidence scores</li> </ul> <p>Limitations:</p> <ul> <li>Costs money per query</li> <li>Slower than regex</li> <li>Requires extraction model</li> </ul> <p>Configuration:</p> <pre><code>extraction_settings:\n  extraction_model:\n    provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n\n  method: \"function_calling\"\n  fallback_to_regex: true\n  min_confidence: 0.7\n</code></pre>"},{"location":"user-guide/features/brand-detection/#method-3-hybrid","title":"Method 3: Hybrid","text":"<p>Combines regex and function calling for best results.</p> <p>How it works:</p> <ol> <li>Try regex first (fast, free)</li> <li>If regex fails, use function calling</li> <li>Merge results with de-duplication</li> </ol> <p>Configuration:</p> <pre><code>extraction_settings:\n  extraction_model:\n    provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n\n  method: \"hybrid\"\n  fallback_to_regex: true\n  min_confidence: 0.7\n</code></pre>"},{"location":"user-guide/features/brand-detection/#detection-results","title":"Detection Results","text":""},{"location":"user-guide/features/brand-detection/#mention-object","title":"Mention Object","text":"<p>Each detected mention includes:</p> <pre><code>{\n  \"brand\": \"HubSpot\",\n  \"normalized_name\": \"hubspot\",\n  \"is_mine\": false,\n  \"rank_position\": 1,\n  \"snippet\": \"...I recommend HubSpot for CRM needs...\",\n  \"confidence\": 1.0,\n  \"detection_method\": \"regex\"\n}\n</code></pre>"},{"location":"user-guide/features/brand-detection/#my-brands-vs-competitors","title":"My Brands vs Competitors","text":"<p>Mentions are categorized:</p> <pre><code>{\n  \"my_mentions\": [\n    {\n      \"brand\": \"Warmly\",\n      \"is_mine\": true,\n      \"rank_position\": 2\n    }\n  ],\n  \"competitor_mentions\": [\n    {\n      \"brand\": \"HubSpot\",\n      \"is_mine\": false,\n      \"rank_position\": 1\n    },\n    {\n      \"brand\": \"Instantly\",\n      \"is_mine\": false,\n      \"rank_position\": 3\n    }\n  ]\n}\n</code></pre>"},{"location":"user-guide/features/brand-detection/#common-detection-patterns","title":"Common Detection Patterns","text":""},{"location":"user-guide/features/brand-detection/#pattern-1-exact-brand-name","title":"Pattern 1: Exact Brand Name","text":"<p>LLM Response:</p> <p>\"The best email warmup tools are Warmly, Instantly, and Lemwarm.\"</p> <p>Detected:</p> <ul> <li>\u2705 Warmly</li> <li>\u2705 Instantly</li> <li>\u2705 Lemwarm</li> </ul>"},{"location":"user-guide/features/brand-detection/#pattern-2-brand-with-tld","title":"Pattern 2: Brand with TLD","text":"<p>LLM Response:</p> <p>\"Check out Warmly.io for email warmup.\"</p> <p>Detected:</p> <ul> <li>\u2705 Warmly.io</li> </ul> <p>Note: Add both \"Warmly\" and \"Warmly.io\" as aliases to catch both.</p>"},{"location":"user-guide/features/brand-detection/#pattern-3-brand-in-context","title":"Pattern 3: Brand in Context","text":"<p>LLM Response:</p> <p>\"Many sales teams use HubSpot CRM to manage leads.\"</p> <p>Detected:</p> <ul> <li>\u2705 HubSpot CRM</li> <li>\u2705 HubSpot (if both aliases configured)</li> </ul>"},{"location":"user-guide/features/brand-detection/#pattern-4-case-variations","title":"Pattern 4: Case Variations","text":"<p>LLM Response:</p> <p>\"HUBSPOT and hubspot are the same product.\"</p> <p>Detected:</p> <ul> <li>\u2705 HubSpot (both instances)</li> </ul>"},{"location":"user-guide/features/brand-detection/#preventing-false-positives","title":"Preventing False Positives","text":""},{"location":"user-guide/features/brand-detection/#use-word-boundaries","title":"Use Word Boundaries","text":"<p>\u274c Bad - Substring Matching:</p> <pre><code>brands:\n  mine:\n    - \"Hub\"  # Matches \"GitHub\", \"HubSpot\", \"hub\"\n</code></pre> <p>This creates false positives.</p> <p>\u2705 Good - Full Word Matching:</p> <pre><code>brands:\n  mine:\n    - \"HubSpot\"  # Only matches \"HubSpot\"\n</code></pre> <p>Word boundaries prevent substring matches.</p>"},{"location":"user-guide/features/brand-detection/#avoid-overly-generic-names","title":"Avoid Overly Generic Names","text":"<p>\u274c Bad:</p> <pre><code>brands:\n  competitors:\n    - \"AI\"  # Too generic\n    - \"The\"\n    - \"Pro\"\n</code></pre> <p>\u2705 Good:</p> <pre><code>brands:\n  competitors:\n    - \"OpenAI\"\n    - \"The Sales Platform\"\n    - \"Pro CRM\"\n</code></pre>"},{"location":"user-guide/features/brand-detection/#test-your-aliases","title":"Test Your Aliases","text":"<pre><code># Validate configuration\nllm-answer-watcher validate --config watcher.config.yaml\n\n# Run with example intents\nllm-answer-watcher run --config watcher.config.yaml\n</code></pre>"},{"location":"user-guide/features/brand-detection/#detection-accuracy","title":"Detection Accuracy","text":""},{"location":"user-guide/features/brand-detection/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>LLM Answer Watcher tracks detection accuracy:</p> Metric Description Target Precision Correct mentions / Total detected \u2265 90% Recall Correct mentions / Expected mentions \u2265 80% F1 Score Harmonic mean of precision and recall \u2265 85%"},{"location":"user-guide/features/brand-detection/#run-evaluations","title":"Run Evaluations","text":"<pre><code>llm-answer-watcher eval --fixtures evals/testcases/fixtures.yaml\n</code></pre> <p>See Evaluation Framework for details.</p>"},{"location":"user-guide/features/brand-detection/#advanced-detection","title":"Advanced Detection","text":""},{"location":"user-guide/features/brand-detection/#special-characters","title":"Special Characters","text":"<p>Escape special characters in brand names:</p> <pre><code>brands:\n  mine:\n    - \"Brand (TM)\"  # Automatically escaped\n    - \"Brand.io\"\n    - \"Brand-Name\"\n</code></pre> <p>The system handles escaping automatically.</p>"},{"location":"user-guide/features/brand-detection/#multi-word-brands","title":"Multi-Word Brands","text":"<pre><code>brands:\n  competitors:\n    - \"Acme Corp\"\n    - \"Big Company Inc\"\n    - \"The Sales Platform\"\n</code></pre> <p>Word boundaries work across multiple words.</p>"},{"location":"user-guide/features/brand-detection/#abbreviations","title":"Abbreviations","text":"<p>Add both full name and abbreviation:</p> <pre><code>brands:\n  competitors:\n    - \"Customer Relationship Management\"\n    - \"CRM\"\n    - \"HubSpot CRM\"\n</code></pre>"},{"location":"user-guide/features/brand-detection/#debugging-detection-issues","title":"Debugging Detection Issues","text":""},{"location":"user-guide/features/brand-detection/#issue-brand-not-detected","title":"Issue: Brand Not Detected","text":"<p>Problem: Your brand appears in response but isn't detected.</p> <p>Solutions:</p> <ol> <li>Check brand alias spelling:</li> </ol> <pre><code># View raw response\ncat output/2025-11-05T14-30-00Z/intent_*_raw_*.json | jq '.answer_text'\n</code></pre> <ol> <li>Add alias variation:</li> </ol> <pre><code>brands:\n  mine:\n    - \"YourBrand\"\n    - \"YourBrand.io\"\n    - \"Your Brand\"  # Add this\n</code></pre> <ol> <li>Check for special formatting:</li> </ol> <pre><code>\"Check out **YourBrand**\"  // Bold formatting\n\"Visit `YourBrand.io`\"     // Code formatting\n</code></pre>"},{"location":"user-guide/features/brand-detection/#issue-false-positives","title":"Issue: False Positives","text":"<p>Problem: Unrelated words are detected as brand mentions.</p> <p>Solutions:</p> <ol> <li>Remove overly generic aliases:</li> </ol> <pre><code># \u274c Remove this\nbrands:\n  mine:\n    - \"AI\"\n\n# \u2705 Use this instead\nbrands:\n  mine:\n    - \"YourBrand AI\"\n</code></pre> <ol> <li>Check word boundaries are working:</li> </ol> <pre><code># Test with evaluation suite\nllm-answer-watcher eval --fixtures evals/testcases/fixtures.yaml\n</code></pre>"},{"location":"user-guide/features/brand-detection/#issue-case-sensitivity","title":"Issue: Case Sensitivity","text":"<p>Problem: Brand detected with wrong capitalization.</p> <p>Solution: Matching is already case-insensitive, but display preserves original case from LLM response.</p> <pre><code># All match the same brand\n\"HubSpot\" \u2192 normalized to \"hubspot\"\n\"hubspot\" \u2192 normalized to \"hubspot\"\n\"HUBSPOT\" \u2192 normalized to \"hubspot\"\n</code></pre>"},{"location":"user-guide/features/brand-detection/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/features/brand-detection/#1-start-with-core-aliases","title":"1. Start with Core Aliases","text":"<pre><code>brands:\n  mine:\n    - \"YourBrand\"      # Exact name\n    - \"YourBrand.io\"   # With TLD\n</code></pre>"},{"location":"user-guide/features/brand-detection/#2-add-variations-incrementally","title":"2. Add Variations Incrementally","text":"<p>Run monitoring, review results, add missing aliases:</p> <pre><code>brands:\n  mine:\n    - \"YourBrand\"\n    - \"YourBrand.io\"\n    - \"YourBrand AI\"    # Added after reviewing results\n    - \"YB\"              # Abbreviation if commonly used\n</code></pre>"},{"location":"user-guide/features/brand-detection/#3-limit-competitor-list","title":"3. Limit Competitor List","text":"<p>Track 10-20 key competitors:</p> <pre><code>brands:\n  competitors:\n    # Top 5 direct competitors\n    - \"Competitor A\"\n    - \"Competitor B\"\n    # Top 3 market leaders\n    - \"Market Leader\"\n</code></pre>"},{"location":"user-guide/features/brand-detection/#4-monitor-detection-metrics","title":"4. Monitor Detection Metrics","text":"<pre><code>-- Check detection rates\nSELECT\n    brand,\n    COUNT(*) as total_mentions,\n    COUNT(DISTINCT run_id) as runs_appeared,\n    COUNT(*) * 100.0 / (SELECT COUNT(*) FROM runs) as appearance_rate\nFROM mentions\nWHERE timestamp_utc &gt;= datetime('now', '-30 days')\nGROUP BY brand\nORDER BY total_mentions DESC;\n</code></pre>"},{"location":"user-guide/features/brand-detection/#5-use-evaluation-suite","title":"5. Use Evaluation Suite","text":"<pre><code># Test detection before deploying\nllm-answer-watcher eval --fixtures evals/testcases/fixtures.yaml\n\n# Add custom test cases for your brands\n# See: evals/testcases/fixtures.yaml\n</code></pre>"},{"location":"user-guide/features/brand-detection/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Rank Extraction</p> <p>Learn how ranking positions are extracted</p> <p>Rank Extraction \u2192</p> </li> <li> <p> Function Calling</p> <p>Use LLM-assisted detection for higher accuracy</p> <p>Function Calling \u2192</p> </li> <li> <p> Evaluation Framework</p> <p>Test and validate detection accuracy</p> <p>Evaluation Guide \u2192</p> </li> <li> <p> Brand Configuration</p> <p>Deep dive into brand configuration strategies</p> <p>Brand Config \u2192</p> </li> </ul>"},{"location":"user-guide/features/cost-management/","title":"Cost Management","text":"<p>Control and monitor LLM API costs with built-in budget protection.</p>"},{"location":"user-guide/features/cost-management/#features","title":"Features","text":"<ul> <li>Pre-run cost estimation</li> <li>Budget limits (per run, per intent)</li> <li>Real-time cost tracking</li> <li>Cost breakdowns by provider/model</li> </ul>"},{"location":"user-guide/features/cost-management/#budget-configuration","title":"Budget Configuration","text":"<pre><code>run_settings:\n  budget:\n    enabled: true\n    max_per_run_usd: 1.00\n    max_per_intent_usd: 0.10\n    warn_threshold_usd: 0.50\n</code></pre>"},{"location":"user-guide/features/cost-management/#cost-estimation","title":"Cost Estimation","text":"<p>Before running, the tool estimates costs based on:</p> <ul> <li>Number of intents</li> <li>Number of models</li> <li>Average tokens per query</li> <li>Provider pricing</li> </ul> <p>See Budget Controls for detailed configuration.</p>"},{"location":"user-guide/features/function-calling/","title":"Function Calling for Extraction","text":"<p>Function calling uses LLMs to extract structured data from responses with higher accuracy than regex-based extraction. This feature enables semantic understanding of brand mentions and rankings.</p>"},{"location":"user-guide/features/function-calling/#overview","title":"Overview","text":"<p>Function calling instructs the LLM to output structured JSON matching a specific schema, ensuring consistent, parseable extraction results.</p>"},{"location":"user-guide/features/function-calling/#when-to-use","title":"When to Use","text":"<p>\u2705 Use function calling when:</p> <ul> <li>Regex extraction misses complex mentions</li> <li>You need contextual understanding</li> <li>Rankings are implicit (not in explicit lists)</li> <li>Budget allows for additional API calls</li> </ul> <p>\u274c Skip function calling when:</p> <ul> <li>Regex works well for your use case</li> <li>Optimizing for cost (regex is free)</li> <li>Brand names are simple and unambiguous</li> <li>Running frequent monitoring (hourly/daily)</li> </ul>"},{"location":"user-guide/features/function-calling/#configuration","title":"Configuration","text":""},{"location":"user-guide/features/function-calling/#basic-setup","title":"Basic Setup","text":"<pre><code>extraction_settings:\n  extraction_model:\n    provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n    system_prompt: \"openai/extraction-default\"\n\n  method: \"function_calling\"\n  fallback_to_regex: true\n  min_confidence: 0.7\n</code></pre>"},{"location":"user-guide/features/function-calling/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>extraction_settings:\n  extraction_model:\n    provider: \"openai\"\n    model_name: \"gpt-4o-mini\"  # Fast, cheap extraction model\n    env_api_key: \"OPENAI_API_KEY\"\n    system_prompt: \"openai/extraction-default\"\n\n  # Extraction method\n  method: \"function_calling\"  # Options: function_calling, regex, hybrid\n\n  # Fall back to regex if function calling fails\n  fallback_to_regex: true\n\n  # Minimum confidence threshold (0.0-1.0)\n  min_confidence: 0.7\n\n  # Maximum extraction attempts\n  max_retries: 2\n</code></pre>"},{"location":"user-guide/features/function-calling/#extraction-methods","title":"Extraction Methods","text":""},{"location":"user-guide/features/function-calling/#method-1-function-calling-only","title":"Method 1: Function Calling Only","text":"<p>Use LLM for all extraction:</p> <pre><code>extraction_settings:\n  extraction_model:\n    provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n\n  method: \"function_calling\"\n  fallback_to_regex: false  # Don't fall back\n</code></pre> <p>Cost: ~$0.001-0.003 per extraction</p>"},{"location":"user-guide/features/function-calling/#method-2-regex-only","title":"Method 2: Regex Only","text":"<p>Use pattern matching (no LLM):</p> <pre><code>run_settings:\n  use_llm_rank_extraction: false\n\n# No extraction_settings needed\n</code></pre> <p>Cost: Free</p>"},{"location":"user-guide/features/function-calling/#method-3-hybrid-recommended","title":"Method 3: Hybrid (Recommended)","text":"<p>Try regex first, use LLM as fallback:</p> <pre><code>extraction_settings:\n  extraction_model:\n    provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n\n  method: \"hybrid\"\n  fallback_to_regex: true\n</code></pre> <p>Cost: Variable (free for regex hits, paid for LLM fallback)</p>"},{"location":"user-guide/features/function-calling/#function-schema","title":"Function Schema","text":""},{"location":"user-guide/features/function-calling/#competitor-detection-function","title":"Competitor Detection Function","text":"<pre><code>{\n  \"name\": \"extract_competitor_mentions\",\n  \"description\": \"Extract mentions of competitor brands from LLM response\",\n  \"parameters\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"competitors\": {\n        \"type\": \"array\",\n        \"items\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"brand\": {\n              \"type\": \"string\",\n              \"description\": \"Exact brand name as mentioned\"\n            },\n            \"rank_position\": {\n              \"type\": \"integer\",\n              \"description\": \"Position in ranked list (1=first, null=not ranked)\"\n            },\n            \"confidence\": {\n              \"type\": \"number\",\n              \"description\": \"Confidence score 0.0-1.0\"\n            },\n            \"context\": {\n              \"type\": \"string\",\n              \"description\": \"Surrounding context of the mention\"\n            }\n          },\n          \"required\": [\"brand\", \"confidence\"]\n        }\n      }\n    },\n    \"required\": [\"competitors\"]\n  }\n}\n</code></pre>"},{"location":"user-guide/features/function-calling/#example-llm-response","title":"Example LLM Response","text":"<p>Input (LLM answer):</p> <pre><code>The best email warmup tools are:\n1. Instantly - Great for cold email\n2. Warmly - Excellent personalization\n3. Lemwarm - Simple and effective\n</code></pre> <p>Function Call Output:</p> <pre><code>{\n  \"competitors\": [\n    {\n      \"brand\": \"Instantly\",\n      \"rank_position\": 1,\n      \"confidence\": 0.95,\n      \"context\": \"Great for cold email\"\n    },\n    {\n      \"brand\": \"Warmly\",\n      \"rank_position\": 2,\n      \"confidence\": 0.95,\n      \"context\": \"Excellent personalization\"\n    },\n    {\n      \"brand\": \"Lemwarm\",\n      \"rank_position\": 3,\n      \"confidence\": 0.90,\n      \"context\": \"Simple and effective\"\n    }\n  ]\n}\n</code></pre>"},{"location":"user-guide/features/function-calling/#confidence-scores","title":"Confidence Scores","text":""},{"location":"user-guide/features/function-calling/#confidence-threshold","title":"Confidence Threshold","text":"<p>Only accept extractions above confidence threshold:</p> <pre><code>extraction_settings:\n  min_confidence: 0.7  # Reject extractions &lt; 70% confidence\n</code></pre>"},{"location":"user-guide/features/function-calling/#confidence-levels","title":"Confidence Levels","text":"Range Quality Action 0.90-1.00 High Accept automatically 0.70-0.89 Medium Accept with review 0.50-0.69 Low Reject or flag for review 0.00-0.49 Very Low Reject"},{"location":"user-guide/features/function-calling/#interpreting-confidence","title":"Interpreting Confidence","text":"<p>High confidence (0.9+):</p> <ul> <li>Clear, unambiguous mention</li> <li>Explicit ranking</li> <li>Standard brand name</li> </ul> <p>Medium confidence (0.7-0.9):</p> <ul> <li>Slight ambiguity</li> <li>Implicit ranking</li> <li>Brand name variation</li> </ul> <p>Low confidence (&lt;0.7):</p> <ul> <li>Ambiguous mention</li> <li>Unclear ranking</li> <li>Possible false positive</li> </ul>"},{"location":"user-guide/features/function-calling/#cost-management","title":"Cost Management","text":""},{"location":"user-guide/features/function-calling/#extraction-costs","title":"Extraction Costs","text":"<p>Function calling adds extra API calls:</p> Model Cost per 1K tokens Typical Extraction Cost gpt-4o-mini $0.15 input / $0.60 output $0.001-0.002 gpt-4o $2.50 input / $10.00 output $0.010-0.020 claude-3-5-haiku $0.80 input / $4.00 output $0.003-0.005"},{"location":"user-guide/features/function-calling/#cost-optimization","title":"Cost Optimization","text":"<p>1. Use cheap extraction models:</p> <pre><code>extraction_settings:\n  extraction_model:\n    provider: \"openai\"\n    model_name: \"gpt-4o-mini\"  # Cheapest option\n</code></pre> <p>2. Use hybrid method:</p> <pre><code>extraction_settings:\n  method: \"hybrid\"  # Free regex first, LLM fallback\n</code></pre> <p>3. Cache extraction results:</p> <p>Extraction results are stored in SQLite and reused.</p> <p>4. Limit extraction to important intents:</p> <pre><code>intents:\n  - id: \"high-priority\"\n    prompt: \"...\"\n    use_extraction: true  # Enable for this intent\n\n  - id: \"low-priority\"\n    prompt: \"...\"\n    use_extraction: false  # Skip for this intent\n</code></pre>"},{"location":"user-guide/features/function-calling/#advantages-over-regex","title":"Advantages Over Regex","text":""},{"location":"user-guide/features/function-calling/#1-semantic-understanding","title":"1. Semantic Understanding","text":"<p>Regex:</p> <pre><code>\"I recommend HubSpot\" \u2192 Detected\n\"HubSpot is not recommended\" \u2192 Detected (false positive)\n</code></pre> <p>Function Calling:</p> <pre><code>\"I recommend HubSpot\" \u2192 Detected with positive context\n\"HubSpot is not recommended\" \u2192 Not detected (understands negation)\n</code></pre>"},{"location":"user-guide/features/function-calling/#2-implicit-rankings","title":"2. Implicit Rankings","text":"<p>LLM Response:</p> <pre><code>\"While Salesforce is the market leader, I prefer HubSpot for startups.\"\n</code></pre> <p>Regex: No ranking detected (no list structure)</p> <p>Function Calling: Detects HubSpot as preferred (rank 1)</p>"},{"location":"user-guide/features/function-calling/#3-context-extraction","title":"3. Context Extraction","text":"<p>Function calling extracts surrounding context:</p> <pre><code>{\n  \"brand\": \"HubSpot\",\n  \"rank_position\": 1,\n  \"context\": \"Great for startups with limited budget\",\n  \"confidence\": 0.92\n}\n</code></pre>"},{"location":"user-guide/features/function-calling/#4-handles-variations","title":"4. Handles Variations","text":"<p>LLM mentions: \"HS CRM\", \"HubSpot's CRM\", \"HubSpot platform\"</p> <p>Regex: Misses variations</p> <p>Function Calling: Normalizes all to \"HubSpot\"</p>"},{"location":"user-guide/features/function-calling/#debugging-function-calling","title":"Debugging Function Calling","text":""},{"location":"user-guide/features/function-calling/#view-function-call-logs","title":"View Function Call Logs","text":"<pre><code># Enable verbose logging\nexport LOG_LEVEL=DEBUG\n\nllm-answer-watcher run --config watcher.config.yaml --verbose\n</code></pre>"},{"location":"user-guide/features/function-calling/#check-extraction-results","title":"Check Extraction Results","text":"<pre><code># View parsed results\ncat output/2025-11-05T14-30-00Z/intent_*_parsed_*.json | jq '.extraction_method'\n# Output: \"function_calling\" or \"regex\"\n</code></pre>"},{"location":"user-guide/features/function-calling/#common-issues","title":"Common Issues","text":"<p>Issue: Low confidence scores</p> <p>Solution: Adjust threshold:</p> <pre><code>extraction_settings:\n  min_confidence: 0.6  # Lower threshold\n</code></pre> <p>Issue: High costs</p> <p>Solution: Switch to hybrid:</p> <pre><code>extraction_settings:\n  method: \"hybrid\"  # Use regex when possible\n</code></pre> <p>Issue: Inconsistent results</p> <p>Solution: Use specific system prompt:</p> <pre><code>extraction_settings:\n  extraction_model:\n    system_prompt: \"openai/extraction-strict\"  # More consistent\n</code></pre>"},{"location":"user-guide/features/function-calling/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/features/function-calling/#1-start-with-regex","title":"1. Start with Regex","text":"<p>Test regex extraction first:</p> <pre><code>run_settings:\n  use_llm_rank_extraction: false\n</code></pre> <p>If accuracy is insufficient, enable function calling.</p>"},{"location":"user-guide/features/function-calling/#2-use-hybrid-method","title":"2. Use Hybrid Method","text":"<p>Best of both worlds:</p> <pre><code>extraction_settings:\n  method: \"hybrid\"\n  fallback_to_regex: true\n</code></pre>"},{"location":"user-guide/features/function-calling/#3-monitor-extraction-costs","title":"3. Monitor Extraction Costs","text":"<pre><code>SELECT\n    DATE(timestamp_utc) as date,\n    SUM(estimated_cost_usd) as total_cost,\n    COUNT(*) as extractions\nFROM answers_raw\nWHERE extraction_method = 'function_calling'\n  AND timestamp_utc &gt;= datetime('now', '-30 days')\nGROUP BY DATE(timestamp_utc);\n</code></pre>"},{"location":"user-guide/features/function-calling/#4-test-with-eval-suite","title":"4. Test with Eval Suite","text":"<pre><code>llm-answer-watcher eval --fixtures evals/testcases/fixtures.yaml\n</code></pre>"},{"location":"user-guide/features/function-calling/#5-use-dedicated-extraction-model","title":"5. Use Dedicated Extraction Model","text":"<p>Don't use expensive models for extraction:</p> <pre><code># \u274c Bad - expensive\nextraction_model:\n  model_name: \"gpt-4o\"\n\n# \u2705 Good - cheap and fast\nextraction_model:\n  model_name: \"gpt-4o-mini\"\n</code></pre>"},{"location":"user-guide/features/function-calling/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Brand Detection</p> <p>Understanding brand mention detection</p> <p>Brand Detection \u2192</p> </li> <li> <p> Rank Extraction</p> <p>How rankings are extracted</p> <p>Rank Extraction \u2192</p> </li> <li> <p> Cost Management</p> <p>Managing LLM costs</p> <p>Cost Management \u2192</p> </li> <li> <p> Evaluation</p> <p>Test extraction accuracy</p> <p>Evaluation \u2192</p> </li> </ul>"},{"location":"user-guide/features/historical-tracking/","title":"Historical Tracking","text":"<p>LLM Answer Watcher stores all query results in a local SQLite database for historical trend analysis.</p>"},{"location":"user-guide/features/historical-tracking/#features","title":"Features","text":"<ul> <li>Long-term Storage: All responses saved indefinitely</li> <li>Trend Analysis: Track brand visibility over time</li> <li>Comparative Analysis: Compare performance across dates</li> <li>Data Export: Query via SQL or export to CSV</li> </ul>"},{"location":"user-guide/features/historical-tracking/#database-location","title":"Database Location","text":"<pre><code>./output/watcher.db\n</code></pre>"},{"location":"user-guide/features/historical-tracking/#querying-historical-data","title":"Querying Historical Data","text":"<pre><code>-- Brand mentions over time\nSELECT DATE(timestamp_utc) as date,\n       COUNT(*) as mentions\nFROM mentions\nWHERE normalized_name = 'yourbrand'\nGROUP BY DATE(timestamp_utc)\nORDER BY date DESC;\n</code></pre> <p>See SQLite Database for more queries.</p>"},{"location":"user-guide/features/html-reports/","title":"HTML Reports","text":"<p>Auto-generated interactive HTML reports for each monitoring run.</p>"},{"location":"user-guide/features/html-reports/#features","title":"Features","text":"<ul> <li>Brand mention visualization</li> <li>Rank distribution charts</li> <li>Cost breakdowns</li> <li>Raw response inspection</li> <li>Historical trends (if multiple runs)</li> </ul>"},{"location":"user-guide/features/html-reports/#report-location","title":"Report Location","text":"<pre><code>output/YYYY-MM-DDTHH-MM-SSZ/report.html\n</code></pre>"},{"location":"user-guide/features/html-reports/#opening-reports","title":"Opening Reports","text":"<pre><code># macOS\nopen output/2025-11-05T14-30-00Z/report.html\n\n# Linux\nxdg-open output/2025-11-05T14-30-00Z/report.html\n</code></pre>"},{"location":"user-guide/features/html-reports/#report-sections","title":"Report Sections","text":"<ol> <li>Summary: Costs, queries, brands found</li> <li>Brand Mentions: Detailed mention tables</li> <li>Rank Distribution: Visual charts</li> <li>Historical Trends: Performance over time</li> <li>Raw Responses: Full LLM outputs</li> </ol>"},{"location":"user-guide/features/rank-extraction/","title":"Rank Extraction","text":"<p>Rank extraction identifies where brands appear in ranked lists within LLM responses. This feature helps track competitive positioning and brand visibility.</p>"},{"location":"user-guide/features/rank-extraction/#overview","title":"Overview","text":"<p>When LLMs generate lists like \"The best tools are:\", rank extraction determines:</p> <ol> <li>Position: Where each brand appears (1<sup>st</sup>, 2<sup>nd</sup>, 3<sup>rd</sup>, etc.)</li> <li>Context: Whether it's an explicit ranking or casual mention</li> <li>Competitors: How your brand ranks against competitors</li> </ol>"},{"location":"user-guide/features/rank-extraction/#how-it-works","title":"How It Works","text":""},{"location":"user-guide/features/rank-extraction/#pattern-based-extraction-default","title":"Pattern-Based Extraction (Default)","text":"<p>Uses regex patterns to detect numbered or bulleted lists:</p> <p>Supported Patterns:</p> <pre><code># Numbered lists\n1. HubSpot\n2. Salesforce\n3. Pipedrive\n\n# With periods\n1) HubSpot\n2) Salesforce\n\n# With dashes\n- HubSpot\n- Salesforce\n\n# With asterisks\n* HubSpot\n* Salesforce\n\n# With letters\na. HubSpot\nb. Salesforce\n</code></pre>"},{"location":"user-guide/features/rank-extraction/#ranking-algorithm","title":"Ranking Algorithm","text":"<ol> <li>Detect List Structure: Find numbered/bulleted lists in response</li> <li>Extract Brand Names: Match brands within list items</li> <li>Assign Positions: Number brands sequentially (1, 2, 3...)</li> <li>Handle Ties: Brands in same list item get same rank</li> </ol>"},{"location":"user-guide/features/rank-extraction/#configuration","title":"Configuration","text":""},{"location":"user-guide/features/rank-extraction/#use-regex-extraction-free","title":"Use Regex Extraction (Free)","text":"<p>Default method - no additional configuration needed:</p> <pre><code>run_settings:\n  use_llm_rank_extraction: false  # Use pattern-based extraction\n</code></pre> <p>Advantages:</p> <ul> <li>\u2705 Zero cost</li> <li>\u2705 Fast</li> <li>\u2705 Deterministic</li> <li>\u2705 Works offline</li> </ul> <p>Limitations:</p> <ul> <li>\u274c May miss implicit rankings</li> <li>\u274c Requires explicit list structure</li> <li>\u274c No semantic understanding</li> </ul>"},{"location":"user-guide/features/rank-extraction/#use-llm-extraction-paid","title":"Use LLM Extraction (Paid)","text":"<p>LLM-assisted extraction for complex rankings:</p> <pre><code>run_settings:\n  use_llm_rank_extraction: true  # Use LLM for extraction\n\nextraction_settings:\n  extraction_model:\n    provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n  method: \"function_calling\"\n  min_confidence: 0.7\n</code></pre> <p>Advantages:</p> <ul> <li>\u2705 Understands context</li> <li>\u2705 Extracts implicit rankings</li> <li>\u2705 Handles complex formats</li> <li>\u2705 Semantic understanding</li> </ul> <p>Limitations:</p> <ul> <li>\u274c Costs money per query</li> <li>\u274c Slower than regex</li> <li>\u274c May be inconsistent</li> </ul>"},{"location":"user-guide/features/rank-extraction/#ranking-examples","title":"Ranking Examples","text":""},{"location":"user-guide/features/rank-extraction/#example-1-simple-numbered-list","title":"Example 1: Simple Numbered List","text":"<p>LLM Response:</p> <pre><code>The best email warmup tools are:\n1. Instantly\n2. Warmly\n3. Lemwarm\n</code></pre> <p>Extracted Rankings:</p> <pre><code>[\n  {\"brand\": \"Instantly\", \"rank_position\": 1},\n  {\"brand\": \"Warmly\", \"rank_position\": 2},\n  {\"brand\": \"Lemwarm\", \"rank_position\": 3}\n]\n</code></pre>"},{"location":"user-guide/features/rank-extraction/#example-2-descriptive-list","title":"Example 2: Descriptive List","text":"<p>LLM Response:</p> <pre><code>Top CRM tools:\n1. HubSpot - Great for startups\n2. Salesforce - Enterprise solution\n3. Pipedrive - Sales-focused\n</code></pre> <p>Extracted Rankings:</p> <pre><code>[\n  {\"brand\": \"HubSpot\", \"rank_position\": 1},\n  {\"brand\": \"Salesforce\", \"rank_position\": 2},\n  {\"brand\": \"Pipedrive\", \"rank_position\": 3}\n]\n</code></pre>"},{"location":"user-guide/features/rank-extraction/#example-3-multiple-brands-per-item","title":"Example 3: Multiple Brands Per Item","text":"<p>LLM Response:</p> <pre><code>Best tools for sales teams:\n1. HubSpot and Salesforce for enterprise\n2. Pipedrive for small teams\n</code></pre> <p>Extracted Rankings:</p> <pre><code>[\n  {\"brand\": \"HubSpot\", \"rank_position\": 1},\n  {\"brand\": \"Salesforce\", \"rank_position\": 1},\n  {\"brand\": \"Pipedrive\", \"rank_position\": 2}\n]\n</code></pre>"},{"location":"user-guide/features/rank-extraction/#example-4-bulleted-list","title":"Example 4: Bulleted List","text":"<p>LLM Response:</p> <pre><code>- Instantly: Best for cold email\n- Warmly: Great for personalization\n- Lemwarm: Simple and effective\n</code></pre> <p>Extracted Rankings:</p> <pre><code>[\n  {\"brand\": \"Instantly\", \"rank_position\": 1},\n  {\"brand\": \"Warmly\", \"rank_position\": 2},\n  {\"brand\": \"Lemwarm\", \"rank_position\": 3}\n]\n</code></pre>"},{"location":"user-guide/features/rank-extraction/#example-5-prose-no-ranking","title":"Example 5: Prose (No Ranking)","text":"<p>LLM Response:</p> <pre><code>I've used HubSpot, Salesforce, and Pipedrive. They're all good options.\n</code></pre> <p>Extracted Rankings:</p> <pre><code>[\n  {\"brand\": \"HubSpot\", \"rank_position\": null},\n  {\"brand\": \"Salesforce\", \"rank_position\": null},\n  {\"brand\": \"Pipedrive\", \"rank_position\": null}\n]\n</code></pre> <p>Note: Mentions detected but no ranking assigned (not in a list).</p>"},{"location":"user-guide/features/rank-extraction/#rank-position-meanings","title":"Rank Position Meanings","text":""},{"location":"user-guide/features/rank-extraction/#position-1","title":"Position 1","text":"<p>Highest visibility - First recommendation.</p> <pre><code>-- Count #1 rankings\nSELECT brand, COUNT(*) as first_place_count\nFROM mentions\nWHERE rank_position = 1\n  AND timestamp_utc &gt;= datetime('now', '-30 days')\nGROUP BY brand\nORDER BY first_place_count DESC;\n</code></pre>"},{"location":"user-guide/features/rank-extraction/#positions-2-5","title":"Positions 2-5","text":"<p>High visibility - Listed in top recommendations.</p>"},{"location":"user-guide/features/rank-extraction/#positions-6-10","title":"Positions 6-10","text":"<p>Medium visibility - Included in comprehensive lists.</p>"},{"location":"user-guide/features/rank-extraction/#position-null","title":"Position NULL","text":"<p>Mentioned but not ranked - Appears in prose or examples.</p>"},{"location":"user-guide/features/rank-extraction/#analyzing-rankings","title":"Analyzing Rankings","text":""},{"location":"user-guide/features/rank-extraction/#average-rank-position","title":"Average Rank Position","text":"<p>Lower is better (1 is best):</p> <pre><code>SELECT\n    brand,\n    AVG(rank_position) as avg_rank,\n    COUNT(*) as mentions,\n    COUNT(CASE WHEN rank_position = 1 THEN 1 END) as first_place_count\nFROM mentions\nWHERE rank_position IS NOT NULL\n  AND timestamp_utc &gt;= datetime('now', '-30 days')\nGROUP BY brand\nORDER BY avg_rank ASC;\n</code></pre>"},{"location":"user-guide/features/rank-extraction/#rank-distribution","title":"Rank Distribution","text":"<p>See where brands typically appear:</p> <pre><code>SELECT\n    rank_position,\n    COUNT(*) as mention_count,\n    COUNT(DISTINCT brand) as unique_brands\nFROM mentions\nWHERE rank_position IS NOT NULL\nGROUP BY rank_position\nORDER BY rank_position;\n</code></pre>"},{"location":"user-guide/features/rank-extraction/#competitor-comparison","title":"Competitor Comparison","text":"<p>Compare your rank against competitors:</p> <pre><code>SELECT\n    m1.run_id,\n    m1.intent_id,\n    my_brand.rank_position as my_rank,\n    competitor.brand as competitor_name,\n    competitor.rank_position as competitor_rank\nFROM mentions m1\nJOIN mentions my_brand ON m1.run_id = my_brand.run_id\n  AND m1.intent_id = my_brand.intent_id\n  AND my_brand.is_mine = 1\nJOIN mentions competitor ON m1.run_id = competitor.run_id\n  AND m1.intent_id = competitor.intent_id\n  AND competitor.is_mine = 0\nWHERE m1.timestamp_utc &gt;= datetime('now', '-7 days')\nORDER BY m1.timestamp_utc DESC;\n</code></pre>"},{"location":"user-guide/features/rank-extraction/#rank-trends","title":"Rank Trends","text":"<p>Track how rankings change over time:</p> <pre><code>SELECT\n    DATE(timestamp_utc) as date,\n    brand,\n    AVG(rank_position) as avg_rank,\n    COUNT(*) as mentions\nFROM mentions\nWHERE rank_position IS NOT NULL\n  AND brand = 'YourBrand'\n  AND timestamp_utc &gt;= datetime('now', '-30 days')\nGROUP BY DATE(timestamp_utc), brand\nORDER BY date DESC;\n</code></pre>"},{"location":"user-guide/features/rank-extraction/#common-ranking-patterns","title":"Common Ranking Patterns","text":""},{"location":"user-guide/features/rank-extraction/#pattern-1-direct-recommendation","title":"Pattern 1: Direct Recommendation","text":"<p>Prompt: \"What's the best CRM?\"</p> <p>Response: \"I recommend HubSpot for most teams.\"</p> <p>Rank: Position 1 (single recommendation)</p>"},{"location":"user-guide/features/rank-extraction/#pattern-2-top-3-list","title":"Pattern 2: Top 3 List","text":"<p>Prompt: \"Top 3 CRM tools?\"</p> <p>Response:</p> <pre><code>1. HubSpot\n2. Salesforce\n3. Pipedrive\n</code></pre> <p>Rank: Explicit positions 1-3</p>"},{"location":"user-guide/features/rank-extraction/#pattern-3-comprehensive-list","title":"Pattern 3: Comprehensive List","text":"<p>Prompt: \"List all major CRM tools\"</p> <p>Response: Lists 10+ tools</p> <p>Rank: All assigned positions, less emphasis on specific rank</p>"},{"location":"user-guide/features/rank-extraction/#pattern-4-categorized-lists","title":"Pattern 4: Categorized Lists","text":"<p>Prompt: \"Best CRM by company size?\"</p> <p>Response:</p> <pre><code>For startups:\n1. HubSpot\n2. Pipedrive\n\nFor enterprise:\n1. Salesforce\n2. Microsoft Dynamics\n</code></pre> <p>Rank: Multiple brands at position 1 (different categories)</p>"},{"location":"user-guide/features/rank-extraction/#debugging-ranking-issues","title":"Debugging Ranking Issues","text":""},{"location":"user-guide/features/rank-extraction/#issue-no-rankings-detected","title":"Issue: No Rankings Detected","text":"<p>Problem: Brands detected but <code>rank_position</code> is <code>null</code>.</p> <p>Cause: Response doesn't contain explicit lists.</p> <p>Example Response:</p> <pre><code>I've used HubSpot and Salesforce. Both are great options.\n</code></pre> <p>Solution:</p> <ol> <li>Update intent prompts to encourage rankings:</li> </ol> <pre><code># \u274c Generic\nprompt: \"Tell me about CRM tools\"\n\n# \u2705 Ranking-focused\nprompt: \"What are the top 5 CRM tools ranked by popularity?\"\n</code></pre> <ol> <li>Enable LLM rank extraction:</li> </ol> <pre><code>run_settings:\n  use_llm_rank_extraction: true\n</code></pre>"},{"location":"user-guide/features/rank-extraction/#issue-incorrect-rankings","title":"Issue: Incorrect Rankings","text":"<p>Problem: Rankings don't match actual LLM response order.</p> <p>Debugging:</p> <pre><code># View raw response\ncat output/2025-11-05T14-30-00Z/intent_*_raw_*.json | jq '.answer_text'\n\n# View extracted rankings\ncat output/2025-11-05T14-30-00Z/intent_*_parsed_*.json | jq '.ranked_list'\n</code></pre> <p>Solutions:</p> <ol> <li>Check for unusual list formatting</li> <li>Enable LLM rank extraction</li> <li>Add evaluation test case</li> </ol>"},{"location":"user-guide/features/rank-extraction/#issue-all-brands-ranked-1","title":"Issue: All Brands Ranked #1","text":"<p>Problem: Multiple brands get <code>rank_position: 1</code>.</p> <p>Cause: Brands appear in separate lists or categories.</p> <p>Example:</p> <pre><code>Best for startups: HubSpot\nBest for enterprise: Salesforce\n</code></pre> <p>Both get rank 1 (different contexts).</p> <p>This is correct behavior - each is #1 in its category.</p>"},{"location":"user-guide/features/rank-extraction/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/features/rank-extraction/#1-design-ranking-friendly-prompts","title":"1. Design Ranking-Friendly Prompts","text":"<pre><code>intents:\n  # \u2705 Good - Encourages ranking\n  - id: \"top-5-crm-tools\"\n    prompt: \"What are the top 5 CRM tools ranked by market share?\"\n\n  # \u2705 Good - Specific ranking criteria\n  - id: \"best-for-startups\"\n    prompt: \"Rank the best CRM tools for early-stage startups\"\n\n  # \u274c Bad - No ranking signal\n  - id: \"crm-info\"\n    prompt: \"Tell me about CRM software\"\n</code></pre>"},{"location":"user-guide/features/rank-extraction/#2-use-regex-first-llm-as-fallback","title":"2. Use Regex First, LLM as Fallback","text":"<pre><code>extraction_settings:\n  method: \"hybrid\"  # Try regex, fallback to LLM\n  fallback_to_regex: true\n</code></pre>"},{"location":"user-guide/features/rank-extraction/#3-track-rank-changes","title":"3. Track Rank Changes","text":"<pre><code>-- Alert when rank drops\nWITH latest_ranks AS (\n  SELECT brand, AVG(rank_position) as current_avg\n  FROM mentions\n  WHERE timestamp_utc &gt;= datetime('now', '-7 days')\n    AND brand = 'YourBrand'\n  GROUP BY brand\n),\nprevious_ranks AS (\n  SELECT brand, AVG(rank_position) as previous_avg\n  FROM mentions\n  WHERE timestamp_utc &gt;= datetime('now', '-14 days')\n    AND timestamp_utc &lt; datetime('now', '-7 days')\n    AND brand = 'YourBrand'\n  GROUP BY brand\n)\nSELECT\n  l.brand,\n  p.previous_avg as previous_rank,\n  l.current_avg as current_rank,\n  l.current_avg - p.previous_avg as rank_change\nFROM latest_ranks l\nJOIN previous_ranks p ON l.brand = p.brand\nWHERE l.current_avg &gt; p.previous_avg;  -- Rank got worse (higher number)\n</code></pre>"},{"location":"user-guide/features/rank-extraction/#4-analyze-by-intent","title":"4. Analyze by Intent","text":"<p>Some intents may favor certain brands:</p> <pre><code>SELECT\n    intent_id,\n    brand,\n    AVG(rank_position) as avg_rank,\n    COUNT(*) as mentions\nFROM mentions\nWHERE rank_position IS NOT NULL\nGROUP BY intent_id, brand\nORDER BY intent_id, avg_rank;\n</code></pre>"},{"location":"user-guide/features/rank-extraction/#5-monitor-first-place-wins","title":"5. Monitor First-Place Wins","text":"<pre><code>-- Track #1 rankings over time\nSELECT\n    DATE(timestamp_utc) as date,\n    COUNT(CASE WHEN is_mine = 1 THEN 1 END) as my_first_place,\n    COUNT(CASE WHEN is_mine = 0 THEN 1 END) as competitor_first_place\nFROM mentions\nWHERE rank_position = 1\n  AND timestamp_utc &gt;= datetime('now', '-30 days')\nGROUP BY DATE(timestamp_utc)\nORDER BY date DESC;\n</code></pre>"},{"location":"user-guide/features/rank-extraction/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Brand Detection</p> <p>Learn how brands are detected</p> <p>Brand Detection \u2192</p> </li> <li> <p> Function Calling</p> <p>Use LLM-assisted ranking extraction</p> <p>Function Calling \u2192</p> </li> <li> <p> Query Examples</p> <p>SQL queries for ranking analysis</p> <p>Query Examples \u2192</p> </li> <li> <p> Trends Analysis</p> <p>Track ranking changes over time</p> <p>Trends Analysis \u2192</p> </li> </ul>"},{"location":"user-guide/features/sentiment-analysis/","title":"Sentiment Analysis &amp; Intent Classification","text":"<p>Advanced analysis features that extract sentiment, context, and intent from brand mentions and user queries using LLM function calling.</p> <p>New in v0.1.0</p> <p>These features were added to enhance brand mention analysis and enable prioritization of high-value queries.</p>"},{"location":"user-guide/features/sentiment-analysis/#overview","title":"Overview","text":"<p>LLM Answer Watcher includes two powerful analysis features:</p> <ol> <li>Sentiment Analysis: Analyzes the tone and context of each brand mention</li> <li>Intent Classification: Determines the user's intent and buyer journey stage for each query</li> </ol> <p>Both features use OpenAI's function calling API for accurate, structured extraction.</p>"},{"location":"user-guide/features/sentiment-analysis/#sentiment-analysis","title":"Sentiment Analysis","text":""},{"location":"user-guide/features/sentiment-analysis/#what-it-analyzes","title":"What It Analyzes","text":"<p>For each brand mention, the system extracts:</p> <p>Sentiment - Emotional tone: - <code>positive</code>: Brand recommended or praised - <code>neutral</code>: Brand mentioned without judgment - <code>negative</code>: Brand criticized or not recommended</p> <p>Mention Context - How the brand was mentioned: - <code>primary_recommendation</code>: Brand is the top recommendation - <code>alternative_listing</code>: Brand listed as one of several options - <code>competitor_negative</code>: Brand mentioned as inferior to others - <code>competitor_neutral</code>: Brand compared without negative bias - <code>passing_reference</code>: Brief mention without detail</p>"},{"location":"user-guide/features/sentiment-analysis/#example","title":"Example","text":"<p>Query: \"What are the best email warmup tools?\"</p> <p>LLM Response: \"The best tools are Lemwarm for automated warmup and Instantly for cold outreach. HubSpot is also an option but quite expensive.\"</p> <p>Extracted Sentiments:</p> Brand Sentiment Context Reasoning Lemwarm <code>positive</code> <code>primary_recommendation</code> Listed first with positive qualifier Instantly <code>positive</code> <code>primary_recommendation</code> Listed alongside Lemwarm with use case HubSpot <code>neutral</code> <code>alternative_listing</code> Mentioned as option with cost caveat"},{"location":"user-guide/features/sentiment-analysis/#configuration","title":"Configuration","text":"<p>Enable sentiment analysis in <code>extraction_settings</code>:</p> <pre><code>extraction_settings:\n  extraction_model:\n    provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n\n  method: \"function_calling\"\n\n  # Enable sentiment analysis (default: true)\n  enable_sentiment_analysis: true\n</code></pre> <p>Function Calling Required</p> <p>Sentiment analysis only works with <code>method: \"function_calling\"</code>. Regex extraction does not support sentiment analysis (fields will be <code>None</code>).</p>"},{"location":"user-guide/features/sentiment-analysis/#cost-impact","title":"Cost Impact","text":"<p>Sentiment analysis is integrated into function calling extraction:</p> <ul> <li>No extra LLM calls - sentiment extracted in same call as brand mentions</li> <li>Cost increase: ~33% per extraction call due to larger response schema</li> <li>Example: $0.0002 \u2192 $0.00027 per extraction with gpt-4o-mini</li> </ul>"},{"location":"user-guide/features/sentiment-analysis/#database-storage","title":"Database Storage","text":"<p>Sentiments are stored in the <code>mentions</code> table:</p> <pre><code>SELECT brand, sentiment, mention_context, timestamp_utc\nFROM mentions\nWHERE sentiment = 'positive'\n  AND mention_context = 'primary_recommendation'\nORDER BY timestamp_utc DESC;\n</code></pre> <p>Schema:</p> <pre><code>ALTER TABLE mentions ADD COLUMN sentiment TEXT;\nALTER TABLE mentions ADD COLUMN mention_context TEXT;\n</code></pre>"},{"location":"user-guide/features/sentiment-analysis/#intent-classification","title":"Intent Classification","text":""},{"location":"user-guide/features/sentiment-analysis/#what-it-classifies","title":"What It Classifies","text":"<p>For each user query, the system determines:</p> <p>Intent Type - What the user wants: - <code>transactional</code>: Ready to buy/use a tool - <code>commercial_investigation</code>: Researching options before purchase - <code>informational</code>: Learning about a topic - <code>navigational</code>: Looking for a specific brand/site</p> <p>Buyer Journey Stage - Where they are in the purchase process: - <code>awareness</code>: Learning about the category - <code>consideration</code>: Evaluating options - <code>decision</code>: Ready to choose/purchase</p> <p>Urgency Signal - How urgent is the need: - <code>high</code>: Immediate need (\"now\", \"urgent\", \"today\") - <code>medium</code>: Near-term need (\"soon\", \"this week\") - <code>low</code>: Future or casual exploration</p> <p>Classification Confidence - How confident the model is (0.0-1.0)</p> <p>Reasoning - Explanation of why it was classified this way</p>"},{"location":"user-guide/features/sentiment-analysis/#examples","title":"Examples","text":""},{"location":"user-guide/features/sentiment-analysis/#high-value-query","title":"High-Value Query","text":"<p>Query: \"What are the best email warmup tools to buy now for my outreach campaign?\"</p> <p>Classification: <pre><code>{\n  \"intent_type\": \"transactional\",\n  \"buyer_stage\": \"decision\",\n  \"urgency_signal\": \"high\",\n  \"classification_confidence\": 0.95,\n  \"reasoning\": \"Query contains 'buy now' and specific use case, indicating ready-to-purchase intent with high urgency\"\n}\n</code></pre></p>"},{"location":"user-guide/features/sentiment-analysis/#research-query","title":"Research Query","text":"<p>Query: \"How do email warmup tools work?\"</p> <p>Classification: <pre><code>{\n  \"intent_type\": \"informational\",\n  \"buyer_stage\": \"awareness\",\n  \"urgency_signal\": \"low\",\n  \"classification_confidence\": 0.92,\n  \"reasoning\": \"Query seeks explanation, indicating learning phase without purchase intent\"\n}\n</code></pre></p>"},{"location":"user-guide/features/sentiment-analysis/#comparison-query","title":"Comparison Query","text":"<p>Query: \"Compare Lemwarm vs Instantly for cold email\"</p> <p>Classification: <pre><code>{\n  \"intent_type\": \"commercial_investigation\",\n  \"buyer_stage\": \"consideration\",\n  \"urgency_signal\": \"medium\",\n  \"classification_confidence\": 0.88,\n  \"reasoning\": \"Direct comparison of specific brands indicates evaluation phase before purchase decision\"\n}\n</code></pre></p>"},{"location":"user-guide/features/sentiment-analysis/#configuration_1","title":"Configuration","text":"<p>Enable intent classification in <code>extraction_settings</code>:</p> <pre><code>extraction_settings:\n  extraction_model:\n    provider: \"openai\"\n    model_name: \"gpt-4o-mini\"\n    env_api_key: \"OPENAI_API_KEY\"\n\n  # Enable intent classification (default: true)\n  enable_intent_classification: true\n</code></pre>"},{"location":"user-guide/features/sentiment-analysis/#cost-impact_1","title":"Cost Impact","text":"<p>Intent classification adds one extra LLM call per unique query:</p> <ul> <li>Cost: ~$0.00012 per query with gpt-4o-mini</li> <li>When: Before extracting brand mentions</li> <li>Caching: Results are cached by query hash, so repeated queries are free</li> </ul> <p>Example cost breakdown: - 3 intents \u00d7 1 model = 3 queries - Intent classification: 3 \u00d7 $0.00012 = $0.00036 - Extraction: 3 \u00d7 $0.0002 = \\(0.0006 - **Total**: ~\\)0.001 per run</p>"},{"location":"user-guide/features/sentiment-analysis/#database-storage_1","title":"Database Storage","text":"<p>Intent classifications are stored in <code>intent_classifications</code> table:</p> <pre><code>SELECT intent_id, intent_type, buyer_stage, urgency_signal, reasoning\nFROM intent_classifications\nWHERE buyer_stage = 'decision'\n  AND urgency_signal = 'high'\nORDER BY classification_confidence DESC;\n</code></pre> <p>Schema:</p> <pre><code>CREATE TABLE intent_classifications (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    run_id TEXT NOT NULL,\n    intent_id TEXT NOT NULL,\n    query_text TEXT NOT NULL,\n    query_hash TEXT NOT NULL,\n    intent_type TEXT NOT NULL,\n    buyer_stage TEXT NOT NULL,\n    urgency_signal TEXT NOT NULL,\n    classification_confidence REAL NOT NULL,\n    reasoning TEXT,\n    timestamp_utc TEXT NOT NULL,\n    UNIQUE(run_id, intent_id)\n);\n</code></pre>"},{"location":"user-guide/features/sentiment-analysis/#query-hash-caching","title":"Query Hash Caching","text":"<p>Intent classifications are cached by query hash:</p> <pre><code># Normalized query \u2192 hash\n\"What are the best email warmup tools?\"\n\u2192 \"5d41402abc4b2a76b9719d911017c592...\"\n\n# Same hash for semantically identical queries\n\"  what are the BEST email warmup tools?  \"\n\u2192 \"5d41402abc4b2a76b9719d911017c592...\" (same hash)\n</code></pre> <p>Caching benefits: - Saves API calls: Repeated queries use cached results - Normalizes variations: Whitespace/case differences don't matter - Persistent cache: Stored in database across runs</p>"},{"location":"user-guide/features/sentiment-analysis/#use-cases","title":"Use Cases","text":""},{"location":"user-guide/features/sentiment-analysis/#1-prioritize-high-value-queries","title":"1. Prioritize High-Value Queries","text":"<p>Focus on queries with high buyer intent:</p> <pre><code>SELECT m.brand, ic.intent_type, ic.buyer_stage, ic.urgency_signal\nFROM mentions m\nJOIN intent_classifications ic ON m.intent_id = ic.intent_id\nWHERE ic.intent_type = 'transactional'\n  AND ic.buyer_stage = 'decision'\n  AND ic.urgency_signal = 'high'\n  AND m.sentiment = 'positive';\n</code></pre>"},{"location":"user-guide/features/sentiment-analysis/#2-track-sentiment-trends","title":"2. Track Sentiment Trends","text":"<p>Monitor how sentiment changes over time:</p> <pre><code>SELECT DATE(timestamp_utc) as date,\n       sentiment,\n       COUNT(*) as mentions\nFROM mentions\nWHERE normalized_name = 'yourbrand'\nGROUP BY DATE(timestamp_utc), sentiment\nORDER BY date DESC;\n</code></pre>"},{"location":"user-guide/features/sentiment-analysis/#3-identify-context-patterns","title":"3. Identify Context Patterns","text":"<p>See how your brand is typically mentioned:</p> <pre><code>SELECT mention_context,\n       COUNT(*) as count,\n       ROUND(AVG(CASE sentiment\n           WHEN 'positive' THEN 1.0\n           WHEN 'neutral' THEN 0.5\n           WHEN 'negative' THEN 0.0\n       END), 2) as sentiment_score\nFROM mentions\nWHERE normalized_name = 'yourbrand'\nGROUP BY mention_context\nORDER BY count DESC;\n</code></pre>"},{"location":"user-guide/features/sentiment-analysis/#4-roi-analysis","title":"4. ROI Analysis","text":"<p>Calculate value of brand mentions by intent:</p> <pre><code>SELECT ic.buyer_stage,\n       COUNT(DISTINCT m.brand) as brands_mentioned,\n       COUNT(*) as total_mentions\nFROM mentions m\nJOIN intent_classifications ic ON m.intent_id = ic.intent_id\nWHERE m.is_mine = 1\nGROUP BY ic.buyer_stage\nORDER BY CASE ic.buyer_stage\n    WHEN 'decision' THEN 1\n    WHEN 'consideration' THEN 2\n    WHEN 'awareness' THEN 3\nEND;\n</code></pre>"},{"location":"user-guide/features/sentiment-analysis/#disabling-features","title":"Disabling Features","text":""},{"location":"user-guide/features/sentiment-analysis/#disable-sentiment-analysis","title":"Disable Sentiment Analysis","text":"<pre><code>extraction_settings:\n  enable_sentiment_analysis: false\n</code></pre> <p>Result: <code>sentiment</code> and <code>mention_context</code> fields will be <code>None</code> in database.</p>"},{"location":"user-guide/features/sentiment-analysis/#disable-intent-classification","title":"Disable Intent Classification","text":"<pre><code>extraction_settings:\n  enable_intent_classification: false\n</code></pre> <p>Result: No rows in <code>intent_classifications</code> table, queries classified as <code>None</code>.</p>"},{"location":"user-guide/features/sentiment-analysis/#disable-both","title":"Disable Both","text":"<pre><code>extraction_settings:\n  enable_sentiment_analysis: false\n  enable_intent_classification: false\n</code></pre> <p>Benefit: Reduces costs by ~33% for extraction calls and eliminates intent classification calls.</p>"},{"location":"user-guide/features/sentiment-analysis/#limitations","title":"Limitations","text":""},{"location":"user-guide/features/sentiment-analysis/#function-calling-only","title":"Function Calling Only","text":"<p>Both features require <code>method: \"function_calling\"</code>:</p> <pre><code>extraction_settings:\n  method: \"function_calling\"  # Required\n  enable_sentiment_analysis: true\n  enable_intent_classification: true\n</code></pre> <p>Regex extraction does not support these features.</p>"},{"location":"user-guide/features/sentiment-analysis/#provider-support","title":"Provider Support","text":"<p>Currently only OpenAI supports function calling for extraction:</p> <pre><code>extraction_model:\n  provider: \"openai\"  # Required\n  model_name: \"gpt-4o-mini\"\n</code></pre> <p>Anthropic, Mistral, and other providers coming soon.</p>"},{"location":"user-guide/features/sentiment-analysis/#confidence-thresholds","title":"Confidence Thresholds","text":"<p>Low confidence classifications may be inaccurate:</p> <pre><code>-- Filter by confidence\nSELECT *\nFROM intent_classifications\nWHERE classification_confidence &gt;= 0.8;\n</code></pre>"},{"location":"user-guide/features/sentiment-analysis/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/features/sentiment-analysis/#1-enable-for-high-value-monitoring","title":"1. Enable for High-Value Monitoring","text":"<p>Use sentiment/intent for business-critical queries:</p> <pre><code># Production config - full analysis\nextraction_settings:\n  method: \"function_calling\"\n  enable_sentiment_analysis: true\n  enable_intent_classification: true\n</code></pre>"},{"location":"user-guide/features/sentiment-analysis/#2-disable-for-cost-optimization","title":"2. Disable for Cost Optimization","text":"<p>Skip for budget-constrained or high-frequency monitoring:</p> <pre><code># Cost-optimized config\nextraction_settings:\n  method: \"regex\"  # No function calling\n  enable_sentiment_analysis: false\n  enable_intent_classification: false\n</code></pre>"},{"location":"user-guide/features/sentiment-analysis/#3-review-classification-reasoning","title":"3. Review Classification Reasoning","text":"<p>Check why queries were classified:</p> <pre><code>SELECT query_text, intent_type, buyer_stage, reasoning\nFROM intent_classifications\nWHERE classification_confidence &lt; 0.8;\n</code></pre>"},{"location":"user-guide/features/sentiment-analysis/#4-track-sentiment-distribution","title":"4. Track Sentiment Distribution","text":"<p>Monitor the health of your brand's mentions:</p> <pre><code>SELECT sentiment,\n       COUNT(*) as mentions,\n       ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 1) as percentage\nFROM mentions\nWHERE normalized_name = 'yourbrand'\nGROUP BY sentiment;\n</code></pre> <p>Healthy distribution: 70%+ positive, &lt;10% negative</p>"},{"location":"user-guide/features/sentiment-analysis/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Function Calling</p> <p>Learn how function calling works</p> <p>Function Calling \u2192</p> </li> <li> <p> Query Examples</p> <p>SQL queries for sentiment analysis</p> <p>Query Examples \u2192</p> </li> <li> <p> Cost Management</p> <p>Understand cost implications</p> <p>Cost Management \u2192</p> </li> <li> <p> Trends Analysis</p> <p>Track sentiment over time</p> <p>Trends \u2192</p> </li> </ul>"},{"location":"user-guide/usage/automation/","title":"Automation","text":"<p>Automate LLM Answer Watcher runs with cron, GitHub Actions, or custom schedulers.</p>"},{"location":"user-guide/usage/automation/#quick-start","title":"Quick Start","text":"<pre><code># Run with no prompts\nllm-answer-watcher run --config config.yaml --yes --format json\n</code></pre>"},{"location":"user-guide/usage/automation/#cron-jobs","title":"Cron Jobs","text":""},{"location":"user-guide/usage/automation/#basic-cron-setup","title":"Basic Cron Setup","text":"<p>Edit crontab:</p> <pre><code>crontab -e\n</code></pre> <p>Add scheduled job:</p> <pre><code># Run daily at 9 AM\n0 9 * * * /path/to/.venv/bin/llm-answer-watcher run --config /path/to/config.yaml --yes --quiet &gt;&gt; /var/log/monitoring.log 2&gt;&amp;1\n\n# Run weekly on Monday\n0 9 * * 1 /path/to/.venv/bin/llm-answer-watcher run --config /path/to/config.yaml --yes --format json &gt; /path/to/results/$(date +\\%Y-\\%m-\\%d).json\n</code></pre>"},{"location":"user-guide/usage/automation/#production-cron-script","title":"Production Cron Script","text":"<pre><code>#!/bin/bash\n# /usr/local/bin/run-monitoring.sh\n\nset -euo pipefail\n\n# Configuration\nCONFIG=\"/home/user/monitoring/config.yaml\"\nVENV=\"/home/user/llm-answer-watcher/.venv\"\nLOG_DIR=\"/var/log/monitoring\"\n\n# Load environment\nsource \"$VENV/bin/activate\"\nsource /home/user/.env  # API keys\n\n# Run with error handling\n\"$VENV/bin/llm-answer-watcher\" run \\\n    --config \"$CONFIG\" \\\n    --yes \\\n    --format json \\\n    &gt; \"$LOG_DIR/$(date +%Y-%m-%d).json\" 2&gt;&amp;1\n\nEXIT_CODE=$?\n\n# Alert on failure\nif [ $EXIT_CODE -eq 4 ]; then\n    echo \"Monitoring failed\" | mail -s \"Alert: Monitoring Failure\" ops@example.com\nfi\n\nexit $EXIT_CODE\n</code></pre> <p>Make executable and schedule:</p> <pre><code>chmod +x /usr/local/bin/run-monitoring.sh\n\n# Add to crontab\n0 9 * * * /usr/local/bin/run-monitoring.sh\n</code></pre>"},{"location":"user-guide/usage/automation/#github-actions","title":"GitHub Actions","text":""},{"location":"user-guide/usage/automation/#basic-workflow","title":"Basic Workflow","text":"<p><code>.github/workflows/brand-monitoring.yml</code>:</p> <pre><code>name: Brand Monitoring\n\non:\n  schedule:\n    # Run daily at 9 AM UTC\n    - cron: '0 9 * * *'\n  workflow_dispatch:  # Manual trigger\n\njobs:\n  monitor:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.12'\n\n      - name: Install dependencies\n        run: |\n          pip install uv\n          uv sync\n\n      - name: Run monitoring\n        env:\n          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}\n        run: |\n          uv run llm-answer-watcher run \\\n            --config configs/production.yaml \\\n            --yes \\\n            --format json \\\n            &gt; results.json\n\n      - name: Upload results\n        uses: actions/upload-artifact@v4\n        with:\n          name: monitoring-results\n          path: |\n            results.json\n            output/\n\n      - name: Commit database\n        run: |\n          git config --local user.email \"bot@example.com\"\n          git config --local user.name \"Monitoring Bot\"\n          git add output/watcher.db\n          git commit -m \"Update monitoring data\"\n          git push\n</code></pre>"},{"location":"user-guide/usage/automation/#advanced-workflow-with-notifications","title":"Advanced Workflow with Notifications","text":"<pre><code>name: Advanced Brand Monitoring\n\non:\n  schedule:\n    - cron: '0 9 * * *'\n\njobs:\n  monitor:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.12'\n\n      - name: Install\n        run: |\n          pip install uv\n          uv sync\n\n      - name: Run monitoring\n        id: monitor\n        env:\n          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n        run: |\n          uv run llm-answer-watcher run \\\n            --config config.yaml \\\n            --yes \\\n            --format json | tee results.json\n          echo \"exit_code=$?\" &gt;&gt; $GITHUB_OUTPUT\n        continue-on-error: true\n\n      - name: Parse results\n        id: parse\n        run: |\n          COST=$(jq -r '.total_cost_usd' results.json)\n          BRANDS=$(jq -r '.brands_detected.mine | length' results.json)\n          echo \"cost=$COST\" &gt;&gt; $GITHUB_OUTPUT\n          echo \"brands_found=$BRANDS\" &gt;&gt; $GITHUB_OUTPUT\n\n      - name: Slack notification\n        if: steps.monitor.outputs.exit_code == '0'\n        uses: slackapi/slack-github-action@v1\n        with:\n          payload: |\n            {\n              \"text\": \"\u2705 Brand monitoring completed\",\n              \"blocks\": [\n                {\n                  \"type\": \"section\",\n                  \"text\": {\n                    \"type\": \"mrkdwn\",\n                    \"text\": \"*Brand Monitoring Results*\\n\u2022 Cost: $$${{ steps.parse.outputs.cost }}\\n\u2022 Brands found: ${{ steps.parse.outputs.brands_found }}\"\n                  }\n                }\n              ]\n            }\n        env:\n          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}\n\n      - name: Alert on failure\n        if: steps.monitor.outputs.exit_code == '4'\n        uses: slackapi/slack-github-action@v1\n        with:\n          payload: |\n            {\n              \"text\": \"\u274c Brand monitoring failed completely\"\n            }\n        env:\n          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}\n</code></pre>"},{"location":"user-guide/usage/automation/#docker-automation","title":"Docker Automation","text":""},{"location":"user-guide/usage/automation/#dockerfile","title":"Dockerfile","text":"<pre><code>FROM python:3.12-slim\n\nWORKDIR /app\n\n# Install uv\nRUN pip install uv\n\n# Copy project\nCOPY . .\n\n# Install dependencies\nRUN uv sync\n\n# Set entrypoint\nENTRYPOINT [\"uv\", \"run\", \"llm-answer-watcher\"]\nCMD [\"run\", \"--config\", \"config.yaml\", \"--yes\", \"--format\", \"json\"]\n</code></pre>"},{"location":"user-guide/usage/automation/#docker-compose","title":"Docker Compose","text":"<pre><code># docker-compose.yml\nversion: '3.8'\n\nservices:\n  monitoring:\n    build: .\n    environment:\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}\n    volumes:\n      - ./output:/app/output\n      - ./configs:/app/configs\n    command: run --config configs/production.yaml --yes --format json\n</code></pre> <p>Run:</p> <pre><code>docker-compose up\n</code></pre>"},{"location":"user-guide/usage/automation/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/usage/automation/#1-use-yes-flag","title":"1. Use --yes Flag","text":"<p>Skip confirmation prompts:</p> <pre><code>llm-answer-watcher run --config config.yaml --yes\n</code></pre>"},{"location":"user-guide/usage/automation/#2-use-json-or-quiet-mode","title":"2. Use JSON or Quiet Mode","text":"<p>For parsing:</p> <pre><code>llm-answer-watcher run --config config.yaml --yes --format json\n</code></pre>"},{"location":"user-guide/usage/automation/#3-handle-exit-codes","title":"3. Handle Exit Codes","text":"<pre><code>llm-answer-watcher run --config config.yaml --yes\ncase $? in\n    0|3) echo \"Success or partial success\" ;;\n    *) echo \"Error occurred\" &amp;&amp; exit 1 ;;\nesac\n</code></pre>"},{"location":"user-guide/usage/automation/#4-secure-api-keys","title":"4. Secure API Keys","text":"<p>Never hardcode API keys:</p> <pre><code># \u2705 Good - from environment\nexport OPENAI_API_KEY=sk-...\n\n# \u2705 Good - from secrets management\nOPENAI_API_KEY=$(aws secretsmanager get-secret-value --secret-id openai-key)\n</code></pre>"},{"location":"user-guide/usage/automation/#5-log-output","title":"5. Log Output","text":"<pre><code>llm-answer-watcher run --config config.yaml --yes \\\n    --format json \\\n    &gt; /var/log/monitoring/$(date +%Y-%m-%d).json 2&gt;&amp;1\n</code></pre>"},{"location":"user-guide/usage/automation/#6-rotate-logs","title":"6. Rotate Logs","text":"<pre><code># Keep last 30 days\nfind /var/log/monitoring -name \"*.json\" -mtime +30 -delete\n</code></pre>"},{"location":"user-guide/usage/automation/#next-steps","title":"Next Steps","text":"<ul> <li>See CI/CD examples</li> <li>Learn about output modes</li> <li>Understand exit codes</li> </ul>"},{"location":"user-guide/usage/cli-commands/","title":"CLI Commands","text":"<p>Complete reference for all LLM Answer Watcher CLI commands.</p>"},{"location":"user-guide/usage/cli-commands/#command-structure","title":"Command Structure","text":"<pre><code>llm-answer-watcher [COMMAND] [OPTIONS]\n</code></pre>"},{"location":"user-guide/usage/cli-commands/#global-options","title":"Global Options","text":"<p>Available for all commands:</p> Option Description <code>--help</code> Show help message <code>--version</code> Show version information"},{"location":"user-guide/usage/cli-commands/#commands","title":"Commands","text":""},{"location":"user-guide/usage/cli-commands/#run","title":"<code>run</code>","text":"<p>Execute a monitoring run with configured models and intents.</p> <p>Usage:</p> <pre><code>llm-answer-watcher run --config CONFIG_PATH [OPTIONS]\n</code></pre> <p>Required Arguments:</p> <ul> <li><code>--config PATH</code> - Path to YAML configuration file</li> </ul> <p>Options:</p> Option Default Description <code>--format [human\\|json\\|quiet]</code> <code>human</code> Output format <code>--yes, -y</code> <code>false</code> Skip confirmation prompts <code>--force</code> <code>false</code> Override budget limits <code>--verbose, -v</code> <code>false</code> Enable verbose logging <p>Examples:</p> <pre><code># Human-friendly output (default)\nllm-answer-watcher run --config config.yaml\n\n# JSON output for automation\nllm-answer-watcher run --config config.yaml --format json\n\n# Quiet mode for scripts\nllm-answer-watcher run --config config.yaml --quiet\n\n# Auto-confirm (no prompts)\nllm-answer-watcher run --config config.yaml --yes\n\n# Override budget limits\nllm-answer-watcher run --config config.yaml --force\n\n# Verbose logging\nllm-answer-watcher run --config config.yaml --verbose\n</code></pre> <p>Exit Codes:</p> <ul> <li><code>0</code>: Success</li> <li><code>1</code>: Configuration error</li> <li><code>2</code>: Database error</li> <li><code>3</code>: Partial failure</li> <li><code>4</code>: Complete failure</li> </ul>"},{"location":"user-guide/usage/cli-commands/#validate","title":"<code>validate</code>","text":"<p>Validate configuration file without running queries.</p> <p>Usage:</p> <pre><code>llm-answer-watcher validate --config CONFIG_PATH [OPTIONS]\n</code></pre> <p>Required Arguments:</p> <ul> <li><code>--config PATH</code> - Path to YAML configuration file</li> </ul> <p>Options:</p> Option Default Description <code>--verbose, -v</code> <code>false</code> Show detailed validation <p>Examples:</p> <pre><code># Basic validation\nllm-answer-watcher validate --config config.yaml\n\n# Detailed validation\nllm-answer-watcher validate --config config.yaml --verbose\n</code></pre> <p>Output:</p> <pre><code>\u2705 Configuration valid\n\u251c\u2500\u2500 Models: 2 configured (openai gpt-4o-mini, anthropic claude-3-5-haiku)\n\u251c\u2500\u2500 Brands: 3 mine, 8 competitors\n\u251c\u2500\u2500 Intents: 4 queries\n\u2514\u2500\u2500 Estimated cost: $0.024 (8 queries total)\n</code></pre>"},{"location":"user-guide/usage/cli-commands/#eval","title":"<code>eval</code>","text":"<p>Run evaluation framework to test extraction accuracy.</p> <p>Usage:</p> <pre><code>llm-answer-watcher eval --fixtures FIXTURES_PATH [OPTIONS]\n</code></pre> <p>Required Arguments:</p> <ul> <li><code>--fixtures PATH</code> - Path to test fixtures YAML file</li> </ul> <p>Options:</p> Option Default Description <code>--db PATH</code> <code>./eval_results.db</code> Evaluation database path <code>--format [human\\|json]</code> <code>human</code> Output format <p>Examples:</p> <pre><code># Run evaluation suite\nllm-answer-watcher eval --fixtures evals/testcases/fixtures.yaml\n\n# Custom database\nllm-answer-watcher eval --fixtures fixtures.yaml --db my_evals.db\n\n# JSON output for CI/CD\nllm-answer-watcher eval --fixtures fixtures.yaml --format json\n</code></pre> <p>Exit Codes:</p> <ul> <li><code>0</code>: All tests passed</li> <li><code>1</code>: Tests failed (below thresholds)</li> <li><code>2</code>: Configuration error</li> </ul>"},{"location":"user-guide/usage/cli-commands/#prices-show","title":"<code>prices show</code>","text":"<p>Display current LLM pricing information.</p> <p>Usage:</p> <pre><code>llm-answer-watcher prices show [OPTIONS]\n</code></pre> <p>Options:</p> Option Description <code>--provider NAME</code> Filter by provider <code>--format [human\\|json]</code> Output format <p>Examples:</p> <pre><code># Show all pricing\nllm-answer-watcher prices show\n\n# OpenAI pricing only\nllm-answer-watcher prices show --provider openai\n\n# JSON format\nllm-answer-watcher prices show --format json\n</code></pre>"},{"location":"user-guide/usage/cli-commands/#prices-refresh","title":"<code>prices refresh</code>","text":"<p>Refresh pricing cache from llm-prices.com.</p> <p>Usage:</p> <pre><code>llm-answer-watcher prices refresh [OPTIONS]\n</code></pre> <p>Options:</p> Option Description <code>--force</code> Force refresh (ignore cache) <p>Examples:</p> <pre><code># Refresh if cache expired\nllm-answer-watcher prices refresh\n\n# Force refresh\nllm-answer-watcher prices refresh --force\n</code></pre>"},{"location":"user-guide/usage/cli-commands/#prices-list","title":"<code>prices list</code>","text":"<p>List all available models with pricing.</p> <p>Usage:</p> <pre><code>llm-answer-watcher prices list [OPTIONS]\n</code></pre> <p>Options:</p> Option Description <code>--provider NAME</code> Filter by provider <code>--format [human\\|json]</code> Output format <p>Examples:</p> <pre><code># List all models\nllm-answer-watcher prices list\n\n# Anthropic models only\nllm-answer-watcher prices list --provider anthropic\n\n# Export as JSON\nllm-answer-watcher prices list --format json &gt; models.json\n</code></pre>"},{"location":"user-guide/usage/cli-commands/#output-modes","title":"Output Modes","text":""},{"location":"user-guide/usage/cli-commands/#human-mode-default","title":"Human Mode (Default)","text":"<p>Beautiful Rich-formatted output with colors, spinners, and tables.</p> <pre><code>llm-answer-watcher run --config config.yaml\n</code></pre> <p>Features: - Progress spinners - Colored status indicators - Formatted tables - Visual charts</p> <p>Best for: Interactive terminal use</p>"},{"location":"user-guide/usage/cli-commands/#json-mode","title":"JSON Mode","text":"<p>Structured JSON output for programmatic consumption.</p> <pre><code>llm-answer-watcher run --config config.yaml --format json\n</code></pre> <p>Features: - Valid JSON output - No ANSI codes - Machine-readable - Complete metadata</p> <p>Best for: AI agents, scripts, APIs</p>"},{"location":"user-guide/usage/cli-commands/#quiet-mode","title":"Quiet Mode","text":"<p>Minimal tab-separated output.</p> <pre><code>llm-answer-watcher run --config config.yaml --quiet\n</code></pre> <p>Output format: <pre><code>RUN_ID  STATUS  QUERIES COST    OUTPUT_DIR\n</code></pre></p> <p>Best for: Shell scripts, pipelines</p>"},{"location":"user-guide/usage/cli-commands/#common-workflows","title":"Common Workflows","text":""},{"location":"user-guide/usage/cli-commands/#development","title":"Development","text":"<pre><code># Validate config\nllm-answer-watcher validate --config dev.yaml\n\n# Run with verbose logging\nllm-answer-watcher run --config dev.yaml --verbose\n</code></pre>"},{"location":"user-guide/usage/cli-commands/#production","title":"Production","text":"<pre><code># Auto-confirm, JSON output\nllm-answer-watcher run --config prod.yaml --yes --format json\n</code></pre>"},{"location":"user-guide/usage/cli-commands/#cicd","title":"CI/CD","text":"<pre><code># Quiet mode with exit code checking\nllm-answer-watcher run --config ci.yaml --quiet --yes\nif [ $? -eq 0 ]; then\n    echo \"Success\"\nelse\n    echo \"Failed\" &amp;&amp; exit 1\nfi\n</code></pre>"},{"location":"user-guide/usage/cli-commands/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about output modes</li> <li>Understand exit codes</li> <li>Automate runs</li> </ul>"},{"location":"user-guide/usage/exit-codes/","title":"Exit Codes","text":"<p>LLM Answer Watcher uses standardized exit codes for automation and error handling.</p>"},{"location":"user-guide/usage/exit-codes/#exit-code-reference","title":"Exit Code Reference","text":"Code Status Meaning When It Occurs 0 Success All queries completed successfully No errors encountered 1 Configuration Error Invalid configuration YAML syntax errors, missing API keys, invalid schema 2 Database Error Cannot access database SQLite file locked, permissions issue, disk full 3 Partial Failure Some queries failed LLM API errors, rate limits, timeouts 4 Complete Failure No queries succeeded All queries failed, fatal errors"},{"location":"user-guide/usage/exit-codes/#exit-code-0-success","title":"Exit Code 0: Success","text":"<p>All queries completed without errors.</p> <p>When: - All LLM API calls succeeded - All brands extracted successfully - All data written to database - Reports generated</p> <p>Example:</p> <pre><code>llm-answer-watcher run --config config.yaml\necho $?  # Prints: 0\n</code></pre>"},{"location":"user-guide/usage/exit-codes/#exit-code-1-configuration-error","title":"Exit Code 1: Configuration Error","text":"<p>Configuration file has issues.</p> <p>When: - YAML syntax errors - Missing required fields - Invalid provider names - API keys not found in environment - Invalid model names - Budget misconfiguration</p> <p>Examples:</p> <pre><code># Missing required field\nrun_settings:\n  # output_dir missing!\n  models:\n    - provider: \"openai\"\n</code></pre> <pre><code># Invalid provider\nmodels:\n  - provider: \"invalid_provider\"  # Not supported\n</code></pre> <p>Handling:</p> <pre><code>llm-answer-watcher run --config config.yaml\nif [ $? -eq 1 ]; then\n    echo \"Configuration error - check your YAML file\"\n    exit 1\nfi\n</code></pre>"},{"location":"user-guide/usage/exit-codes/#exit-code-2-database-error","title":"Exit Code 2: Database Error","text":"<p>Cannot create or access SQLite database.</p> <p>When: - Database file locked by another process - Insufficient disk space - Permission denied on output directory - Corrupted database file</p> <p>Handling:</p> <pre><code>llm-answer-watcher run --config config.yaml\ncase $? in\n    2)\n        echo \"Database error - check permissions and disk space\"\n        # Try to fix permissions\n        chmod 755 output/\n        # Retry\n        llm-answer-watcher run --config config.yaml\n        ;;\nesac\n</code></pre>"},{"location":"user-guide/usage/exit-codes/#exit-code-3-partial-failure","title":"Exit Code 3: Partial Failure","text":"<p>Some queries succeeded, others failed.</p> <p>When: - Rate limits hit mid-run - Network timeouts - Invalid API responses - Model-specific errors</p> <p>Example Scenario:</p> <pre><code>3 intents \u00d7 2 models = 6 total queries\n\u2705 4 succeeded\n\u274c 2 failed (rate limit)\nExit code: 3 (partial failure)\n</code></pre> <p>Handling:</p> <pre><code>llm-answer-watcher run --config config.yaml --format json &gt; result.json\nif [ $? -eq 3 ]; then\n    echo \"\u26a0\ufe0f Partial failure - some queries failed\"\n    # Check which queries failed\n    jq '.per_intent_results[] | select(.status==\"failed\")' result.json\n    # Continue with successful results\nfi\n</code></pre> <p>Best Practice: Accept partial failures in production. The succeeded queries still provide value.</p>"},{"location":"user-guide/usage/exit-codes/#exit-code-4-complete-failure","title":"Exit Code 4: Complete Failure","text":"<p>All queries failed.</p> <p>When: - All API keys invalid - Network completely down - All models unreachable - Severe runtime errors</p> <p>Handling:</p> <pre><code>llm-answer-watcher run --config config.yaml\nif [ $? -eq 4 ]; then\n    echo \"\u274c Complete failure - no queries succeeded\"\n    # Alert on-call engineer\n    # Don't continue pipeline\n    exit 1\nfi\n</code></pre>"},{"location":"user-guide/usage/exit-codes/#practical-examples","title":"Practical Examples","text":""},{"location":"user-guide/usage/exit-codes/#basic-error-handling","title":"Basic Error Handling","text":"<pre><code>#!/bin/bash\nllm-answer-watcher run --config config.yaml --yes\n\ncase $? in\n    0)\n        echo \"\u2705 Success - all queries completed\"\n        ;;\n    1)\n        echo \"\u274c Configuration error - fix YAML file\"\n        exit 1\n        ;;\n    2)\n        echo \"\u274c Database error - check permissions\"\n        exit 1\n        ;;\n    3)\n        echo \"\u26a0\ufe0f Partial failure - continuing\"\n        # Partial success is OK\n        ;;\n    4)\n        echo \"\u274c Complete failure - aborting\"\n        exit 1\n        ;;\nesac\n</code></pre>"},{"location":"user-guide/usage/exit-codes/#retry-logic","title":"Retry Logic","text":"<pre><code>#!/bin/bash\nMAX_RETRIES=3\nRETRY_COUNT=0\n\nwhile [ $RETRY_COUNT -lt $MAX_RETRIES ]; do\n    llm-answer-watcher run --config config.yaml --yes\n    EXIT_CODE=$?\n\n    case $EXIT_CODE in\n        0|3)\n            # Success or partial success\n            exit 0\n            ;;\n        1|2)\n            # Config or DB error - don't retry\n            exit $EXIT_CODE\n            ;;\n        4)\n            # Complete failure - retry\n            RETRY_COUNT=$((RETRY_COUNT + 1))\n            echo \"Retry $RETRY_COUNT/$MAX_RETRIES after complete failure\"\n            sleep $((2 ** RETRY_COUNT))  # Exponential backoff\n            ;;\n    esac\ndone\n\necho \"Max retries exceeded\"\nexit 4\n</code></pre>"},{"location":"user-guide/usage/exit-codes/#cicd-integration","title":"CI/CD Integration","text":"<pre><code># .github/workflows/monitoring.yml\n- name: Run Monitoring\n  id: monitor\n  run: |\n    llm-answer-watcher run --config config.yaml --format json --yes\n    echo \"exit_code=$?\" &gt;&gt; $GITHUB_OUTPUT\n  continue-on-error: true\n\n- name: Handle Result\n  run: |\n    case ${{ steps.monitor.outputs.exit_code }} in\n      0)\n        echo \"\u2705 Success\"\n        ;;\n      1)\n        echo \"\u274c Configuration error\"\n        exit 1\n        ;;\n      2)\n        echo \"\u274c Database error\"\n        exit 1\n        ;;\n      3)\n        echo \"\u26a0\ufe0f Partial failure (acceptable)\"\n        ;;\n      4)\n        echo \"\u274c Complete failure\"\n        exit 1\n        ;;\n    esac\n</code></pre>"},{"location":"user-guide/usage/exit-codes/#alerting-based-on-exit-codes","title":"Alerting Based on Exit Codes","text":"<pre><code>#!/bin/bash\nllm-answer-watcher run --config config.yaml --yes\nEXIT_CODE=$?\n\nif [ $EXIT_CODE -eq 4 ]; then\n    # Send alert for complete failure\n    curl -X POST https://alerts.example.com/webhook \\\n        -d '{\"alert\": \"LLM monitoring complete failure\", \"severity\": \"critical\"}'\nelif [ $EXIT_CODE -eq 3 ]; then\n    # Log partial failure (no alert)\n    echo \"$(date): Partial failure\" &gt;&gt; /var/log/monitoring.log\nfi\n</code></pre>"},{"location":"user-guide/usage/exit-codes/#testing-exit-codes","title":"Testing Exit Codes","text":""},{"location":"user-guide/usage/exit-codes/#simulate-errors","title":"Simulate Errors","text":"<p>Test your error handling:</p> <pre><code># Force configuration error\nllm-answer-watcher run --config nonexistent.yaml\necho $?  # Should be 1\n\n# Invalid API key\nexport OPENAI_API_KEY=invalid\nllm-answer-watcher run --config config.yaml\necho $?  # Should be 1 or 4\n</code></pre>"},{"location":"user-guide/usage/exit-codes/#validation-testing","title":"Validation Testing","text":"<pre><code># This should exit 0 (validation success)\nllm-answer-watcher validate --config config.yaml\necho $?\n</code></pre>"},{"location":"user-guide/usage/exit-codes/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/usage/exit-codes/#1-always-check-exit-codes","title":"1. Always Check Exit Codes","text":"<pre><code># \u274c Bad - ignores errors\nllm-answer-watcher run --config config.yaml\n\n# \u2705 Good - checks exit code\nllm-answer-watcher run --config config.yaml\nif [ $? -ne 0 ]; then\n    handle_error\nfi\n</code></pre>"},{"location":"user-guide/usage/exit-codes/#2-differentiate-error-types","title":"2. Differentiate Error Types","text":"<p>Don't treat all non-zero exits the same:</p> <pre><code># \u2705 Good - handles each error type\ncase $? in\n    1|2) exit 1 ;;      # Fatal - abort\n    3) continue ;;      # Partial - OK\n    4) retry ;;         # Complete - retry\nesac\n</code></pre>"},{"location":"user-guide/usage/exit-codes/#3-log-exit-codes","title":"3. Log Exit Codes","text":"<pre><code>EXIT_CODE=$?\necho \"$(date): Exit code $EXIT_CODE\" &gt;&gt; monitoring.log\n</code></pre>"},{"location":"user-guide/usage/exit-codes/#4-accept-partial-failures","title":"4. Accept Partial Failures","text":"<p>In production, partial success is often acceptable:</p> <pre><code>if [ $EXIT_CODE -eq 0 ] || [ $EXIT_CODE -eq 3 ]; then\n    echo \"Run completed with usable results\"\n    continue_pipeline\nfi\n</code></pre>"},{"location":"user-guide/usage/exit-codes/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about output modes</li> <li>Automate monitoring runs</li> <li>See CI/CD examples</li> </ul>"},{"location":"user-guide/usage/output-modes/","title":"Output Modes","text":"<p>LLM Answer Watcher supports three output modes to serve different use cases: humans, AI agents, and shell scripts.</p>"},{"location":"user-guide/usage/output-modes/#human-mode-default","title":"Human Mode (Default)","text":"<p>Beautiful Rich-formatted output designed for interactive terminal use.</p>"},{"location":"user-guide/usage/output-modes/#usage","title":"Usage","text":"<pre><code>llm-answer-watcher run --config config.yaml\n# or explicitly:\nllm-answer-watcher run --config config.yaml --format human\n</code></pre>"},{"location":"user-guide/usage/output-modes/#features","title":"Features","text":"<ul> <li>Progress Spinners: Real-time progress indication</li> <li>Colors: Status indicators (\u2705 green success, \u274c red errors)</li> <li>Tables: Formatted data presentation</li> <li>Panels: Organized information display</li> <li>Live Updates: Dynamic progress tracking</li> </ul>"},{"location":"user-guide/usage/output-modes/#example-output","title":"Example Output","text":"<pre><code>\ud83d\udd0d Running LLM Answer Watcher...\n\u251c\u2500\u2500 Configuration loaded from config.yaml\n\u251c\u2500\u2500 Models: 2 configured\n\u251c\u2500\u2500 Intents: 3 queries\n\u2514\u2500\u2500 Estimated cost: $0.012\n\n\ud83d\udce4 Query 1/3: \"What are the best email warmup tools?\"\n\u251c\u2500\u2500 Provider: OpenAI (gpt-4o-mini)\n\u251c\u2500\u2500 Sending request... \u23f3\n\u251c\u2500\u2500 \u2705 Response received (1.2s)\n\u251c\u2500\u2500 Tokens: 145 input, 387 output\n\u251c\u2500\u2500 Cost: $0.004\n\u2514\u2500\u2500 Brands detected: 3 found\n\n\u2705 Run completed successfully!\n\n\ud83d\udcca Summary:\n\u251c\u2500\u2500 Run ID: 2025-11-05T14-30-00Z\n\u251c\u2500\u2500 Queries: 3/3 completed (100%)\n\u251c\u2500\u2500 Total cost: $0.012\n\u2514\u2500\u2500 Output: ./output/2025-11-05T14-30-00Z/\n</code></pre>"},{"location":"user-guide/usage/output-modes/#json-mode","title":"JSON Mode","text":"<p>Structured JSON output for programmatic consumption and AI agent automation.</p>"},{"location":"user-guide/usage/output-modes/#usage_1","title":"Usage","text":"<pre><code>llm-answer-watcher run --config config.yaml --format json\n</code></pre>"},{"location":"user-guide/usage/output-modes/#features_1","title":"Features","text":"<ul> <li>Valid JSON: Parseable by any JSON library</li> <li>No ANSI Codes: Clean output for parsing</li> <li>Complete Metadata: All run information included</li> <li>Deterministic: Same format every time</li> </ul>"},{"location":"user-guide/usage/output-modes/#output-structure","title":"Output Structure","text":"<pre><code>{\n  \"run_id\": \"2025-11-05T14-30-00Z\",\n  \"status\": \"success\",\n  \"timestamp_utc\": \"2025-11-05T14:30:00Z\",\n  \"queries_completed\": 3,\n  \"queries_failed\": 0,\n  \"total_cost_usd\": 0.012,\n  \"output_dir\": \"./output/2025-11-05T14-30-00Z\",\n  \"brands_detected\": {\n    \"mine\": [\"Lemwarm\", \"Lemlist\"],\n    \"competitors\": [\"Instantly\", \"HubSpot\", \"Apollo.io\"]\n  },\n  \"per_intent_results\": [\n    {\n      \"intent_id\": \"best-email-warmup-tools\",\n      \"status\": \"success\",\n      \"cost_usd\": 0.004,\n      \"brands_found\": [\"Lemwarm\", \"Instantly\", \"HubSpot\"]\n    }\n  ]\n}\n</code></pre>"},{"location":"user-guide/usage/output-modes/#use-cases","title":"Use Cases","text":""},{"location":"user-guide/usage/output-modes/#ai-agent-automation","title":"AI Agent Automation","text":"<pre><code>import subprocess\nimport json\n\nresult = subprocess.run([\n    \"llm-answer-watcher\", \"run\",\n    \"--config\", \"config.yaml\",\n    \"--format\", \"json\",\n    \"--yes\"\n], capture_output=True, text=True)\n\ndata = json.loads(result.stdout)\n\nif data[\"status\"] == \"success\":\n    print(f\"Found {len(data['brands_detected']['mine'])} of our brands\")\n</code></pre>"},{"location":"user-guide/usage/output-modes/#cicd-integration","title":"CI/CD Integration","text":"<pre><code># .github/workflows/brand-monitoring.yml\n- name: Run Brand Monitoring\n  id: monitor\n  run: |\n    OUTPUT=$(llm-answer-watcher run --config config.yaml --format json --yes)\n    echo \"result=$OUTPUT\" &gt;&gt; $GITHUB_OUTPUT\n\n- name: Check Results\n  run: |\n    STATUS=$(echo '${{ steps.monitor.outputs.result }}' | jq -r '.status')\n    if [ \"$STATUS\" != \"success\" ]; then\n      exit 1\n    fi\n</code></pre>"},{"location":"user-guide/usage/output-modes/#quiet-mode","title":"Quiet Mode","text":"<p>Minimal tab-separated output for shell scripts and pipelines.</p>"},{"location":"user-guide/usage/output-modes/#usage_2","title":"Usage","text":"<pre><code>llm-answer-watcher run --config config.yaml --quiet\n</code></pre>"},{"location":"user-guide/usage/output-modes/#output-format","title":"Output Format","text":"<pre><code>RUN_ID  STATUS  QUERIES_COMPLETED   COST_USD    OUTPUT_DIR\n</code></pre>"},{"location":"user-guide/usage/output-modes/#example-output_1","title":"Example Output","text":"<pre><code>2025-11-05T14-30-00Z    success 3   0.012   ./output/2025-11-05T14-30-00Z\n</code></pre>"},{"location":"user-guide/usage/output-modes/#use-cases_1","title":"Use Cases","text":""},{"location":"user-guide/usage/output-modes/#shell-scripts","title":"Shell Scripts","text":"<pre><code>#!/bin/bash\nOUTPUT=$(llm-answer-watcher run --config config.yaml --quiet --yes)\n\nRUN_ID=$(echo \"$OUTPUT\" | cut -f1)\nSTATUS=$(echo \"$OUTPUT\" | cut -f2)\nCOST=$(echo \"$OUTPUT\" | cut -f4)\n\necho \"Run $RUN_ID completed with status $STATUS (cost: \\$$ $COST)\"\n</code></pre>"},{"location":"user-guide/usage/output-modes/#csv-export","title":"CSV Export","text":"<pre><code># Append to CSV file\necho \"timestamp,run_id,status,queries,cost\" &gt; monitoring_log.csv\nllm-answer-watcher run --config config.yaml --quiet --yes &gt;&gt; monitoring_log.csv\n</code></pre>"},{"location":"user-guide/usage/output-modes/#pipeline-processing","title":"Pipeline Processing","text":"<pre><code># Process multiple configs\nfor config in configs/*.yaml; do\n    llm-answer-watcher run --config \"$config\" --quiet --yes | \\\n        awk '{print $1 \"\\t\" $2 \"\\t\" $4}'\ndone\n</code></pre>"},{"location":"user-guide/usage/output-modes/#comparing-output-modes","title":"Comparing Output Modes","text":"Feature Human JSON Quiet Colors/Emojis \u2705 Yes \u274c No \u274c No Progress Indicators \u2705 Yes \u274c No \u274c No Machine Parseable \u274c No \u2705 Yes \u2705 Yes Size Large Medium Minimal Use Case Interactive Automation Scripts ANSI Codes \u2705 Yes \u274c No \u274c No"},{"location":"user-guide/usage/output-modes/#verbose-logging","title":"Verbose Logging","text":"<p>Enable verbose logging in any mode:</p> <pre><code>llm-answer-watcher run --config config.yaml --verbose\n</code></pre> <p>Adds detailed logging information:</p> <pre><code>[2025-11-05 14:30:00] INFO: Loading configuration from config.yaml\n[2025-11-05 14:30:00] DEBUG: Validating YAML schema\n[2025-11-05 14:30:00] DEBUG: Resolving environment variables\n[2025-11-05 14:30:00] INFO: API key loaded for provider: openai\n[2025-11-05 14:30:01] DEBUG: Sending request to OpenAI API\n[2025-11-05 14:30:02] DEBUG: Response received: 200 OK\n</code></pre>"},{"location":"user-guide/usage/output-modes/#mode-selection-guide","title":"Mode Selection Guide","text":""},{"location":"user-guide/usage/output-modes/#choose-human-mode-when","title":"Choose Human Mode When:","text":"<ul> <li>Running manually in terminal</li> <li>Debugging configuration issues</li> <li>Watching progress in real-time</li> <li>Presenting to stakeholders</li> </ul>"},{"location":"user-guide/usage/output-modes/#choose-json-mode-when","title":"Choose JSON Mode When:","text":"<ul> <li>Integrating with AI agents</li> <li>Building dashboards/UIs</li> <li>Processing results programmatically</li> <li>CI/CD automation</li> </ul>"},{"location":"user-guide/usage/output-modes/#choose-quiet-mode-when","title":"Choose Quiet Mode When:","text":"<ul> <li>Shell script automation</li> <li>Logging to files</li> <li>CSV/TSV export</li> <li>Minimal bandwidth/storage</li> </ul>"},{"location":"user-guide/usage/output-modes/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about exit codes</li> <li>Automate monitoring runs</li> <li>See CI/CD examples</li> </ul>"}]}