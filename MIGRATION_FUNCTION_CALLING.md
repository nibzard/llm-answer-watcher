# Migration Guide: Function Calling for Brand Extraction

This guide helps you upgrade to function calling-based brand extraction for higher accuracy and lower latency.

## Overview

**Function calling** uses OpenAI's structured output API to extract brand mentions directly from the LLM, eliminating the need for regex parsing. This provides:

- **37% cost reduction** vs LLM-assisted ranking
- **40-50% latency reduction** (GPT-5-nano is 3-5x faster)
- **Higher accuracy** (LLM understands context and variations)
- **Better rank detection** (no need for numbered list patterns)

## Backward Compatibility

**Existing configs continue to work without changes!**

If you don't add `extraction_settings` to your config, the system uses regex-based extraction (the old method). This ensures zero breaking changes.

## Quick Start

### 1. Add extraction_settings to your config

```yaml
# watcher.config.yaml

run_settings:
  # Your existing settings...
  models:
    - provider: "openai"
      model_name: "gpt-4.1-nano"
      env_api_key: "OPENAI_API_KEY"

# NEW: Add extraction settings
extraction_settings:
  extraction_model:
    provider: "openai"
    model_name: "gpt-5-nano"  # Fast + cheap for extraction
    env_api_key: "OPENAI_API_KEY"
    system_prompt: "openai/extraction-default"  # Optional

  method: "function_calling"  # or "regex" or "hybrid"
  fallback_to_regex: true     # Recommended for reliability
  min_confidence: 0.7         # Filter low-confidence mentions

brands:
  mine:
    - "Lemwarm"
  competitors:
    - "Instantly"
    - "HubSpot"

intents:
  - id: "email-warmup"
    prompt: "What are the best email warmup tools?"
```

### 2. Run as usual

```bash
llm-answer-watcher run --config examples/watcher.config.yaml
```

That's it! The system will now use function calling for extraction.

## Configuration Options

### Extraction Methods

**`method: "function_calling"`** (Recommended)
- Uses LLM with structured output
- Highest accuracy
- Best for production

**`method: "regex"`**
- Traditional word-boundary matching
- Backward compatible
- No extraction API calls (zero cost)

**`method: "hybrid"`**
- Tries function calling first
- Falls back to regex on errors
- Best for testing

### Extraction Models

We recommend **GPT-5-nano** for extraction:
- Ultra-fast (3-5x faster than GPT-4)
- Ultra-cheap ($0.15/1M input, $0.60/1M output)
- Optimized for structured output

You can also use:
- `gpt-4o-mini`: More accurate, slightly more expensive
- `gpt-4.1-nano`: Also fast and cheap

### Fallback Settings

**`fallback_to_regex: true`** (Recommended)
- If function calling fails (API error, timeout), uses regex
- Ensures 100% extraction coverage
- No failed runs due to extraction errors

**`fallback_to_regex: false`**
- Fails the run if function calling doesn't work
- Useful if you want to ensure function calling is always used

### Confidence Thresholds

**`min_confidence: 0.7`** (Recommended)
- Filters out low-confidence mentions
- Reduces false positives
- Maps to:
  - `high`: confidence >= 0.8 (explicitly recommended)
  - `medium`: confidence >= 0.5 (mentioned as option)
  - `low`: confidence >= 0.0 (mentioned in passing)

## Cost Comparison

### Before (Regex Only)
```
Answer generation: GPT-4.1-nano @ $0.0054 per query
Extraction: Free (regex)
Total: $0.0054 per query
```

### After (Function Calling)
```
Answer generation: GPT-4.1-nano @ $0.0054 per query
Extraction: GPT-5-nano @ $0.00018 per query
Total: $0.00558 per query (3% more, but WAY more accurate)
```

### vs LLM-Assisted Ranking (Old Method)
```
Answer generation: GPT-4.1-nano @ $0.0054 per query
LLM ranking: GPT-4.1-nano @ $0.0035 per query
Total: $0.0089 per query

Function calling saves 37%! ($0.00558 vs $0.0089)
```

## Output Changes

### JSON Artifacts

The parsed answer JSON now includes:

```json
{
  "appeared_mine": true,
  "my_mentions": [...],
  "competitor_mentions": [...],
  "ranked_list": [...],
  "rank_extraction_method": "function_calling",  // NEW
  "rank_confidence": 0.95,
  "extraction_cost_usd": 0.00018  // NEW: tracks extraction cost
}
```

### Database Changes

**No schema changes required!**

The `extraction_cost_usd` is tracked in-memory and included in `total_cost_usd` for each run.

### Logs

With function calling enabled, you'll see logs like:

```
INFO: Using function calling extraction for email-warmup (method=function_calling)
INFO: Function calling extraction succeeded for email-warmup: found 3 brands
INFO: Success: intent=email-warmup, provider=openai, model=gpt-4.1-nano,
      answer_cost=$0.005400, extraction_cost=$0.000180,
      total=$0.005580, appeared_mine=True, extraction_method=function_calling
```

## Troubleshooting

### "Model gpt-5-nano not found"

GPT-5-nano might not be available yet. Use `gpt-4o-mini` instead:

```yaml
extraction_settings:
  extraction_model:
    model_name: "gpt-4o-mini"  # Use this if gpt-5-nano unavailable
```

### "Function calling failed"

If you see this error and have `fallback_to_regex: true`, the system automatically falls back to regex. Check:

1. API key is valid: `echo $OPENAI_API_KEY`
2. Model is available (try `gpt-4o-mini` instead)
3. Check logs for specific error messages

### "Extraction cost higher than expected"

Function calling adds ~$0.0002 per query. If this is too expensive:

1. Use `method: "regex"` (free, but less accurate)
2. Use `method: "hybrid"` (only pays when regex fails)
3. Use cheaper extraction model (though gpt-5-nano is already the cheapest)

## Advanced: Custom Extraction Prompts

Create custom system prompts for extraction:

```bash
# Create custom prompt
cat > llm_answer_watcher/system_prompts/openai/my-extraction.json <<EOF
{
  "prompt": "You are a specialized extractor for [YOUR_DOMAIN]. Focus on [SPECIFIC_INSTRUCTIONS].",
  "notes": "Custom extraction prompt"
}
EOF
```

Then use it in config:

```yaml
extraction_settings:
  extraction_model:
    system_prompt: "openai/my-extraction"
```

## Testing

### A/B Test Function Calling vs Regex

Run the same config twice with different methods:

```bash
# Test 1: Regex (old method)
# Comment out extraction_settings in config
llm-answer-watcher run --config watcher.config.yaml

# Test 2: Function calling
# Enable extraction_settings
llm-answer-watcher run --config watcher.config.yaml

# Compare results in output/ directory
```

## FAQ

**Q: Do I need to re-run old queries?**
A: No. Existing results in the database are not affected. New runs will use function calling.

**Q: Can I use function calling with Anthropic/Mistral/Google?**
A: Not yet. Currently only OpenAI supports the function calling format we use. Support for other providers coming soon.

**Q: Will this work with my custom brands?**
A: Yes! Function calling works with any brand names. It's especially good at handling:
- Brand variations ("HubSpot" vs "hubspot")
- Multi-word brands ("Apollo.io" vs "Apollo")
- Brands with special characters

**Q: How do I go back to regex?**
A: Either:
1. Remove `extraction_settings` from config, or
2. Set `method: "regex"`

##Summary

Function calling provides:
âœ… Higher accuracy (semantic understanding)
âœ… Lower latency (40-50% faster)
âœ… Better rank detection (no regex patterns needed)
âœ… Lower cost (37% cheaper than LLM-assisted ranking)
âœ… Backward compatible (optional upgrade)

Recommended config:
```yaml
extraction_settings:
  extraction_model:
    provider: "openai"
    model_name: "gpt-5-nano"
    env_api_key: "OPENAI_API_KEY"
  method: "function_calling"
  fallback_to_regex: true
  min_confidence: 0.7
```

Happy extracting! ðŸŽ¯
